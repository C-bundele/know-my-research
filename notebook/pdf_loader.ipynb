{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688487df",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a15e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chanc\\anaconda3\\envs\\ragproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d785d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf\n",
      "  ✓ Loaded 8 pages\n",
      "\n",
      "Processing: Team 28-VLM Hallucination.pdf\n",
      "  ✓ Loaded 8 pages\n",
      "\n",
      "Total documents loaded: 16\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e36e444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/379933077\\nImproving Earth Observations by correlating Multiple Satellite Data: A\\nComparative Analysis of Landsat, MODIS and Sentinel Satellite Data for Flood\\nMapping\\nConference Paper · February 2024\\nDOI: 10.23919/INDIACom61295.2024.10498948\\nCITATIONS\\n7\\nREADS\\n209\\n9 authors, including:\\nSonali Kadam\\nBharati Vidyapeeth College of Engineering for Women\\n54 PUBLICATIONS\\xa0\\xa0\\xa0123 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAnjali Kadam\\nBharati Vidyapeeth's College of Engineering for Women, Pune,India\\n22 PUBLICATIONS\\xa0\\xa0\\xa023 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAhilya Bandgar\\nBharati Vidyapeeth's College Of Engineering for Women\\n4 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nRavindra V. Kale\\nNational Institute of Hydrology\\n45 PUBLICATIONS\\xa0\\xa0\\xa0341 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Anjali Kadam on 27 June 2024.\\nThe user has requested enhancement of the downloaded file.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\nBharati Vidyapeeth's Institute of Computer Applications and Management (BVICAM), New Delhi (INDIA)  \\n \\n \\nImproving earth observations by correlating multiple \\nsatellite data: A Comparative Analysis of Landsat, \\nMODIS, and Sentinel Satellite Data for flood \\nmapping \\nSonali Kadam \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nsonali.kadam@bharatividyapeeth.edu \\nAnjali Kadam \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nAnjali.kadam@bharatividyapeeth.edu \\nPrakash Devale \\nDepartment of Information Technology \\nBharati Vidyapeeth (Deemed to be) \\nUniversity College of Engineering \\nPune, Maharashtra \\nprdevale@bvucoep.edu.in \\nAhilya Bandgar \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nbandgarahilya@gmail.com \\nRajlaxmi Manepatil \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nmanepatilrajlaxmi@gmail.com \\n \\nRavindra Kale  \\nNational Institute of Hydrology \\nRoorkee, Uttarakhand, India \\nravikale.nihr@gmail.com \\nJotiram Gujar \\nDepartment of Chemical Engineering \\nSinhgad College of Engineering  \\nPune, Maharashtra \\njotiramgujar@gmail.com \\n \\nChanchal Bundele \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nchanchalbundele04@gmail.com \\n \\nTanmayi Chavan  \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of  \\nEngineering for Women  \\nPune, Maharashtra \\ntanmayichavan0502@gmail.com \\n  \\n \\nAbstract— Flooding is a recurring and severe calamity, \\naggravated by climate change and rapid urbanization, resulting \\nin substantial loss of life, property, and economic disruption. To \\naddress this critical issue, a comprehensive study focuses on \\ndelineating flood-prone zones using satellite data from Landsat, \\nMODIS, and Sentinel sources. Geographic Information System \\n(GIS) technology w ith Google Earth Engine (GEE) platform, \\nproviding an environment which is based on the cloud for \\nanalysis of geospatial data. The methodology encompasses data \\ncollection, preprocessing, and the application of algorithms for \\nflood mapping and depth estimati on. By assessing flood extents \\nand depths, it contributes to informed decision- making for flood \\nmanagement and disaster preparedness. Researchers have made \\nprominent efforts to use these satellite data in the individual \\nmanner or using techniques like fusi on to combine two different \\nsatellite data. The author has made an attempt to design a \\nplatform where all three- satellite data would be available \\ntogether so that valuable insights are drawn in the field of flood \\nmapping. This paper is prominently focused to corelate those \\nsatellite data with the help of difference mapping. \\nKeywords— Landsat, MODIS, Sentinel, GIS, flood extent mapping, \\nflood depth estimation. \\nI. INTRODUCTION \\nA flooding event can be described as the non- permanent \\ninundation of water that submer ges usual arid regions in any \\ngeographical area. Flooding resulting from rapid snowmelt, \\nstorm surges impacting inland regions from the sea, persistent \\nand intense rainfall during the monsoon period, and the \\ndestruction of dams, embankments, or levees in t he season of \\nhigh winds and heavy rains [1].Deforestation has results in, the \\nincreased frequency of coastal storms can rise because of \\nabrupt changes in land use, inappropriate management of urban \\nstormwater runoffs, and climate change. In recent times, t here \\nhas been a notable increase in the frequency of global flooding \\nevents [2]. Flash floods and the more common river floods are \\nthe two main categories of flooding.River floods usually cause \\nmore property damage, but flash floods usually cause more \\nfatalities [2]. \\nSatellite data is very informative way to extract the images of \\nthe particular area in an efficient manner [22]. Launched in \\nJuly 1972, Landsat 1 was the first digital spaceborne sensor \\ndeveloped for monitoring the terrestrial environment, NASA is \\nin charge of designing and launching the Landsat satellites and \\nsensors into near -polar low earth orbit [14]. The U.S. \\nGeological Survey (USGS) is in charge of flight operations, \\narchiving, ground processing, and distribution, and an affiliated\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\nLandsat Science Team performs scientific and technical \\nevaluation, Landsat has a highest series of missions (landsat -\\n1,2,3,4,5,7,8,9) [10, 14]. Earth Observations entered a new era \\nwith the launch of the Earth Observing System (EOS) Terra \\nsatellite in December 1999 and the Aqua satellite in May 2002. \\nThe NASA Ocean Biology Processing Group (OBPG) and \\nother science teams contributed to these satellite launches [12,       \\n13]. Sentinel -1 (S1) and Sentinel-2 (S2)  which are  \\nmultispectral optical instruments of the Europ ean Space \\nAgency were launched in 2014 and 2017 respectively [15, 16]. \\nIn this research paper, a concerted effort is undertaken to \\nintegrate the capabilities of three prominent Earth observation \\nsatellites – namely, LANDSAT, MODIS, and SENTINEL –  \\nwithin a unified platform utilizing Geographic Information \\nSystem (GIS) tools. The primary objective is to harness the \\nsynergies offered by these satellite systems to deliver efficient \\nand timely flood difference maps. By amalgamating data from \\nmultiple sources, th is approach aims to enhance the precision \\nand reliability of flood monitoring, providing researchers and \\nend-users with prompt access to valuable information. \\nUltimately, this paper envisions a comprehensive and user -\\nfriendly solution that not only leverag es the strengths of \\nmultiple satellite systems but also harnesses the power of GIS \\ntools to deliver swift and reliable flood difference maps. The \\nintegration of LANDSAT, MODIS, and SENTINEL within a \\nunified platform represents a noteworthy advancement in t he \\nfield of remote sensing and geographic information science, \\noffering a valuable resource for those involved in flood \\nresearch and mitigation efforts. \\nII. L\\nITERATURE REVIEW \\nStochastic approaches to flood control encompass a variety of \\nanalytical methods aime d at understanding and managing the \\nuncertainties associated with flooding events. Among these \\napproaches are Monte Carlo Markov Chain Algorithms \\n(MCMC) [3], which leverage probabilistic algorithms to \\nsimulate and sample from probability distributions, ena bling \\nthe assessment of various flood scenarios. Artificial Neural \\nNetwork (ANN)-based predictive models [4] represent another \\nfacet of stochastic analysis, utilizing machine learning \\ntechniques to predict river discharge and other relevant \\nparameters base d on historical data. Conventional modelling \\nschemes [5] in this context typically refer to traditional, \\ndeterministic models used in hydrology and hydraulics  [6] to \\nsimulate the behaviour of rivers and water systems. The Index -\\nFlood Method  [7] involves st atistical analysis of historical \\nflood data to estimate the probability of floods of different \\nmagnitudes occurring, contributing to a more comprehensive \\nunderstanding of flood risk. \\nOn the other hand, Geographic Information System (GIS) \\ncalculations, part icularly those conducted using tools like \\nGoogle Earth Engine (GEE), provide a spatial perspective in \\nflood control strategies.GIS focuses on the analysis of spatial \\nand geographic data  [24], allowing for the mapping and \\nassessment of physical characterist ics of the terrain and land \\nuse. GEE, being a cloud-based platform, facilitates the efficient \\nprocessing and analysis of large -scale [26] Earth observation \\ndata. While stochastic approaches delve into the predictive \\nmodelling of floods, GIS calculations [8] emphasize the spatial \\naspects of flood -prone areas, considering factors such as \\ntopography, land cover, and infrastructure that influence the \\ndynamics of flooding events. The satellite data provides \\nrequired data over long period of time [10], also the geo spatial \\nmethodologies [22] have made a significant contribution for the \\nflood [11], disaster analysis [23]. \\nWith the GEE computing platform, you can do away with the \\nneed for downloading, preprocessing, and a powerful \\ncomputing environment all of which are  typically associated \\nwith using remotely sensed data. GEE is an adaptable and \\ntransparent platform that can handle a wide range of research \\nareas, from forestry to crop mapping to drought monitoring, \\neven though it is optimized for big data. Land use land \\ncoverchange monitoring is made more dependable and \\neffective by its ability to leverage cloud computing resources \\nand access a wealth of multisource datasets [17, 18].In addition \\nto enabling the integration of multi-source and  sensing images, \\nthe rise an d development of satellite remote sensing , cloud \\nstorage, exemplified by GEEalso making full-band image \\ncomputing possible [18]. \\nIII. \\nDATASET USED \\nAs mentioned, we have used LANSAT, MODIS and \\nSNTINEL satellites to perform our analysis in our study. \\nBelow is a list of these satellites' specific attributes. \\nTABLE I. CHARACTERISTICS OF LANDSAT SATELLITE  \\nSatellite Sensor Launc\\nhed \\nYear \\nBands \\nLandsat - 1 MSS 1972 Green, Red, NIR, IR \\nLandsat - 2 MSS 1975 Green, Red, Near-Infrared, \\nInfrared \\nLandsat - 3 RBV 1978 Blue, Green, Red \\nLandsat - 4 TM 1982 TIR, MIR, NIR \\nLandsat - 5 TM 1984 TIR, MIR, NIR, and visible \\nLandsat - 6 TM 1993 TIR, MIR, NIR, and visible \\nLandsat - 7 ETM+ 1999 Evident, TIR, panchromatic, \\nNIR, MIR \\nLandsat - 8 OLI/TIRS 2013 Visible, Panchromatic, SWI, \\nNIR, and TIR \\nLandsat - 9 OLI/TI RS 2021 Visible NR, Visible Blue, \\nVG, RP, SWIR 1, SWIR 2, \\nand Coastal Aerosol \\n  \\nTable I shows the characteristics of Landsat data. The spatial \\nresolution of the Green, Red, and Near Infrared bands on \\nLandsat 1–5 is 60 meters when using the multispectral scanner \\n(MSS). The thermal band in Landsat 4 -5 has a spatial \\nresolution of 120( 17) meters, while the blue, green, red, near \\ninfrared, and SWIR -1/2 bands have a spatial resolution of 30 \\nmeters using Thematic Mapper(TM) [8, 19]. \\n The thermal and panchromatic bands in Landsat 7 have a \\nspatial resolution of 15 and 30 meters, respectively, while the \\nblue, green, red, and NIR SWIR -1/2 bands have a spatial \\nresolution of 30 meters.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\nThe Coastal Aerosol, Blue, Gr een Red, NIR, SWIR- 1/2, and \\nCirrus have a resolution of 30 meters in Landsat 8 -9 using \\nOperational Land Imager and Thermal Infrared Sensors; the \\nPanchromatic band has a resolution of 15 meters, and the \\nTIRS-1/2 has a resolution of 100 meters [19]. \\nTABLE II. Characteristics of MODIS Satellite \\nSatellite Sensor Launch \\nYear \\nBands \\nTerra  MODIS 1999 36 spectral bands that span \\nthe visible to thermal \\ninfrared portions of the \\nelectromagnetic spectrum. \\nAqua MODIS 2002 Like Terra MODIS, it has 36 \\nspectral bands that allow for \\ndetailed observations of the \\nEarth\\'s surface and \\natmosphere. \\n  \\nThe spatial resolution of the Aqua and Terra satellites is \\nidentical; bands 1 and 2 have a 250 m spatial resolution, bands \\n3-7 have a 500 m spatial resolution, and bands 8 -36 have a \\n1000 m spatial resolution, respectively [20]. Table 2 illustrates \\nthe characteristics of MODIS satellite [9]. \\nTABLE III.            Characteristics of Sentinel Satellite  \\nSatellite Sensor Launc\\nh Year \\nBands \\nSentinel - 1 C- band \\nSAR \\n1-A: \\n2014 \\n1-B: \\n2016 \\nThe single SAR sensor \\ncarried by Sentinel -1 can \\nfunction in multiple \\npolarizations, such as HH \\nand VV. \\nSentinel - 2 MSI 2017 A multi-spectral imager with \\n13 spectral bands, including \\nvisible, NIR and SWIR \\nwavelengths, is carried by \\nSentinel-2A and Sentinel -\\n2B. \\nSentinel - 3 OLC/SL\\nSTR/SA\\nRA \\n2018 Sentinel-3A and Sentinel -3B \\nare equipped with multiple \\nspectral bands to measure \\nland surface temperature and \\nocean color. \\n \\nSentinel-1 has four operational modes, SM mode features \\n5X5 metre, IW features 5X20 metre, EW features 20X40 metre \\nand WV mode features 5X5 metre of spatial resolution. \\nThe Sentinel-2 has a spatial resolution of 10 m for the Blue, \\nGreen, Red, and NIR bands; 20 m for the vegetation red edge, \\nNIR, and band -11/12 for the SWIR bands; and 60 m eters for \\nthe coastal aerosol, water vapour, and SWIR Cirrus bands. \\nIn Sentinel-3 as per SLSTR the initial six spectral bands has \\nthe VNIR along with SWIR, in VNIR the band 1 -3 and in \\nSWIR the bands 4 -6 have a spatial resolution of 500 meters  \\n[10]. \\nIV. METHODOLOGY \\nFig. 1 shows the methodology for generating a hazard map, \\nusing the combination of Landsat, MODIS and Sentinel \\nmethodologies together. The method for each satellite is briefly \\nexplained below.  \\nA. Methods For Sentinel \\n The following steps illustrateflood depth and flood area \\ncalculations of Sentinel dataset using GEE. \\n• Gathering data, preparing it for processing, and \\nloading the COPERNICUS/S1_GRD satellite dataset \\nand data of study area into the GEE Code Editor – \\nLoad the dataset of COPERNIC US/S1_GRD along \\nwith the river basin dataset. Next, filter the collection \\nto exclusively contain images with the \\n\\'instrumentMode\\' property set to \\'IW\\'. Subsequently, \\nnarrow down the collection even further to \\nencompass only those images where the \\n\\'transmitterReceiverPolarisation\\' property contains \\n\\'VV\\'. Following this, apply a spatial filter to the \\ncollection, ensuring it comprises only images that \\nintersect with a specified area of interest (AOI). \\nLastly, concluded by selecting solely the \\'VV\\' band \\nfrom each image within the filtered collection. \\n \\n• Mosaic and clip satellite data – \\nIn this, an Earth Engine Image Collection is subjected \\nto two different filtering methods. One which creates \\na composite image that represents the \"before\" flood \\nimages, from the collection that falls between the \\ngiven dates are first filtered. Then, they are combined \\ninto one image using the ‘mosaic\\' function. Similar to \\nthe first filtering process, a second filtering operation \\nis performed to choose images according to the flood \\noccurred dates. These images are likewise selected, \\nand they are then combined using ‘mosaic\\' to create a \\ncomposite image that represents the \"after\" flood \\nimagery.  \\n• Apply Smoothening Filter– \\nA smoothing filter is applied to two previously \\nclipped images. First, the before image, which has \\nbeen clipped and is subjected to a focal m edian filter \\nwith a radius of 30 meters and a circular \\nneighbourhood, resulting in a smoothed version. \\nSimilarly, smoothening of after image is carried out. \\n \\n• Calculating Flood extent-  \\nThe smoothed before image is subtracted from the \\nsmoothed after image i n a difference operation to get \\nan image that shows the difference between the two \\ntimes. A binary mask picture is then created by \\napplying a threshold to the difference image, where \\npixels with values less than - 3 are thought to indicate \\nthe extent of a f lood. In order to effectively mask out \\nall non-flood areas and produce a final binary image \\nthat highlights the flooded parts, the flood extent \\nmask is applied to itself using the \\'updateMask\\' \\nfunction.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\n \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\ndata, if available, can be loaded to determine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It then \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the   ability to add \\nadditional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts  \\noutlier identification. It uses a cumulative cost \\nmethod to determine the floodwater surface elevation \\nmodel, then uses a low -pass filter to determine the \\nfloodwater depth and smooth it out. Regular water \\nbodies and 0 values are excluded using optional \\nmasking processes.  \\nB. Methods For Landsat \\nThe following steps illustrateflood depth and flood area \\ncalculations of Landsat dataset using GEE. \\n• Gathering information, preprocessing it, and loading \\nthe LANDSAT/LC08/C02/T1_RT satellite dataset \\nand study area data into the GEE Code Editor – \\nA specific area of interest (AOI) corresponding to the \\nriver basin is selected from a table and added to the \\nmap for visualization. Then, a Landsat image \\ncollection is filtered to include only images from the \\ndate ranging within the b ounds of the previously \\ndefined AOI. The images are mosaicked into a single \\ncomposite, and the result is clipped to the same AOI. \\nTwo bands, B5 and B3, are selected from this dataset. \\nFor each band, visualization parameters are set to \\nspecify the minimum a nd maximum values for \\ndisplay within the specified range of 0.0 to 30,000.0. \\n• Visualizing NDWI - \\nTo visualize NDWI is carried out by subtracting \\nBand B2 from Band B1 and then dividing the result \\nby the sum of Band B1 and Band B2. Visualization \\nparameters ar e set with a minimum value of -1, a \\nmaximum value of 1, and a colour palette ranging \\nfrom white (for values less than -1), through red, to \\nblue (for values greater than 1). Finally, the NDWI \\nlayer is added into the map with the specified \\nvisualization settings which allows to visualize water \\nbodies and their intensity within the selected dataset. \\n• Calculating Flooded Area-  \\nA binary mask is made to identify the blue region in \\nthe NDWI (Normalized Difference Water Index) \\nlayer in order to determine the floode d area. The \\nNDWI \\nvalues between 0 and 1, which often depict aquatic \\nbodies, fall into this blue zone. By determining \\nwhether NDWI values are larger than 0 and less than \\n1, the binary mask is produced. To determine the \\nflooded area within each pixel, this m ask is then \\nmultiplied by the pixel area image. This procedure \\ngenerates a \"floodedAreaImage\" that enables the \\nquantification of flooded areas within the dataset. \\nEach pixel value reflects the area of the flooded zone \\nin square meters. \\n• Total Flooded Area Calculation – \\nThe entire flooded area is determined in square \\nmetres. First, a region reduction of the \\n\\'floodedAreaImage\\' is performed using the sum \\nreducer within the defined area of interest (AOI) \\ngeometry. To properly manage big datasets, a scale of \\n30 m eters, which corresponds to the resolution of \\nLandsat pictures, is established. The entire area that \\nhas been inundated is then calculated in square \\nmeters. The result, which represents the entire \\nflooded area in square kilometres, is displayed to the \\nconsole for reference after the value is divided by 1e6 \\nto show the result in square kilometres. \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\nFig. 1.  Methodology for Calculating Hazard Map.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\ndata, if available, can be loaded to d etermine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It then \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the ability to add \\nadditional water bodies to the flood  extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a \\ncumulative cost method to determine the floodwater \\nsurface elevation model, then uses a low -pass filter to \\ndetermine the floodwater depth and s mooth it out. \\nRegular water bodies and 0 values are excluded using \\noptional masking processes.  \\nC. Methods For Modis \\nThe following steps illustrate flood depth and flood area \\ncalculations of  MODIS dataset using GEE. \\n• Gathering data, preprocessing it, and l oading the \\nStudy Area data and Satellite dataset \\n(MODIS/006/MOD09A1) – \\nThe area of interest (AOI) corresponding to the \\ntargeted river basin which is selected from the table \\nand added to the map for visualization. Next, a \\nMODIS Image Collection is filtered to include \\nimages from the range of flood occurred dates, within \\nthe bounds of the specified AOI. The images are then \\nmosaicked where in this case, the median value is \\nused to creating a composite image of the selected \\ndate, and the result is clipped to th e AOI. Two bands, \\n'sur_refl_b02' and 'sur_refl_b06', are selected from \\nthis dataset. Visualization parameters are set to define \\nthe display range for each band, and both b02 and \\nb06 bands are added as layers to the map, allowing \\nfor the visualization of MODIS data for the AOI river \\nbasin. \\n \\n• NDWI Calculation and Flooded Area Estimation - \\n'sur_refl_b02' (b02) and'sur_refl_b06' (b06) MODIS \\nbands are used to calculate the NDWI. An 'NDWI' \\nlayer is produced by computing the NDWI by \\ndeducting the near -infrared values from the green \\nvalues and dividing the result by the sum of b02 and \\nb06. This NDWI layer is added to the map, and the \\nparameters are set to display NDWI values in the \\nrange of - 1 to 1 with a colour palette ranging from \\nwhite to red to blue. Furtherm ore, a binary mask with \\nvalues between 0 and 1 is made for the blue region of \\nthe NDWI. Potential water bodies are identified by \\nthis 'blueRegionMask'. The pixel area image is then \\nmultiplied by this mask to enable the estimation of \\nthe flooded area in each pixel, which can be valuable \\nfor flood analysis and mapping. \\n \\n• Total Flooded Area Calculation – \\nThere is a calculation of the total flooded area which \\nis calculated in square meters and is the total area \\nsubmerged by flooding. Using the sum reducer, a \\nregion reduction operation is performed on the \\n'floodedAreaImage,' which reflects the extent of \\nflooded regions within each pixel. This reduction is \\ncarried out inside the boundaries of the area of \\ninterest (AOI) that was previously specified. To \\nmanage huge datasets effectively, a specified scale of \\n30 meters, which corresponds to the resolution of the \\nMODIS data, is chosen (it can be changed as \\nnecessary). Additionally, a maximum pixel limit of 1 \\nbillion pixels is determined. Calculated results can be \\nfurther processed or displayed as needed for flood \\nassessment and analysis. The result, which represents \\nthe entire flooded area in square kilometres, is \\ndisplayed to the console for reference after the value \\nis divided by 1e6 to show the result in square \\nkilometres. \\n \\n• Calculating Flood Depth – \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\ndata, if available, can be loaded to determine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It the n \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the ability to add \\nadditional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a \\ncumulative cost method to determine the floodwater \\nsurface elevation model, then uses a low -pass filter to \\ndetermine the floodwater depth and smooth it out. \\nRegular water bodies and 0 values are excluded using \\noptional masking processes.  \\nV. R\\nESULTS AND DISCUSSION \\n    The proposed study evaluates the flood extent maps during \\nthe period of 2019 August to September in Mahanadi River \\nbasin. For this evaluation, calculated flood extent maps for the \\nindividual satellite data using above mentioned methods. Fig.  \\n2 depicts the flood extend map of sentinel data calculated by \\nadjusting the threshold-value.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\n \\nFig. 2.  Flood extent map using Sentinel data. \\nFig. 3 depicts the flood extent map of Landsat data, featuring \\nNDWI with bands B3 and B5. \\n \\nFig. 3. Flood extent map using Landsat data. \\nFig. 4 depicts the flood extent map of Modis data, with the \\nhelp of binary flood mask using band sur_refl_b0 1 and \\nsur_refl_b02. \\n \\nFig. 4. Flood extent map using Modis data. \\nThe above Fig. 4 illustrate the flood extent areas with \\nvariations in their values. The results obtained out of flood \\nextent maps are not that efficient to refer them for further \\nstudies. In order to refer a correct flood extent area, we need to \\napply certain techniques and methods to get an accurate flood \\nextent area. In this study an attempt is made to get an accurate \\nflood extent area using the concept of difference mapping. To \\ngenerate a difference map, individual flood extent map are \\nused and the bands  of interest from both sensors are \\nsubtracted. The pixel -wise difference is calculated and the \\nobtained calculation is the accurate flood extent area. \\n \\nFig. 5. Difference map of Sentinel and Modis data \\nFig. 2, 3 and 4 illustrates the flood extent maps for  the \\nSentinel, Landsat and Modis data respectively showing a \\nsignificant difference between the flood extent area. Hence an \\nattempt is made to get the results more accurately using the \\ndifference maps. The difference map is generated using \\nsubtracting the bands of interest from both sensors and the \\npixel-wise difference is calculated. \\nThe research related to the difference mapping for flood extent \\ncalculation is generally limited to the individual satellite data \\n[21], in this paper an attempt is made to cal culate the \\ndifference map for multiple satellite data in combination. \\nThe difference map is then encompassed with the GIS tool to \\nget the accurate results for further studies, hence as per user’s \\nchoice a combination of results would be obtained on one \\nplatform. This difference map can be used for generating flood \\ninundation maps, flood hazard maps, for monitoring the river \\ndynamics, time series analysis, land cover changes, etc \\nV.\\n CONCLUSION AND FUTURE SCOPE \\n Proposed study provides a comprehensive examina tion of \\nmultiple satellite datasets consolidated onto a singular platform. \\nThe subsequent evaluation and comparison of flood extent \\nareas for each satellite data source revealed significant \\ndifferences, prompting a detailed analysis and the creation of a \\ndifference map to visualize and interpret these variations in \\naccuracy. This study contributes valuable insights for \\nresearchers and practitioners relying on satellite data for flood \\nmonitoring and related applications. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\nfields of early warning systems, flood management, \\nresearchers, etc. \\n To quantify and visualize the observed differences in \\naccuracy among the various satellite datasets, we employed a \\nsystematic approach. Specifically, we generated a difference \\nmap that highlighted the disparities in flood extent \\nidentification across the differ ent satellite datasets. This map \\nserved as a valuable tool for effectively illustrating the \\nvariations in accuracy and allowed for a nuanced interpretation \\nof the discrepancies.Also, the difference maps play a vital role \\nin flood mapping by enabling the detection, analysis, and \\nmonitoring of changes in the landscape related to flooding. \\nThese maps provide valuable information for emergency \\nresponse, risk assessment, and long-term flood management. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the \\nfields of early warning systems, flood management, \\nresearchers, etc. But, certain limitations such as limited \\nspectral information, land cover changes or lack of data \\naccessibility may lead to the discrepancies in the results.  \\nR\\nEFERENCES \\n[1] Nsangou D, Kpoumié A, Mfonka Z, Bateni SM, Ngouh AN, Ndam          \\nNgoupayou JR, “The Mfoundi Watershed at Yaoundé in the \\nHumid Tropical Zone of Camer oon: a case study of urban food \\nsusceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol. \\n5, Issue 4, pp.861-871,2021.  \\n[3] Marinho G. Andrade, Marcelo D. Fragoso, Adriano A.F.M. \\nCarneiro, “A stochastic approach to the flood control problem,” \\nApplied Mathematical Modelling, vol. 25, Issue 6, 2001. \\n[4] Yiming Wei, Weixuan Xu, Ying Fan, Hsien -Tang Tasi, “Artificial \\nneural network based predi ctive method for flood disaster,” \\nComputers & Industrial Engineering vol. 42, Issues 2–4, 2002.  \\n[5] Shangyou Zhang, Ian Cordery, Ashish Sharma, “Application of an \\nimproved linear storage routing model for the estimation of large \\nfloods”, Journal of Hydrology, vol. 258, Issues 1–4, 2002. \\n[6] Karsten Jaspe, Joachim Gurtz, Herbert Lang, “Advanced flood \\nforecasting in Alpine watersheds by coupling meteorological \\nobservations and forecasts with a distributed hydrological model,” \\nJournal of Hydrology vol. 267, Issues 1–2,2002.  \\n[7] Thomas Rodding Kjeldsen, Jeff Smithers, Roland Schulze, \\n“Regional flood frequency analysis in the KwaZulu -Natal \\nprovince, South Africa, using the index -flood method,” Journal of \\nHydrology, vol. 255, Issues 1–4, 2002. \\n[8] Helder I. Chamine, Alcides  J. S. C. Pereira, Ana  C. Teodoro \\n“Remote sensing and  GIS applications in  earth and environmental \\nsystems sciences,” Springer Nature, vol. 3, 2021. \\n[9] Christopher J. Crawford, David P. Roy, Saeed Arab, \\nChristopherBarnes, et al., “The 50 -year Landsat collection 2 \\narchive,” Science of Remote Sensing, vol. 8, 2023. \\n[10] Christoph Kubitza, Vijesh Krishna, “Estimating adoption and \\nimpacts of agricultural management practices in developing \\ncountries using satellite data. A scoping review,” Springer, vol. \\n40,2020. \\n[11] Sreechanth Sundaram, Suresh Devaraj & Kiran Yarrakula \\n“Modeling, mapping and analysis of urban floods in India —a \\nreview on geospatial methodologies,” Springer, vol. 28,2021. \\n[12] Alexei Lyapustin et al., “Calibration of the SNPP and NOAA 20 \\nVIIRS sensors for continuity of  the MODIS climate data records”, \\nRemote Sensing of Environment, vol. 295, 2023. \\n[13] M. Bellaoui, K. Bouchouicha, B. Oulimar, “Daily Global Solar \\nRadiation Based on MODIS Products: The Case Study of ADRAR \\nRegion (Algeria)”, Springer, vol. 102, 2019. \\n[14] Ekrem Sara lioglu, Can vatandaslar “Land use/land cover \\nclassification with Landsat -8 and Landsat -9 satellite images: a \\ncomparative analysis between forest - and agriculture -dominated \\nlandscapes using different machine learning methods”, Springer \\nvol. 57,2022. \\n[15] Beste Tavus, Sultan Kocaman, Candan Gokceoglu, “Flood damage \\nassessment with Sentinel-1 and Sentinel -2 data after Sardoba dam \\nbreak with GLCM features and Random Forest method,” Science \\nof The Total Environment vol. 816, 2022. \\n[16] Giacomo Caporusso, Marino Dell’Olo, et al, “Use of the Sentinel-1 \\nSatellite Data in the SNAP Platform and the WebGNOME \\nSimulation Model for Change Detection Analyses on the Persian \\nGulf Oil Spill,” Springer vol. 13379, 2022. \\n[17] Hamdi A.  Zurqani, “An automated approach for developing a \\nregional-scale 1 -m forest canopy cover dataset using machine \\nlearning and Google Earth Engine cloud computing platform,” \\nSoftware Impacts, vol. 19, 2024. \\n[18] Jintao Liang a, Chao Chen b, Yongze Song c, Weiwei Sun d, Gang\\n Yang d, “Long-term mapping of land use and cover changes using \\nLandsat images on the Google Earth Engine Cloud Platform in bay \\narea - A case study of Hangzhou Bay, China,” Sustainable \\nHorizons, vol. 7, 2023. \\n[19] USGS, “What are the band designations for the Landsat satellites?”  \\n[Online]. Available: https://www.usgs.gov/faqs/what-are-band-\\ndesignations-landsat-satellites. [Accessed: 5- Dec- 2023]. \\n[20] Sadashiva Devadiga, “Terra & Aqua Moderate Resolution Imaging \\nSpectroradiometer (MODIS)” [Online] Available:  \\nhttps://ladsweb.modaps.eosdis.nasa.gov/missions-and-\\nmeasurements/modis/#:~:text=MODIS%20data%20products%2C\\n%20in%20three,and%20in%20the%20lower%20atmosphere. \\n[Accessed: 7- Dec- 2023]. \\n[21] Ramesh Sivan Pillai, Kevin M. Jacobs Chloe M. Mattilio, Ela V. \\nPiskorski, “Rapid flood inundation mapping by differen cing water \\nindices from pre - and post -flood Landsat images ,” Springer vol. \\n15,2021. \\n[22] Malay S. Bhatt, Tejas P. Patalia, “ Content-based high -resolution \\nsatellite image classification,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2018 vol. 11, 2019. \\n[23] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Proposal and evaluation of tsunami disaster drill support \\nsystem using tablet computer,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2023 vol. 15, 2023. \\n[24] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Bhoomi Prahari - e governance tool for monitoring \\nencroachment on government land using mobile and GIS \\ntechnology,” IJIT - Bharati Vidyapeeth’s Institute of Comp uter \\nApplications and Management 2023 vol. 14, 2023. \\n[25] Venkata Sangameswar Mandavillil, Nagabhushanarao Madamala, \\n“Detection of natural disaster affected areas using R,” IJIT- Bharati \\nVidyapeeth’s Institute of Computer Applications and Management \\n2018 vol. 10, 2018. \\n[26] Mohd. Tajammul, Rafat Parveen, “Auto encryption algorithm for \\nuploading data on cloud storage”, IJIT - Bharati Vidyapeeth’s \\nInstitute of Computer Applications and Management 2020  vol. \\n12,2020.\\n \\nView publication stats'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning\\nProject Option: Comparative Analysis\\nAVIPSA BHUJABAL, University at Buffalo, USA\\nCHANCHAL BUNDELE, University at Buffalo, USA\\nVISHAL LAVANGARE,University at Buffalo, USA\\n1 Abstract\\nThe goal of this project is to construct a robust image captioning\\npipeline capable of generating high-quality, semantically accurate\\ncaptions for actual images. We explore a number of architectures\\nto this end, beginning with a baseline CNN-LSTM pipeline using\\nDenseNet201 as the feature extractor and an LSTM decoder trained\\non the Flickr8k dataset. For comparison with newer models, we\\nadded BLIP (Bootstrapping Language-Image Pretraining), a state-\\nof-the-art Vision-Language Model (VLM) that has been extremely\\neffective for captioning tasks.\\nTo improve the factual correctness of generated captions and\\nreduce hallucination—when models describe objects that are not\\npresent in the image—we integrated object detection using YOLOv5.\\nThis allowed us to identify the actual content of the image and\\neliminate semantically unnecessary or hallucinated words from\\ngenerated captions. This approach to reducing hallucination was\\nguided by methods discussed in papers our advisor exposed us to.\\nWe evaluated the models in terms of BLEU-1 to BLEU-4 scores and\\nalso plotted performance using t-SNE to verify clustering in image\\nfeature space. Quantitative and qualitative results show that BLIP\\nsignificantly outperforms traditional CNN-RNN models, producing\\nmore accurate and descriptive captions. The hallucination filtering\\nmethod enhanced the factual accuracy of the captions regarding\\nvisual content.\\nOverall, this project demonstrates a complete image captioning\\npipeline with a focus on benchmarking, hallucination detection, and\\nvisualization, offering a pragmatic platform for building semanti-\\ncally grounded captioning systems.\\n1.1 Why This is Interesting\\nImage captioning, the task of generating natural language descrip-\\ntions from images, has recently emerged as a sought-after problem\\ndue to its extensive applications in accessibility aides, content-based\\nimage retrieval systems, social media automation, and human-robot\\ninteraction. The prime challenge lies in effectively bridging the\\nvision-language gap—capturing the content in an image and con-\\nveying it through coherent, meaningful text.\\nTraditionally, encoder-decoder architectures have been employed,\\nwhere a convolutional neural network (CNN) extracts image fea-\\ntures and a recurrent neural network (RNN), such as LSTM, gen-\\nerates a caption. Although these models initially demonstrated\\npromise, they often suffered from poor generalization and hallu-\\ncination—generating information not present in the image. The\\nadvent of pretrained vision-language models (VLMs) like BLIP and\\nAuthors’ Contact Information: Avipsa Bhujabal, University at Buffalo, Buffalo, USA,\\nNew York, avipsabh@buffalo.edu; Chanchal Bundele, University at Buffalo, Buffalo,\\nUSA, cbundele@buffalo.edu; Vishal Lavangare, University at Buffalo, Buffalo, USA,\\nvlavanga@buffalo.edu.\\nCLIP marked a significant shift toward multimodal transformers,\\nresulting in enhanced performance across captioning, retrieval, and\\nvisual question answering (VQA) tasks.\\nOur research presents a full image captioning pipeline that com-\\npares a traditional CNN-RNN model using DenseNet201 with a\\nVLM-based model using BLIP. Beyond performance evaluation, we\\nincorporate object detection using YOLOv5 to address hallucination\\nby anchoring captions to actual detected objects, inspired by recent\\nefforts in semantic grounding.\\nThe motivation stems from real-world implications—such as in\\nassistive technology for the visually impaired, where hallucinated\\ncaptions (e.g., describing a dog that isn’t present) can mislead users.\\nA hallucination-aware captioning model enhances the reliability\\nand informativeness of AI-generated descriptions.\\nWe train both baseline and state-of-the-art models on the Flickr8k\\ndataset, integrate hallucination filtering, and evaluate them using\\nBLEU scores and visualizations such as t-SNE. Our key contributions\\ninclude:\\n• A comparative benchmark of traditional and modern image\\ncaptioning models.\\n• A hallucination filtering module leveraging object detection\\nto ground captions in detected visual content.\\n• Visualization tools to interpret model behavior and feature\\nrepresentations.\\nIn the broader multimodal AI landscape, this work offers a practi-\\ncal and educational examination of how emerging technologies can\\nbe harnessed to address persistent challenges in image captioning,\\nnotably hallucination and factual consistency.\\n2 Related Work\\nThe field of image captioning has evolved significantly, with nu-\\nmerous models tackling the challenge of generating accurate and\\ncontextually relevant descriptions from images.\\n• Show and Tell [1]: Introduced the foundational encoder-\\ndecoder architecture using CNNs and RNNs. It produced\\ngeneric captions but struggled with novel object descrip-\\ntions.\\n• Show, Attend and Tell [2]: Employed attention mecha-\\nnisms to focus on specific regions of the image. Despite\\nimprovements, it sometimes misaligned attention, leading\\nto incorrect descriptions.\\n• Bottom-Up and Top-Down Attention [3]: Combined ob-\\nject detection and attention to improve detail, though it\\noccasionally hallucinated non-existent objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='2 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\n• Adversarial Semantic Alignment [4]: Leveraged GANs\\nto improve semantic alignment between captions and im-\\nage content. However, training GANs on discrete text data\\nremains a challenge.\\n• Visual Fact Checker (VFC) [5]: Proposed a training-free\\ncaption fact-checking system using object detection and\\nVQA. Its effectiveness depends on the external tools’ quality.\\n• BLIP [6]: A vision-language model achieving state-of-the-\\nart results across multiple tasks, though prone to hallucina-\\ntion without explicit grounding.\\n• ALOHa [7]: Introduced a large-language-model-based hal-\\nlucination detection metric, improving on previous fixed-\\nvocabulary approaches.\\n• Reducing Object Hallucination [8]: Proposed training\\nobjective optimization and object presence constraints to\\nreduce hallucination.\\n• Object Grounding and Hallucination [9]: Found that\\ngrounding alone may not sufficiently reduce hallucination,\\nemphasizing the need for holistic strategies.\\n• Scalable Vision Learners [10]: Showed that captioning\\nmodels can be robust visual learners with appropriate scale\\nand fine-tuning.\\n• EVCap [11]: Used external memory for object name retrieval\\nto improve open-world captioning.\\n• Multimodal Hallucination Detection in 3D [12]: Ad-\\ndressed hallucination across modalities for 3D content but\\nfocused on a different domain.\\n• Captioning Evaluation with LLMs [13]: Highlighted the\\nneed for better metrics capturing hallucination and factual\\nconsistency in image captioning.\\n• Pseudo Content Hallucination[14]: Used pseudo-generated\\ncontent for unpaired image captioning, albeit with potential\\nnoise.\\n• Hyperbolic Learning for Open-World Detection [15]:\\nApplied synthetic captions in hyperbolic space to improve\\ndetection, aiding captioning tasks.\\n3 Proposed Approach and Contributions\\nIn this paper, we present an end-to-end image captioning pipeline\\nthat synergizes state-of-the-art vision-language models with ad-\\nvanced evaluation methods to generate accurate and contextually\\nrelevant captions. Our approach centers around comparing the per-\\nformance of BLIP (Bootstrapped Language-Image Pretraining), a\\nstate-of-the-art vision-language model, to that of a CNN-RNN-based\\ncaptioning architecture using DenseNet201 features. Additionally,\\nwe integrate YOLOv5-based object detection to mitigate hallucinated\\nobjects in generated captions—addressing the well-documented is-\\nsue of hallucination in captioning models.\\n3.1 Primary Contributions\\n(1) Multi-model Captioning Benchmark: We establish a\\ncomparative benchmark between a traditional CNN-RNN\\narchitecture and a transformer-based vision-language model\\n(BLIP). Using evaluation metrics such as BLEU, we demon-\\nstrate the superior performance of recent pre-trained trans-\\nformers in generating coherent and relevant image captions.\\n(2) Hallucination Reduction via Object Detection: Drawing\\ninspiration from recent research in hallucination detection\\nand semantic grounding, we employ YOLOv5 to detect ob-\\njects within the image and remove tokens in the generated\\ncaptions that lack visual grounding. This results in more\\nfactual and accurate descriptions.\\n(3) Interactive Visual Analysis: We develop side-by-side visu-\\nalizations that display input images, captions generated by\\nboth BLIP and CNN-RNN models, and highlight hallucinated\\ncontent. These visual aids enhance model interpretability\\nand support thorough error analysis.\\n(4) Robust Preprocessing and Dataset Exploration: We ap-\\nply preprocessing techniques such as lemmatization, stop-\\nword removal, and caption length filtering on the Flickr8k\\ndataset. These steps contribute to a cleaner training dataset\\nand more reliable tokenizer performance.\\n(5) End-to-End Pipeline with BLIP + YOLOv5 Integration:\\nTo the best of our knowledge, this work is among the first in\\ncoursework to combine BLIP-based captioning with YOLOv5-\\nbased hallucination filtering, DenseNet-based baselines, and\\ninteractive visualizations in a cohesive and modular pipeline.\\nOur work bridges the gap between captioning accuracy and fac-\\ntual coherence, offering a flexible framework that can be extended\\nwith other vision-language models such as CLIP or grounding-\\nspecific techniques. All code is made publicly available and is Colab-\\nfriendly to support reproducibility and experimentation.\\n4 Methodology\\nOur project aims to solve the problem of automatic image captioning\\nusing both conventional deep learning models and state-of-the-\\nart transformer-based models. We explore two primary modeling\\napproaches, along with hallucination filtering and comprehensive\\nevaluation.\\n4.1 CNN-RNN Baseline using DenseNet201 + LSTM\\nDecoder\\nWe adopt a standard encoder-decoder architecture comprising:\\n• Encoder: DenseNet201, a deep CNN pretrained on Ima-\\ngeNet, is used to extract 1920-dimensional feature vectors\\nfrom input images. The classification head is removed to\\nretain only visual embeddings.\\n• Decoder: An LSTM-based RNN processes the image fea-\\ntures concatenated with embedded word sequences to pre-\\ndict the next word. The decoder includes dropout layers for\\nregularization and a softmax output over the vocabulary.\\nThis model is trained using a custom data generator that dynam-\\nically pairs image features with caption sequences. Captions are\\ntokenized into (input, output) word pairs, and sequences are\\npadded and one-hot encoded.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 3\\n4.2 BLIP: Bootstrapped Language-Image Pretraining\\nWe leverage BLIP, a state-of-the-art pretrained vision-language\\nmodel from Salesforce. BLIP uses a Vision Transformer (ViT) en-\\ncoder and a language transformer decoder. Unlike our CNN-RNN\\nbaseline, BLIP directly takes raw images and generates captions\\nthrough pretrained attention mechanisms. We initialize BLIP in\\nzero-shot mode via the Hugging Face Transformers library, with-\\nout further fine-tuning. This enables a comparative study between\\nhandcrafted and pretrained models.\\n4.3 Hallucination Minimization with YOLOv5 Integration\\nTo address the common issue of hallucination—generating captions\\nwith objects not present in the image—we integrate the YOLOv5\\nobject detection framework. For each test image:\\n• YOLOv5 is used to detect objects in the image.\\n• The generated caption is post-processed, and non-grounded\\ntokens (not in the detected object list) are removed, exclud-\\ning stopwords and function words.\\nThis approach semantically aligns captions with detected content,\\ndrawing on current research in factual grounding and hallucination\\nmitigation.\\n4.4 Evaluation and Visualization\\n• We evaluate captions using BLEU-1 to BLEU-4 scores.\\n• We visualize results by showing:\\n– Original image with ground truth caption\\n– DenseNet-LSTM-generated caption\\n– BLIP-generated caption\\n– YOLOv5-filtered caption (hallucination-minimized)\\n• We apply t-SNE and KMeans clustering on image features\\nto analyze semantic grouping.\\n• Additional metrics such as METEOR and ROUGE are calcu-\\nlated using the nlg-eval toolkit where available.\\n5 Preprocessing, Model Architecture, and Training\\n5.1 Preprocessing and Data Preparation\\nImage Preprocessing. To ensure consistency and model compatibility,\\nthe following steps are performed on raw images:\\n• Resizing: All images are resized to224 × 224 pixels to match\\nthe input requirement of DenseNet201.\\n• Normalization: Pixel values are normalized to the range\\n[0, 1] by dividing each pixel by 255. This aids in faster con-\\nvergence during training.\\n• Feature Extraction: DenseNet201, pretrained on ImageNet,\\nis used without its final classification layer. We extract the\\n1920-dimensional feature vector from the second-to-last\\nlayer, capturing high-level semantics.\\n• Feature Storage: Extracted features are stored in a dic-\\ntionary with image filenames as keys to avoid redundant\\ncomputation and accelerate model training.\\nText Preprocessing. Since raw captions are not model-compatible,\\nwe perform the following preprocessing:\\n• Lowercasing: Converts all text to lowercase to avoid case-\\nbased duplicates.\\n• Punctuation and Number Removal: Uses regular expres-\\nsions to clean special characters and numeric values.\\n• Whitespace Normalization: Removes extra spaces and\\nnormalizes the spacing.\\n• Short Word Filtering: Removes tokens with fewer than\\ntwo characters.\\n• Lemmatization: Reduces words to their root form using\\nspaCy.\\n• Stopword Removal: Filters out common stopwords for\\nbetter semantic focus.\\n• Start/End Tokens: Captions are enclosed with startseq\\nand endseq to define boundaries for generation.\\nTokenization and Splitting.\\n• Tokenizer: Keras’sTokenizer is used to convert words into\\ninteger indices, resulting in a vocabulary of approximately\\n6700 words.\\n• Max Caption Length: The longest caption length ( 34 to-\\nkens) is saved and used for sequence padding.\\n• Padding: All sequences are padded to the maximum length\\nto standardize input shapes.\\n• Train-Test Split: The dataset is split into 85% training and\\n15% validation, ensuring no image overlap.\\n5.2 Model Architecture\\nVisual Feature Extraction using DenseNet201.\\n• Dense Connectivity: Promotes gradient flow and feature\\nreuse.\\n• Transfer Learning:Leverages pretrained ImageNet weights\\nfor faster convergence.\\n• Output Feature Vector: Extracts a 1920-dimensional rep-\\nresentation per image.\\nTextual Decoder with LSTM. The caption generation model includes:\\n• Embedding Layer: Transforms tokenized words into dense\\n256-dimensional vectors.\\n• Image Compression: The 1920-dimensional feature vec-\\ntor is processed through a dense layer to align with text\\nembeddings.\\n• Concatenation: The processed image vector is concate-\\nnated with the embedded caption sequence.\\n• LSTM Decoder: A 512-unit LSTM processes the combined\\nsequence to predict the next word.\\n• Residual Connection: Adds LSTM output back to the im-\\nage vector to reinforce visual grounding.\\n• Dense Layers: A fully connected Dense(128) layer followed\\nby a Dense layer with softmax activation over the vocabu-\\nlary.\\nThe final model contains approximately 5.74 million trainable\\nparameters.\\n5.3 Training Strategy\\nCustom Data Generator. To accommodate varying caption lengths\\nand memory constraints, a custom Keras Sequence generator is\\nimplemented to:\\n• Load precomputed image features.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='4 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\n• Dynamically generate (input sequence, target word) training\\npairs.\\n• Apply tokenization, padding, and one-hot encoding on-the-\\nfly.\\nTraining Configuration.\\n• Loss Function: Categorical Crossentropy for multi-class\\nword prediction.\\n• Optimizer: Adam with a learning rate of 0.001.\\n• Batch Size: 64\\n• Epochs: Trained for up to 30 epochs.\\n• Callbacks:\\n– ModelCheckpoint: Saves the model with the lowest\\nvalidation loss.\\n– EarlyStopping: Stops training if no improvement is\\nseen for 5 epochs.\\n– ReduceLROnPlateau: Halves the learning rate if vali-\\ndation loss plateaus for 3 epochs.\\n6 Evaluation and Results\\nEvaluation Metrics\\nWe evaluate the performance of our captioning models using the\\nfollowing metrics:\\n• BLEU-1 to BLEU-4: Measures n-gram precision between\\ngenerated and reference captions.\\n• METEOR: Considers synonym matching and word stems,\\nproviding recall-oriented evaluation.\\n• ROUGE-L: Evaluates the longest common subsequence be-\\ntween candidate and reference captions.\\n• CIDEr: Measures consensus between generated captions\\nand human annotations. Due to the limited size of Flickr8k,\\nCIDEr scores were found to be low.\\n7 Flickr8k Dataset Overview\\nFor our project, we use the Flickr8k dataset, a widely used bench-\\nmarking dataset for automatic image captioning. It is a collection\\nof images with multiple human-written captions, making it an ideal\\ntool for evaluating the performance of machine learning models in\\ngenerating textual descriptions.\\n7.1 Characteristics of the Dataset\\n• Source: The dataset was introduced by Hodosh, Young, and\\nHockenmaier (2013).\\n• Accessible on Kaggle: https://www.kaggle.com/datasets/\\nadityajn105/flickr8k\\n• Size:\\n– Total Images: 8,092 JPEG images.\\n– Total Captions: 40,000 human-written captions.\\n– Captions per Image: Each image has five different\\ncaptions written by human annotators.\\n• Format:\\n– Images are available in JPEG format.\\n– Captions are stored in a text file linking each image to\\nits corresponding descriptions.\\nFig. 1. Sample Dataset\\n7.2 Challenges in the Flickr8k Dataset\\n• Data Size Constraint: Unlike MS-COCO (123,000+ im-\\nages), Flickr8k contains only 8,092 images, making gener-\\nalization more difficult.\\n• Caption Variability: Each image has multiple captions,\\nintroducing lexical and syntactic differences. Evaluating\\ncaptions is challenging due to varying levels of detail.\\n• Diverse Image Content: The dataset includes human be-\\nhavior, animals, objects, and natural scenes , requiring\\nmodels to handle contextual understanding.\\n• No Explicit Object Annotations:Unlike MS-COCO, Flickr8k\\ndoes not include bounding box annotations. Models must\\ninfer semantic relationships from image features alone.\\n• Computational Efficiency: The dataset’ssmall size al-\\nlows for rapid model training and testing , but high-\\ncapacity models like transformers risk overfitting with-\\nout sufficient regularization.\\n8 Why Flickr8k?\\nThe Flickr8k dataset is chosen because it allows us to:\\n• Evaluate models under low-resource conditions.\\n• Analyze performance on a dataset with diverse cap-\\ntions and image types.\\n• Benchmark models without requiring massive com-\\nputational resources.\\nTo mitigate dataset limitations, we employ:\\n• Pretrained Transformer Models (e.g., BLIP, GIT) to lever-\\nage larger datasets.\\n• Data Augmentation techniques to artificially increase cap-\\ntion diversity.\\n• Regularization & Fine-tuning to reduce overfitting risks.\\n9 Results\\nThis section presents the experimental evaluations of the proposed\\nCNN-LSTM-based image captioning model. We benchmarked our\\nmodel against the state-of-the-art BLIP model to assess both gener-\\native quality and factual grounding. Additionally, we analyzed the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 5\\neffect of hallucination filtering on caption accuracy using YOLOv5\\nobject detection.\\n9.1 Evaluation Setup\\nWe used a subset of theFlickr8k dataset comprising 6,000 training\\nand 2,000 validation images. Each image has five corresponding\\ncaptions. The following metrics were used to quantitatively evaluate\\ncaptioning quality:\\n• BLEU (1 to 4): N-gram precision.\\n• METEOR: Semantic matching with synonyms and stems.\\n• ROUGE-L: Longest common subsequence overlap.\\n• CIDEr: Consensus-based evaluation (ineffective on small\\ndatasets).\\nThe models compared:\\n• Baseline: CNN-LSTM using DenseNet201 features.\\n• Transformer-based: BLIP (Bootstrapped Language-Image\\nPretraining).\\n9.2 Training and Validation Loss Analysis\\nFigure 2 presents the training and validation loss curves observed\\nduring model training. Initially, both losses decrease steadily, in-\\ndicating that the model is learning effective representations from\\nthe image-caption pairs. After around the 5th epoch, the validation\\nloss begins to plateau while the training loss continues to decline,\\nsuggesting the model is starting to slightly overfit the training data.\\nDespite this, the gap between training and validation loss remains\\nsmall, which indicates good generalization. The trend suggests that\\nour custom data generator and regularization techniques, such as\\ndropout, are effective in mitigating overfitting. This learning behav-\\nior supports the robustness of the training setup and highlights the\\nmodel’s ability to capture the underlying semantics required for\\ngenerating meaningful image captions.\\n9.3 Feature Representation Analysis\\nTo evaluate how well our feature extractor (DenseNet201) captures\\nthe semantic diversity in images, we employ t-SNE (t-Distributed\\nStochastic Neighbor Embedding) for dimensionality reduction and\\nvisualization. This technique allows us to project high-dimensional\\nfeature vectors into a 2D plane, revealing patterns and clusters.\\n• Figure 3 displays the image-level t-SNE representation clus-\\ntered using KMeans. This provides qualitative evidence that\\nsimilar image types group together in the feature space.\\nThese plots confirm that DenseNet201-derived features retain\\nmeaningful distinctions necessary for downstream caption genera-\\ntion.\\nFig. 3. t-SNE + KMeans cluster visualization of image features with thumb-\\nnails from the Flickr8k dataset. Similar images tend to group together,\\nvalidating our DenseNet feature extractor.\\n9.4 Qualitative Comparison of Captions\\nTo gain a more intuitive understanding of model behavior, we\\npresent side-by-side comparisons of image captions generated by\\nDenseNet and BLIP. Each example includes the original image, the\\nground-truth caption, predictions by both models, and their respec-\\ntive CLIP similarity scores. This helps assess factual grounding and\\nsemantic accuracy.\\n9.5 Quantitative Results\\nThese examples demonstrate BLIP’s ability to generate semantically\\nrich and grounded captions compared to DenseNet, particularly in\\ncomplex scenes involving multiple objects or fine-grained relation-\\nships.\\nTable 1. Average Metric Comparison Across 100 Test Samples\\nMetric DenseNet Model BLIP Model\\nBLEU-1 0.2078 0.1512\\nBLEU-2 0.0936 0.0778\\nBLEU-3 0.0519 0.0055\\nBLEU-4 0.0319 0.0000\\nMETEOR 0.1231 0.1165\\nROUGE-L 0.2654 0.2479\\nCIDEr 0.0000 0.0000\\nInsight: Despite BLIP’s advanced architecture, the CNN-LSTM\\nmodel showed stronger performance on n-gram-based metrics due\\nto better generalization on smaller datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='6 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\nFig. 2. Training vs. Validation Loss and BLEU Score Comparison Before and After Hallucination Filtering. The left plot shows the convergence of training\\nand validation loss, while the table on the right highlights the drop in BLEU scores after filtering hallucinated tokens using YOLOv5-detected objects. This\\nvalidates that the original model tends to hallucinate terms not grounded in visual input.\\nFig. 4. Example: BLIP generates a more semantically rich caption (\"snowy\\nslope\") and yields a significantly higher CLIP score than DenseNet, reflecting\\nimproved alignment with the visual content.\\n9.6 BLEU Score Comparison\\nTable 2. BLEU-1 Score Comparison (Sample Batch)\\nModel BLEU-1 Score\\nDenseNet 0.2284\\nBLIP 0.1751\\n9.7 Hallucination Filtering with YOLOv5\\nWe employed YOLOv5 object detection to detect visual elements\\nand filter out non-grounded words from the generated captions.\\nFig. 5. Example: While both models identify the dog, DenseNet overlooks\\nthe muzzle detail present in the ground truth. BLIP provides a more contex-\\ntually accurate description but still misses that specificity.\\nTable 3. BLEU Score Before and After Hallucination Filtering\\nFiltering Stage BLEU-1 BLEU-2\\nBefore 0.1814 0.1016\\nAfter 0.0313 0.0132\\nObservation: The substantial drop in BLEU scores post-filtering\\nconfirms the presence of hallucinated tokens that were not visually\\ngrounded.\\n9.8 Qualitative Results\\nWe visualized predictions of both models for randomly selected im-\\nages. DenseNet-based captions appeared more factual, while BLIP’s'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 7\\nFig. 6. Example: BLIP correctly identifies the main subjects—a lion and\\na buffalo—demonstrating its grounding strength. DenseNet, on the other\\nhand, misidentifies the context entirely.\\nFig. 7. Example: While both models recognize a running dog, only the\\nground truth acknowledges the muzzle, which is missed by BLIP and\\nDenseNet. The omission lowers their CLIP scores, highlighting the challenge\\nin object-level grounding.\\nwere descriptive but often hallucinated objects not present in the\\nimage.\\n9.9 Summary of Findings\\n• CNN-LSTM outperforms BLIP in BLEU and ROUGE metrics\\non Flickr8k.\\n• Hallucination is a serious issue in transformer-based models\\nunder small data regimes.\\n• YOLOv5-based hallucination filtering helped us identify se-\\nmantic inconsistencies.\\n10 Conclusion\\nIn this project, we did the job of automatic image captioning us-\\ning traditional and state-of-the-art approaches. We developed a\\nCNN-LSTM model where DenseNet201 was used as a visual feature\\nextractor and a custom LSTM-based decoder to decode text descrip-\\ntions. In addition to this, we also compared this baseline with recent\\ntransformer-based vision-language models like BLIP.\\nExperiments, conducted on the Flickr8k dataset, reveal that transformer-\\nbased models like BLIP outperform traditional CNN-LSTM architec-\\ntures on qualitative and quantitative metrics like BLEU scores. BLIP\\nuse led to more fluent, varied, and contextually appropriate captions.\\nAside from this, we also incorporated hallucination detection from\\nrecent literature and assessed whether the generated captions were\\ngrounded in visual content.\\nThe project illustrates the increasing capability of pre-trained\\nVLMs to generate high-quality captions, especially when fine-tuned\\non heterogeneous datasets. Training and generalization, however,\\nremain sensitive to dataset size and feature quality. We also observed\\nthat lighter-weight models like CNN-LSTM, though less expressive,\\ncan be used in resource-constrained environments.\\nIn the future, we will:\\n• Incorporate CLIP-based benchmarking for better alignment\\nof image-text embeddings.\\n• Fine-tune BLIP and other vision-language models on domain-\\nspecific caption datasets.\\n• Experiment with multimodal grounding techniques to re-\\nduce hallucinations and improve factual consistency even\\nmore.\\n• Scale to larger datasets like MS-COCO or NoCaps to verify\\ngeneralization.\\nOverall, our project provides insights into how generation quality\\nand visual semantic alignment differ between conventional and\\ntransformer-based frameworks.\\n11 Individual Contributions\\n11.1 Vishal\\nVishal led the data preprocessing and image feature extraction\\npipeline. He implemented techniques to resize, normalize, and clean\\nimage-caption pairs effectively. He integrated DenseNet201 as a\\npretrained CNN for extracting high-dimensional feature vectors,\\nstored efficiently for reuse. Vishal also managed aspects of caption\\ntokenization and ensured compatibility between the image and text\\nmodalities during training.\\n11.2 Avipsa\\nAvipsa focused on model architecture development and com-\\nparison. She implemented the CNN-LSTM-based caption gener-\\nation pipeline and worked on training configurations such as hy-\\nperparameter tuning, dropout strategies, and callbacks . She\\nalso integrated BLIP, a transformer-based model, for captioning\\ncomparison, and generated visualizations for model output, loss\\ntrends, and BLEU-based evaluations. Avipsa further contributed to\\nhallucination filtering using YOLOv5 object detection, addressing\\ngrounding issues as cited in recent literature.\\n11.3 Chanchal\\nChanchal handled evaluation, visualization, and benchmarking.\\nShe performed metric analysis using BLEU, METEOR, ROUGE,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='8 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\nand CIDEr , and supported hallucination evaluation using pre-\\nand post-filter BLEU scores. She contributed to generating t-SNE\\nplots and clustering visualizations to interpret model embed-\\ndings. Chanchal also began exploring CLIP-based benchmarks\\nand worked on preparing comprehensive documentation for final\\nreporting and presentation.\\n12 REFERENCES\\nReferences\\n[1] Vinyals et al., Show and Tell: A Neural Image Caption Generator , CVPR 2015.\\nhttps://arxiv.org/abs/1411.4555\\n[2] Xu et al., Show, Attend and Tell, ICML 2015. https://arxiv.org/abs/1502.03044\\n[3] Anderson et al., Bottom-Up and Top-Down Attention , CVPR 2018. https://arxiv.\\norg/abs/1707.07998\\n[4] Dognin et al., Adversarial Semantic Alignment , CVPR 2019. https:\\n//openaccess.thecvf.com/content_CVPR_2019/papers/Dognin_Adversarial_\\nSemantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.\\ncom/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-\\nFidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf\\n[6] Li et al., BLIP, ICML 2022. https://proceedings.mlr.press/v162/li22n.html\\n[7] CVPR 2024. https://arxiv.org/abs/2404.02904\\n[8] Biten et al., Let There Be a Clock on the Beach , WACV 2022. https://arxiv.org/abs/\\n2110.01705\\n[9] EMNLP 2024. https://arxiv.org/abs/2406.14492\\n[10] NeurIPS 2023. https://arxiv.org/abs/2306.07915\\n[11] Li et al., EVCap, CVPR 2024. https://arxiv.org/abs/2311.15879\\n[12] NeurIPS 2024. https://papers.nips.cc/paper_files/paper/2024/hash/\\nd75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html\\n[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.\\n[15] Hyperbolic Learning with Synthetic Captions , CVPR 2024.\\nProject Repository\\nThe complete code and resources for this project can be found on\\nGitHub:\\nhttps://github.com/Avipsa-Bhujabal/Image𝐶𝑎𝑝𝑡𝑖𝑜𝑛𝑖𝑛𝑔 𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dac224",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0573a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 16 documents into 85 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/379933077\n",
      "Improving Earth Observations by correlating Multiple Satellite Data: A\n",
      "Comparati...\n",
      "Metadata: {'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/379933077\\nImproving Earth Observations by correlating Multiple Satellite Data: A\\nComparative Analysis of Landsat, MODIS and Sentinel Satellite Data for Flood\\nMapping\\nConference Paper · February 2024\\nDOI: 10.23919/INDIACom61295.2024.10498948\\nCITATIONS\\n7\\nREADS\\n209\\n9 authors, including:\\nSonali Kadam\\nBharati Vidyapeeth College of Engineering for Women\\n54 PUBLICATIONS\\xa0\\xa0\\xa0123 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAnjali Kadam\\nBharati Vidyapeeth's College of Engineering for Women, Pune,India\\n22 PUBLICATIONS\\xa0\\xa0\\xa023 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAhilya Bandgar\\nBharati Vidyapeeth's College Of Engineering for Women\\n4 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nRavindra V. Kale\\nNational Institute of Hydrology\\n45 PUBLICATIONS\\xa0\\xa0\\xa0341 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Anjali Kadam on 27 June 2024.\\nThe user has requested enhancement of the downloaded file.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\nBharati Vidyapeeth's Institute of Computer Applications and Management (BVICAM), New Delhi (INDIA)  \\n \\n \\nImproving earth observations by correlating multiple \\nsatellite data: A Comparative Analysis of Landsat, \\nMODIS, and Sentinel Satellite Data for flood \\nmapping \\nSonali Kadam \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nsonali.kadam@bharatividyapeeth.edu \\nAnjali Kadam \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nAnjali.kadam@bharatividyapeeth.edu \\nPrakash Devale \\nDepartment of Information Technology \\nBharati Vidyapeeth (Deemed to be) \\nUniversity College of Engineering \\nPune, Maharashtra \\nprdevale@bvucoep.edu.in \\nAhilya Bandgar \\nDepartment of Computer Engineering\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Bharati Vidyapeeth (Deemed to be) \\nUniversity College of Engineering \\nPune, Maharashtra \\nprdevale@bvucoep.edu.in \\nAhilya Bandgar \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nbandgarahilya@gmail.com \\nRajlaxmi Manepatil \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nmanepatilrajlaxmi@gmail.com \\n \\nRavindra Kale  \\nNational Institute of Hydrology \\nRoorkee, Uttarakhand, India \\nravikale.nihr@gmail.com \\nJotiram Gujar \\nDepartment of Chemical Engineering \\nSinhgad College of Engineering  \\nPune, Maharashtra \\njotiramgujar@gmail.com \\n \\nChanchal Bundele \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nchanchalbundele04@gmail.com \\n \\nTanmayi Chavan  \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of  \\nEngineering for Women  \\nPune, Maharashtra \\ntanmayichavan0502@gmail.com'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='chanchalbundele04@gmail.com \\n \\nTanmayi Chavan  \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of  \\nEngineering for Women  \\nPune, Maharashtra \\ntanmayichavan0502@gmail.com \\n  \\n \\nAbstract— Flooding is a recurring and severe calamity, \\naggravated by climate change and rapid urbanization, resulting \\nin substantial loss of life, property, and economic disruption. To \\naddress this critical issue, a comprehensive study focuses on \\ndelineating flood-prone zones using satellite data from Landsat, \\nMODIS, and Sentinel sources. Geographic Information System \\n(GIS) technology w ith Google Earth Engine (GEE) platform, \\nproviding an environment which is based on the cloud for \\nanalysis of geospatial data. The methodology encompasses data \\ncollection, preprocessing, and the application of algorithms for \\nflood mapping and depth estimati on. By assessing flood extents \\nand depths, it contributes to informed decision- making for flood'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='collection, preprocessing, and the application of algorithms for \\nflood mapping and depth estimati on. By assessing flood extents \\nand depths, it contributes to informed decision- making for flood \\nmanagement and disaster preparedness. Researchers have made \\nprominent efforts to use these satellite data in the individual \\nmanner or using techniques like fusi on to combine two different \\nsatellite data. The author has made an attempt to design a \\nplatform where all three- satellite data would be available \\ntogether so that valuable insights are drawn in the field of flood \\nmapping. This paper is prominently focused to corelate those \\nsatellite data with the help of difference mapping. \\nKeywords— Landsat, MODIS, Sentinel, GIS, flood extent mapping, \\nflood depth estimation. \\nI. INTRODUCTION \\nA flooding event can be described as the non- permanent \\ninundation of water that submer ges usual arid regions in any \\ngeographical area. Flooding resulting from rapid snowmelt,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='I. INTRODUCTION \\nA flooding event can be described as the non- permanent \\ninundation of water that submer ges usual arid regions in any \\ngeographical area. Flooding resulting from rapid snowmelt, \\nstorm surges impacting inland regions from the sea, persistent \\nand intense rainfall during the monsoon period, and the \\ndestruction of dams, embankments, or levees in t he season of \\nhigh winds and heavy rains [1].Deforestation has results in, the \\nincreased frequency of coastal storms can rise because of \\nabrupt changes in land use, inappropriate management of urban \\nstormwater runoffs, and climate change. In recent times, t here \\nhas been a notable increase in the frequency of global flooding \\nevents [2]. Flash floods and the more common river floods are \\nthe two main categories of flooding.River floods usually cause \\nmore property damage, but flash floods usually cause more \\nfatalities [2]. \\nSatellite data is very informative way to extract the images of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='more property damage, but flash floods usually cause more \\nfatalities [2]. \\nSatellite data is very informative way to extract the images of \\nthe particular area in an efficient manner [22]. Launched in \\nJuly 1972, Landsat 1 was the first digital spaceborne sensor \\ndeveloped for monitoring the terrestrial environment, NASA is \\nin charge of designing and launching the Landsat satellites and \\nsensors into near -polar low earth orbit [14]. The U.S. \\nGeological Survey (USGS) is in charge of flight operations, \\narchiving, ground processing, and distribution, and an affiliated'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\nLandsat Science Team performs scientific and technical \\nevaluation, Landsat has a highest series of missions (landsat -\\n1,2,3,4,5,7,8,9) [10, 14]. Earth Observations entered a new era \\nwith the launch of the Earth Observing System (EOS) Terra \\nsatellite in December 1999 and the Aqua satellite in May 2002. \\nThe NASA Ocean Biology Processing Group (OBPG) and \\nother science teams contributed to these satellite launches [12,       \\n13]. Sentinel -1 (S1) and Sentinel-2 (S2)  which are  \\nmultispectral optical instruments of the Europ ean Space \\nAgency were launched in 2014 and 2017 respectively [15, 16]. \\nIn this research paper, a concerted effort is undertaken to \\nintegrate the capabilities of three prominent Earth observation \\nsatellites – namely, LANDSAT, MODIS, and SENTINEL –'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='In this research paper, a concerted effort is undertaken to \\nintegrate the capabilities of three prominent Earth observation \\nsatellites – namely, LANDSAT, MODIS, and SENTINEL –  \\nwithin a unified platform utilizing Geographic Information \\nSystem (GIS) tools. The primary objective is to harness the \\nsynergies offered by these satellite systems to deliver efficient \\nand timely flood difference maps. By amalgamating data from \\nmultiple sources, th is approach aims to enhance the precision \\nand reliability of flood monitoring, providing researchers and \\nend-users with prompt access to valuable information. \\nUltimately, this paper envisions a comprehensive and user -\\nfriendly solution that not only leverag es the strengths of \\nmultiple satellite systems but also harnesses the power of GIS \\ntools to deliver swift and reliable flood difference maps. The \\nintegration of LANDSAT, MODIS, and SENTINEL within a \\nunified platform represents a noteworthy advancement in t he'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='tools to deliver swift and reliable flood difference maps. The \\nintegration of LANDSAT, MODIS, and SENTINEL within a \\nunified platform represents a noteworthy advancement in t he \\nfield of remote sensing and geographic information science, \\noffering a valuable resource for those involved in flood \\nresearch and mitigation efforts. \\nII. L\\nITERATURE REVIEW \\nStochastic approaches to flood control encompass a variety of \\nanalytical methods aime d at understanding and managing the \\nuncertainties associated with flooding events. Among these \\napproaches are Monte Carlo Markov Chain Algorithms \\n(MCMC) [3], which leverage probabilistic algorithms to \\nsimulate and sample from probability distributions, ena bling \\nthe assessment of various flood scenarios. Artificial Neural \\nNetwork (ANN)-based predictive models [4] represent another \\nfacet of stochastic analysis, utilizing machine learning \\ntechniques to predict river discharge and other relevant'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Network (ANN)-based predictive models [4] represent another \\nfacet of stochastic analysis, utilizing machine learning \\ntechniques to predict river discharge and other relevant \\nparameters base d on historical data. Conventional modelling \\nschemes [5] in this context typically refer to traditional, \\ndeterministic models used in hydrology and hydraulics  [6] to \\nsimulate the behaviour of rivers and water systems. The Index -\\nFlood Method  [7] involves st atistical analysis of historical \\nflood data to estimate the probability of floods of different \\nmagnitudes occurring, contributing to a more comprehensive \\nunderstanding of flood risk. \\nOn the other hand, Geographic Information System (GIS) \\ncalculations, part icularly those conducted using tools like \\nGoogle Earth Engine (GEE), provide a spatial perspective in \\nflood control strategies.GIS focuses on the analysis of spatial \\nand geographic data  [24], allowing for the mapping and'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Google Earth Engine (GEE), provide a spatial perspective in \\nflood control strategies.GIS focuses on the analysis of spatial \\nand geographic data  [24], allowing for the mapping and \\nassessment of physical characterist ics of the terrain and land \\nuse. GEE, being a cloud-based platform, facilitates the efficient \\nprocessing and analysis of large -scale [26] Earth observation \\ndata. While stochastic approaches delve into the predictive \\nmodelling of floods, GIS calculations [8] emphasize the spatial \\naspects of flood -prone areas, considering factors such as \\ntopography, land cover, and infrastructure that influence the \\ndynamics of flooding events. The satellite data provides \\nrequired data over long period of time [10], also the geo spatial \\nmethodologies [22] have made a significant contribution for the \\nflood [11], disaster analysis [23]. \\nWith the GEE computing platform, you can do away with the \\nneed for downloading, preprocessing, and a powerful'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='flood [11], disaster analysis [23]. \\nWith the GEE computing platform, you can do away with the \\nneed for downloading, preprocessing, and a powerful \\ncomputing environment all of which are  typically associated \\nwith using remotely sensed data. GEE is an adaptable and \\ntransparent platform that can handle a wide range of research \\nareas, from forestry to crop mapping to drought monitoring, \\neven though it is optimized for big data. Land use land \\ncoverchange monitoring is made more dependable and \\neffective by its ability to leverage cloud computing resources \\nand access a wealth of multisource datasets [17, 18].In addition \\nto enabling the integration of multi-source and  sensing images, \\nthe rise an d development of satellite remote sensing , cloud \\nstorage, exemplified by GEEalso making full-band image \\ncomputing possible [18]. \\nIII. \\nDATASET USED \\nAs mentioned, we have used LANSAT, MODIS and \\nSNTINEL satellites to perform our analysis in our study.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"computing possible [18]. \\nIII. \\nDATASET USED \\nAs mentioned, we have used LANSAT, MODIS and \\nSNTINEL satellites to perform our analysis in our study. \\nBelow is a list of these satellites' specific attributes. \\nTABLE I. CHARACTERISTICS OF LANDSAT SATELLITE  \\nSatellite Sensor Launc\\nhed \\nYear \\nBands \\nLandsat - 1 MSS 1972 Green, Red, NIR, IR \\nLandsat - 2 MSS 1975 Green, Red, Near-Infrared, \\nInfrared \\nLandsat - 3 RBV 1978 Blue, Green, Red \\nLandsat - 4 TM 1982 TIR, MIR, NIR \\nLandsat - 5 TM 1984 TIR, MIR, NIR, and visible \\nLandsat - 6 TM 1993 TIR, MIR, NIR, and visible \\nLandsat - 7 ETM+ 1999 Evident, TIR, panchromatic, \\nNIR, MIR \\nLandsat - 8 OLI/TIRS 2013 Visible, Panchromatic, SWI, \\nNIR, and TIR \\nLandsat - 9 OLI/TI RS 2021 Visible NR, Visible Blue, \\nVG, RP, SWIR 1, SWIR 2, \\nand Coastal Aerosol \\n  \\nTable I shows the characteristics of Landsat data. The spatial \\nresolution of the Green, Red, and Near Infrared bands on \\nLandsat 1–5 is 60 meters when using the multispectral scanner\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Table I shows the characteristics of Landsat data. The spatial \\nresolution of the Green, Red, and Near Infrared bands on \\nLandsat 1–5 is 60 meters when using the multispectral scanner \\n(MSS). The thermal band in Landsat 4 -5 has a spatial \\nresolution of 120( 17) meters, while the blue, green, red, near \\ninfrared, and SWIR -1/2 bands have a spatial resolution of 30 \\nmeters using Thematic Mapper(TM) [8, 19]. \\n The thermal and panchromatic bands in Landsat 7 have a \\nspatial resolution of 15 and 30 meters, respectively, while the \\nblue, green, red, and NIR SWIR -1/2 bands have a spatial \\nresolution of 30 meters.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\nThe Coastal Aerosol, Blue, Gr een Red, NIR, SWIR- 1/2, and \\nCirrus have a resolution of 30 meters in Landsat 8 -9 using \\nOperational Land Imager and Thermal Infrared Sensors; the \\nPanchromatic band has a resolution of 15 meters, and the \\nTIRS-1/2 has a resolution of 100 meters [19]. \\nTABLE II. Characteristics of MODIS Satellite \\nSatellite Sensor Launch \\nYear \\nBands \\nTerra  MODIS 1999 36 spectral bands that span \\nthe visible to thermal \\ninfrared portions of the \\nelectromagnetic spectrum. \\nAqua MODIS 2002 Like Terra MODIS, it has 36 \\nspectral bands that allow for \\ndetailed observations of the \\nEarth's surface and \\natmosphere. \\n  \\nThe spatial resolution of the Aqua and Terra satellites is \\nidentical; bands 1 and 2 have a 250 m spatial resolution, bands \\n3-7 have a 500 m spatial resolution, and bands 8 -36 have a\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='The spatial resolution of the Aqua and Terra satellites is \\nidentical; bands 1 and 2 have a 250 m spatial resolution, bands \\n3-7 have a 500 m spatial resolution, and bands 8 -36 have a \\n1000 m spatial resolution, respectively [20]. Table 2 illustrates \\nthe characteristics of MODIS satellite [9]. \\nTABLE III.            Characteristics of Sentinel Satellite  \\nSatellite Sensor Launc\\nh Year \\nBands \\nSentinel - 1 C- band \\nSAR \\n1-A: \\n2014 \\n1-B: \\n2016 \\nThe single SAR sensor \\ncarried by Sentinel -1 can \\nfunction in multiple \\npolarizations, such as HH \\nand VV. \\nSentinel - 2 MSI 2017 A multi-spectral imager with \\n13 spectral bands, including \\nvisible, NIR and SWIR \\nwavelengths, is carried by \\nSentinel-2A and Sentinel -\\n2B. \\nSentinel - 3 OLC/SL\\nSTR/SA\\nRA \\n2018 Sentinel-3A and Sentinel -3B \\nare equipped with multiple \\nspectral bands to measure \\nland surface temperature and \\nocean color. \\n \\nSentinel-1 has four operational modes, SM mode features'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='RA \\n2018 Sentinel-3A and Sentinel -3B \\nare equipped with multiple \\nspectral bands to measure \\nland surface temperature and \\nocean color. \\n \\nSentinel-1 has four operational modes, SM mode features \\n5X5 metre, IW features 5X20 metre, EW features 20X40 metre \\nand WV mode features 5X5 metre of spatial resolution. \\nThe Sentinel-2 has a spatial resolution of 10 m for the Blue, \\nGreen, Red, and NIR bands; 20 m for the vegetation red edge, \\nNIR, and band -11/12 for the SWIR bands; and 60 m eters for \\nthe coastal aerosol, water vapour, and SWIR Cirrus bands. \\nIn Sentinel-3 as per SLSTR the initial six spectral bands has \\nthe VNIR along with SWIR, in VNIR the band 1 -3 and in \\nSWIR the bands 4 -6 have a spatial resolution of 500 meters  \\n[10]. \\nIV. METHODOLOGY \\nFig. 1 shows the methodology for generating a hazard map, \\nusing the combination of Landsat, MODIS and Sentinel \\nmethodologies together. The method for each satellite is briefly \\nexplained below.  \\nA. Methods For Sentinel'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"using the combination of Landsat, MODIS and Sentinel \\nmethodologies together. The method for each satellite is briefly \\nexplained below.  \\nA. Methods For Sentinel \\n The following steps illustrateflood depth and flood area \\ncalculations of Sentinel dataset using GEE. \\n• Gathering data, preparing it for processing, and \\nloading the COPERNICUS/S1_GRD satellite dataset \\nand data of study area into the GEE Code Editor – \\nLoad the dataset of COPERNIC US/S1_GRD along \\nwith the river basin dataset. Next, filter the collection \\nto exclusively contain images with the \\n'instrumentMode' property set to 'IW'. Subsequently, \\nnarrow down the collection even further to \\nencompass only those images where the \\n'transmitterReceiverPolarisation' property contains \\n'VV'. Following this, apply a spatial filter to the \\ncollection, ensuring it comprises only images that \\nintersect with a specified area of interest (AOI). \\nLastly, concluded by selecting solely the 'VV' band\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='collection, ensuring it comprises only images that \\nintersect with a specified area of interest (AOI). \\nLastly, concluded by selecting solely the \\'VV\\' band \\nfrom each image within the filtered collection. \\n \\n• Mosaic and clip satellite data – \\nIn this, an Earth Engine Image Collection is subjected \\nto two different filtering methods. One which creates \\na composite image that represents the \"before\" flood \\nimages, from the collection that falls between the \\ngiven dates are first filtered. Then, they are combined \\ninto one image using the ‘mosaic\\' function. Similar to \\nthe first filtering process, a second filtering operation \\nis performed to choose images according to the flood \\noccurred dates. These images are likewise selected, \\nand they are then combined using ‘mosaic\\' to create a \\ncomposite image that represents the \"after\" flood \\nimagery.  \\n• Apply Smoothening Filter– \\nA smoothing filter is applied to two previously \\nclipped images. First, the before image, which has'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='composite image that represents the \"after\" flood \\nimagery.  \\n• Apply Smoothening Filter– \\nA smoothing filter is applied to two previously \\nclipped images. First, the before image, which has \\nbeen clipped and is subjected to a focal m edian filter \\nwith a radius of 30 meters and a circular \\nneighbourhood, resulting in a smoothed version. \\nSimilarly, smoothening of after image is carried out. \\n \\n• Calculating Flood extent-  \\nThe smoothed before image is subtracted from the \\nsmoothed after image i n a difference operation to get \\nan image that shows the difference between the two \\ntimes. A binary mask picture is then created by \\napplying a threshold to the difference image, where \\npixels with values less than - 3 are thought to indicate \\nthe extent of a f lood. In order to effectively mask out \\nall non-flood areas and produce a final binary image \\nthat highlights the flooded parts, the flood extent \\nmask is applied to itself using the \\'updateMask\\' \\nfunction.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\n \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\ndata, if available, can be loaded to determine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It then \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the   ability to add \\nadditional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts  \\noutlier identification. It uses a cumulative cost \\nmethod to determine the floodwater surface elevation \\nmodel, then uses a low -pass filter to determine the \\nfloodwater depth and smooth it out. Regular water \\nbodies and 0 values are excluded using optional \\nmasking processes.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='model, then uses a low -pass filter to determine the \\nfloodwater depth and smooth it out. Regular water \\nbodies and 0 values are excluded using optional \\nmasking processes.  \\nB. Methods For Landsat \\nThe following steps illustrateflood depth and flood area \\ncalculations of Landsat dataset using GEE. \\n• Gathering information, preprocessing it, and loading \\nthe LANDSAT/LC08/C02/T1_RT satellite dataset \\nand study area data into the GEE Code Editor – \\nA specific area of interest (AOI) corresponding to the \\nriver basin is selected from a table and added to the \\nmap for visualization. Then, a Landsat image \\ncollection is filtered to include only images from the \\ndate ranging within the b ounds of the previously \\ndefined AOI. The images are mosaicked into a single \\ncomposite, and the result is clipped to the same AOI. \\nTwo bands, B5 and B3, are selected from this dataset. \\nFor each band, visualization parameters are set to \\nspecify the minimum a nd maximum values for'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Two bands, B5 and B3, are selected from this dataset. \\nFor each band, visualization parameters are set to \\nspecify the minimum a nd maximum values for \\ndisplay within the specified range of 0.0 to 30,000.0. \\n• Visualizing NDWI - \\nTo visualize NDWI is carried out by subtracting \\nBand B2 from Band B1 and then dividing the result \\nby the sum of Band B1 and Band B2. Visualization \\nparameters ar e set with a minimum value of -1, a \\nmaximum value of 1, and a colour palette ranging \\nfrom white (for values less than -1), through red, to \\nblue (for values greater than 1). Finally, the NDWI \\nlayer is added into the map with the specified \\nvisualization settings which allows to visualize water \\nbodies and their intensity within the selected dataset. \\n• Calculating Flooded Area-  \\nA binary mask is made to identify the blue region in \\nthe NDWI (Normalized Difference Water Index) \\nlayer in order to determine the floode d area. The \\nNDWI \\nvalues between 0 and 1, which often depict aquatic'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='the NDWI (Normalized Difference Water Index) \\nlayer in order to determine the floode d area. The \\nNDWI \\nvalues between 0 and 1, which often depict aquatic \\nbodies, fall into this blue zone. By determining \\nwhether NDWI values are larger than 0 and less than \\n1, the binary mask is produced. To determine the \\nflooded area within each pixel, this m ask is then \\nmultiplied by the pixel area image. This procedure \\ngenerates a \"floodedAreaImage\" that enables the \\nquantification of flooded areas within the dataset. \\nEach pixel value reflects the area of the flooded zone \\nin square meters. \\n• Total Flooded Area Calculation – \\nThe entire flooded area is determined in square \\nmetres. First, a region reduction of the \\n\\'floodedAreaImage\\' is performed using the sum \\nreducer within the defined area of interest (AOI) \\ngeometry. To properly manage big datasets, a scale of \\n30 m eters, which corresponds to the resolution of \\nLandsat pictures, is established. The entire area that'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='geometry. To properly manage big datasets, a scale of \\n30 m eters, which corresponds to the resolution of \\nLandsat pictures, is established. The entire area that \\nhas been inundated is then calculated in square \\nmeters. The result, which represents the entire \\nflooded area in square kilometres, is displayed to the \\nconsole for reference after the value is divided by 1e6 \\nto show the result in square kilometres. \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\nFig. 1.  Methodology for Calculating Hazard Map.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\ndata, if available, can be loaded to d etermine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It then \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the ability to add \\nadditional water bodies to the flood  extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a \\ncumulative cost method to determine the floodwater \\nsurface elevation model, then uses a low -pass filter to \\ndetermine the floodwater depth and s mooth it out. \\nRegular water bodies and 0 values are excluded using \\noptional masking processes.  \\nC. Methods For Modis \\nThe following steps illustrate flood depth and flood area \\ncalculations of  MODIS dataset using GEE.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"optional masking processes.  \\nC. Methods For Modis \\nThe following steps illustrate flood depth and flood area \\ncalculations of  MODIS dataset using GEE. \\n• Gathering data, preprocessing it, and l oading the \\nStudy Area data and Satellite dataset \\n(MODIS/006/MOD09A1) – \\nThe area of interest (AOI) corresponding to the \\ntargeted river basin which is selected from the table \\nand added to the map for visualization. Next, a \\nMODIS Image Collection is filtered to include \\nimages from the range of flood occurred dates, within \\nthe bounds of the specified AOI. The images are then \\nmosaicked where in this case, the median value is \\nused to creating a composite image of the selected \\ndate, and the result is clipped to th e AOI. Two bands, \\n'sur_refl_b02' and 'sur_refl_b06', are selected from \\nthis dataset. Visualization parameters are set to define \\nthe display range for each band, and both b02 and \\nb06 bands are added as layers to the map, allowing\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"this dataset. Visualization parameters are set to define \\nthe display range for each band, and both b02 and \\nb06 bands are added as layers to the map, allowing \\nfor the visualization of MODIS data for the AOI river \\nbasin. \\n \\n• NDWI Calculation and Flooded Area Estimation - \\n'sur_refl_b02' (b02) and'sur_refl_b06' (b06) MODIS \\nbands are used to calculate the NDWI. An 'NDWI' \\nlayer is produced by computing the NDWI by \\ndeducting the near -infrared values from the green \\nvalues and dividing the result by the sum of b02 and \\nb06. This NDWI layer is added to the map, and the \\nparameters are set to display NDWI values in the \\nrange of - 1 to 1 with a colour palette ranging from \\nwhite to red to blue. Furtherm ore, a binary mask with \\nvalues between 0 and 1 is made for the blue region of \\nthe NDWI. Potential water bodies are identified by \\nthis 'blueRegionMask'. The pixel area image is then \\nmultiplied by this mask to enable the estimation of\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"the NDWI. Potential water bodies are identified by \\nthis 'blueRegionMask'. The pixel area image is then \\nmultiplied by this mask to enable the estimation of \\nthe flooded area in each pixel, which can be valuable \\nfor flood analysis and mapping. \\n \\n• Total Flooded Area Calculation – \\nThere is a calculation of the total flooded area which \\nis calculated in square meters and is the total area \\nsubmerged by flooding. Using the sum reducer, a \\nregion reduction operation is performed on the \\n'floodedAreaImage,' which reflects the extent of \\nflooded regions within each pixel. This reduction is \\ncarried out inside the boundaries of the area of \\ninterest (AOI) that was previously specified. To \\nmanage huge datasets effectively, a specified scale of \\n30 meters, which corresponds to the resolution of the \\nMODIS data, is chosen (it can be changed as \\nnecessary). Additionally, a maximum pixel limit of 1 \\nbillion pixels is determined. Calculated results can be\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='MODIS data, is chosen (it can be changed as \\nnecessary). Additionally, a maximum pixel limit of 1 \\nbillion pixels is determined. Calculated results can be \\nfurther processed or displayed as needed for flood \\nassessment and analysis. The result, which represents \\nthe entire flooded area in square kilometres, is \\ndisplayed to the console for reference after the value \\nis divided by 1e6 to show the result in square \\nkilometres. \\n \\n• Calculating Flood Depth – \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\ndata, if available, can be loaded to determine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It the n \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the ability to add \\nadditional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='additional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a \\ncumulative cost method to determine the floodwater \\nsurface elevation model, then uses a low -pass filter to \\ndetermine the floodwater depth and smooth it out. \\nRegular water bodies and 0 values are excluded using \\noptional masking processes.  \\nV. R\\nESULTS AND DISCUSSION \\n    The proposed study evaluates the flood extent maps during \\nthe period of 2019 August to September in Mahanadi River \\nbasin. For this evaluation, calculated flood extent maps for the \\nindividual satellite data using above mentioned methods. Fig.  \\n2 depicts the flood extend map of sentinel data calculated by \\nadjusting the threshold-value.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\n \\nFig. 2.  Flood extent map using Sentinel data. \\nFig. 3 depicts the flood extent map of Landsat data, featuring \\nNDWI with bands B3 and B5. \\n \\nFig. 3. Flood extent map using Landsat data. \\nFig. 4 depicts the flood extent map of Modis data, with the \\nhelp of binary flood mask using band sur_refl_b0 1 and \\nsur_refl_b02. \\n \\nFig. 4. Flood extent map using Modis data. \\nThe above Fig. 4 illustrate the flood extent areas with \\nvariations in their values. The results obtained out of flood \\nextent maps are not that efficient to refer them for further \\nstudies. In order to refer a correct flood extent area, we need to \\napply certain techniques and methods to get an accurate flood \\nextent area. In this study an attempt is made to get an accurate'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='studies. In order to refer a correct flood extent area, we need to \\napply certain techniques and methods to get an accurate flood \\nextent area. In this study an attempt is made to get an accurate \\nflood extent area using the concept of difference mapping. To \\ngenerate a difference map, individual flood extent map are \\nused and the bands  of interest from both sensors are \\nsubtracted. The pixel -wise difference is calculated and the \\nobtained calculation is the accurate flood extent area. \\n \\nFig. 5. Difference map of Sentinel and Modis data \\nFig. 2, 3 and 4 illustrates the flood extent maps for  the \\nSentinel, Landsat and Modis data respectively showing a \\nsignificant difference between the flood extent area. Hence an \\nattempt is made to get the results more accurately using the \\ndifference maps. The difference map is generated using \\nsubtracting the bands of interest from both sensors and the \\npixel-wise difference is calculated.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='difference maps. The difference map is generated using \\nsubtracting the bands of interest from both sensors and the \\npixel-wise difference is calculated. \\nThe research related to the difference mapping for flood extent \\ncalculation is generally limited to the individual satellite data \\n[21], in this paper an attempt is made to cal culate the \\ndifference map for multiple satellite data in combination. \\nThe difference map is then encompassed with the GIS tool to \\nget the accurate results for further studies, hence as per user’s \\nchoice a combination of results would be obtained on one \\nplatform. This difference map can be used for generating flood \\ninundation maps, flood hazard maps, for monitoring the river \\ndynamics, time series analysis, land cover changes, etc \\nV.\\n CONCLUSION AND FUTURE SCOPE \\n Proposed study provides a comprehensive examina tion of \\nmultiple satellite datasets consolidated onto a singular platform. \\nThe subsequent evaluation and comparison of flood extent'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proposed study provides a comprehensive examina tion of \\nmultiple satellite datasets consolidated onto a singular platform. \\nThe subsequent evaluation and comparison of flood extent \\nareas for each satellite data source revealed significant \\ndifferences, prompting a detailed analysis and the creation of a \\ndifference map to visualize and interpret these variations in \\naccuracy. This study contributes valuable insights for \\nresearchers and practitioners relying on satellite data for flood \\nmonitoring and related applications. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\nfields of early warning systems, flood management, \\nresearchers, etc. \\n To quantify and visualize the observed differences in \\naccuracy among the various satellite datasets, we employed a \\nsystematic approach. Specifically, we generated a difference \\nmap that highlighted the disparities in flood extent \\nidentification across the differ ent satellite datasets. This map \\nserved as a valuable tool for effectively illustrating the \\nvariations in accuracy and allowed for a nuanced interpretation \\nof the discrepancies.Also, the difference maps play a vital role \\nin flood mapping by enabling the detection, analysis, and \\nmonitoring of changes in the landscape related to flooding. \\nThese maps provide valuable information for emergency \\nresponse, risk assessment, and long-term flood management.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='monitoring of changes in the landscape related to flooding. \\nThese maps provide valuable information for emergency \\nresponse, risk assessment, and long-term flood management. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the \\nfields of early warning systems, flood management, \\nresearchers, etc. But, certain limitations such as limited \\nspectral information, land cover changes or lack of data \\naccessibility may lead to the discrepancies in the results.  \\nR\\nEFERENCES \\n[1] Nsangou D, Kpoumié A, Mfonka Z, Bateni SM, Ngouh AN, Ndam          \\nNgoupayou JR, “The Mfoundi Watershed at Yaoundé in the \\nHumid Tropical Zone of Camer oon: a case study of urban food \\nsusceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='susceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol. \\n5, Issue 4, pp.861-871,2021.  \\n[3] Marinho G. Andrade, Marcelo D. Fragoso, Adriano A.F.M. \\nCarneiro, “A stochastic approach to the flood control problem,” \\nApplied Mathematical Modelling, vol. 25, Issue 6, 2001. \\n[4] Yiming Wei, Weixuan Xu, Ying Fan, Hsien -Tang Tasi, “Artificial \\nneural network based predi ctive method for flood disaster,” \\nComputers & Industrial Engineering vol. 42, Issues 2–4, 2002.  \\n[5] Shangyou Zhang, Ian Cordery, Ashish Sharma, “Application of an \\nimproved linear storage routing model for the estimation of large \\nfloods”, Journal of Hydrology, vol. 258, Issues 1–4, 2002. \\n[6] Karsten Jaspe, Joachim Gurtz, Herbert Lang, “Advanced flood \\nforecasting in Alpine watersheds by coupling meteorological \\nobservations and forecasts with a distributed hydrological model,”'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='[6] Karsten Jaspe, Joachim Gurtz, Herbert Lang, “Advanced flood \\nforecasting in Alpine watersheds by coupling meteorological \\nobservations and forecasts with a distributed hydrological model,” \\nJournal of Hydrology vol. 267, Issues 1–2,2002.  \\n[7] Thomas Rodding Kjeldsen, Jeff Smithers, Roland Schulze, \\n“Regional flood frequency analysis in the KwaZulu -Natal \\nprovince, South Africa, using the index -flood method,” Journal of \\nHydrology, vol. 255, Issues 1–4, 2002. \\n[8] Helder I. Chamine, Alcides  J. S. C. Pereira, Ana  C. Teodoro \\n“Remote sensing and  GIS applications in  earth and environmental \\nsystems sciences,” Springer Nature, vol. 3, 2021. \\n[9] Christopher J. Crawford, David P. Roy, Saeed Arab, \\nChristopherBarnes, et al., “The 50 -year Landsat collection 2 \\narchive,” Science of Remote Sensing, vol. 8, 2023. \\n[10] Christoph Kubitza, Vijesh Krishna, “Estimating adoption and \\nimpacts of agricultural management practices in developing'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='archive,” Science of Remote Sensing, vol. 8, 2023. \\n[10] Christoph Kubitza, Vijesh Krishna, “Estimating adoption and \\nimpacts of agricultural management practices in developing \\ncountries using satellite data. A scoping review,” Springer, vol. \\n40,2020. \\n[11] Sreechanth Sundaram, Suresh Devaraj & Kiran Yarrakula \\n“Modeling, mapping and analysis of urban floods in India —a \\nreview on geospatial methodologies,” Springer, vol. 28,2021. \\n[12] Alexei Lyapustin et al., “Calibration of the SNPP and NOAA 20 \\nVIIRS sensors for continuity of  the MODIS climate data records”, \\nRemote Sensing of Environment, vol. 295, 2023. \\n[13] M. Bellaoui, K. Bouchouicha, B. Oulimar, “Daily Global Solar \\nRadiation Based on MODIS Products: The Case Study of ADRAR \\nRegion (Algeria)”, Springer, vol. 102, 2019. \\n[14] Ekrem Sara lioglu, Can vatandaslar “Land use/land cover \\nclassification with Landsat -8 and Landsat -9 satellite images: a \\ncomparative analysis between forest - and agriculture -dominated'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='[14] Ekrem Sara lioglu, Can vatandaslar “Land use/land cover \\nclassification with Landsat -8 and Landsat -9 satellite images: a \\ncomparative analysis between forest - and agriculture -dominated \\nlandscapes using different machine learning methods”, Springer \\nvol. 57,2022. \\n[15] Beste Tavus, Sultan Kocaman, Candan Gokceoglu, “Flood damage \\nassessment with Sentinel-1 and Sentinel -2 data after Sardoba dam \\nbreak with GLCM features and Random Forest method,” Science \\nof The Total Environment vol. 816, 2022. \\n[16] Giacomo Caporusso, Marino Dell’Olo, et al, “Use of the Sentinel-1 \\nSatellite Data in the SNAP Platform and the WebGNOME \\nSimulation Model for Change Detection Analyses on the Persian \\nGulf Oil Spill,” Springer vol. 13379, 2022. \\n[17] Hamdi A.  Zurqani, “An automated approach for developing a \\nregional-scale 1 -m forest canopy cover dataset using machine \\nlearning and Google Earth Engine cloud computing platform,” \\nSoftware Impacts, vol. 19, 2024.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='regional-scale 1 -m forest canopy cover dataset using machine \\nlearning and Google Earth Engine cloud computing platform,” \\nSoftware Impacts, vol. 19, 2024. \\n[18] Jintao Liang a, Chao Chen b, Yongze Song c, Weiwei Sun d, Gang\\n Yang d, “Long-term mapping of land use and cover changes using \\nLandsat images on the Google Earth Engine Cloud Platform in bay \\narea - A case study of Hangzhou Bay, China,” Sustainable \\nHorizons, vol. 7, 2023. \\n[19] USGS, “What are the band designations for the Landsat satellites?”  \\n[Online]. Available: https://www.usgs.gov/faqs/what-are-band-\\ndesignations-landsat-satellites. [Accessed: 5- Dec- 2023]. \\n[20] Sadashiva Devadiga, “Terra & Aqua Moderate Resolution Imaging \\nSpectroradiometer (MODIS)” [Online] Available:  \\nhttps://ladsweb.modaps.eosdis.nasa.gov/missions-and-\\nmeasurements/modis/#:~:text=MODIS%20data%20products%2C\\n%20in%20three,and%20in%20the%20lower%20atmosphere. \\n[Accessed: 7- Dec- 2023].'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='https://ladsweb.modaps.eosdis.nasa.gov/missions-and-\\nmeasurements/modis/#:~:text=MODIS%20data%20products%2C\\n%20in%20three,and%20in%20the%20lower%20atmosphere. \\n[Accessed: 7- Dec- 2023]. \\n[21] Ramesh Sivan Pillai, Kevin M. Jacobs Chloe M. Mattilio, Ela V. \\nPiskorski, “Rapid flood inundation mapping by differen cing water \\nindices from pre - and post -flood Landsat images ,” Springer vol. \\n15,2021. \\n[22] Malay S. Bhatt, Tejas P. Patalia, “ Content-based high -resolution \\nsatellite image classification,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2018 vol. 11, 2019. \\n[23] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Proposal and evaluation of tsunami disaster drill support \\nsystem using tablet computer,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2023 vol. 15, 2023. \\n[24] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Bhoomi Prahari - e governance tool for monitoring'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='of Computer Applications and Management 2023 vol. 15, 2023. \\n[24] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Bhoomi Prahari - e governance tool for monitoring \\nencroachment on government land using mobile and GIS \\ntechnology,” IJIT - Bharati Vidyapeeth’s Institute of Comp uter \\nApplications and Management 2023 vol. 14, 2023. \\n[25] Venkata Sangameswar Mandavillil, Nagabhushanarao Madamala, \\n“Detection of natural disaster affected areas using R,” IJIT- Bharati \\nVidyapeeth’s Institute of Computer Applications and Management \\n2018 vol. 10, 2018. \\n[26] Mohd. Tajammul, Rafat Parveen, “Auto encryption algorithm for \\nuploading data on cloud storage”, IJIT - Bharati Vidyapeeth’s \\nInstitute of Computer Applications and Management 2020  vol. \\n12,2020.\\n \\nView publication stats'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning\\nProject Option: Comparative Analysis\\nAVIPSA BHUJABAL, University at Buffalo, USA\\nCHANCHAL BUNDELE, University at Buffalo, USA\\nVISHAL LAVANGARE,University at Buffalo, USA\\n1 Abstract\\nThe goal of this project is to construct a robust image captioning\\npipeline capable of generating high-quality, semantically accurate\\ncaptions for actual images. We explore a number of architectures\\nto this end, beginning with a baseline CNN-LSTM pipeline using\\nDenseNet201 as the feature extractor and an LSTM decoder trained\\non the Flickr8k dataset. For comparison with newer models, we\\nadded BLIP (Bootstrapping Language-Image Pretraining), a state-\\nof-the-art Vision-Language Model (VLM) that has been extremely\\neffective for captioning tasks.\\nTo improve the factual correctness of generated captions and\\nreduce hallucination—when models describe objects that are not\\npresent in the image—we integrated object detection using YOLOv5.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='To improve the factual correctness of generated captions and\\nreduce hallucination—when models describe objects that are not\\npresent in the image—we integrated object detection using YOLOv5.\\nThis allowed us to identify the actual content of the image and\\neliminate semantically unnecessary or hallucinated words from\\ngenerated captions. This approach to reducing hallucination was\\nguided by methods discussed in papers our advisor exposed us to.\\nWe evaluated the models in terms of BLEU-1 to BLEU-4 scores and\\nalso plotted performance using t-SNE to verify clustering in image\\nfeature space. Quantitative and qualitative results show that BLIP\\nsignificantly outperforms traditional CNN-RNN models, producing\\nmore accurate and descriptive captions. The hallucination filtering\\nmethod enhanced the factual accuracy of the captions regarding\\nvisual content.\\nOverall, this project demonstrates a complete image captioning\\npipeline with a focus on benchmarking, hallucination detection, and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='visual content.\\nOverall, this project demonstrates a complete image captioning\\npipeline with a focus on benchmarking, hallucination detection, and\\nvisualization, offering a pragmatic platform for building semanti-\\ncally grounded captioning systems.\\n1.1 Why This is Interesting\\nImage captioning, the task of generating natural language descrip-\\ntions from images, has recently emerged as a sought-after problem\\ndue to its extensive applications in accessibility aides, content-based\\nimage retrieval systems, social media automation, and human-robot\\ninteraction. The prime challenge lies in effectively bridging the\\nvision-language gap—capturing the content in an image and con-\\nveying it through coherent, meaningful text.\\nTraditionally, encoder-decoder architectures have been employed,\\nwhere a convolutional neural network (CNN) extracts image fea-\\ntures and a recurrent neural network (RNN), such as LSTM, gen-\\nerates a caption. Although these models initially demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='where a convolutional neural network (CNN) extracts image fea-\\ntures and a recurrent neural network (RNN), such as LSTM, gen-\\nerates a caption. Although these models initially demonstrated\\npromise, they often suffered from poor generalization and hallu-\\ncination—generating information not present in the image. The\\nadvent of pretrained vision-language models (VLMs) like BLIP and\\nAuthors’ Contact Information: Avipsa Bhujabal, University at Buffalo, Buffalo, USA,\\nNew York, avipsabh@buffalo.edu; Chanchal Bundele, University at Buffalo, Buffalo,\\nUSA, cbundele@buffalo.edu; Vishal Lavangare, University at Buffalo, Buffalo, USA,\\nvlavanga@buffalo.edu.\\nCLIP marked a significant shift toward multimodal transformers,\\nresulting in enhanced performance across captioning, retrieval, and\\nvisual question answering (VQA) tasks.\\nOur research presents a full image captioning pipeline that com-\\npares a traditional CNN-RNN model using DenseNet201 with a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='visual question answering (VQA) tasks.\\nOur research presents a full image captioning pipeline that com-\\npares a traditional CNN-RNN model using DenseNet201 with a\\nVLM-based model using BLIP. Beyond performance evaluation, we\\nincorporate object detection using YOLOv5 to address hallucination\\nby anchoring captions to actual detected objects, inspired by recent\\nefforts in semantic grounding.\\nThe motivation stems from real-world implications—such as in\\nassistive technology for the visually impaired, where hallucinated\\ncaptions (e.g., describing a dog that isn’t present) can mislead users.\\nA hallucination-aware captioning model enhances the reliability\\nand informativeness of AI-generated descriptions.\\nWe train both baseline and state-of-the-art models on the Flickr8k\\ndataset, integrate hallucination filtering, and evaluate them using\\nBLEU scores and visualizations such as t-SNE. Our key contributions\\ninclude:\\n• A comparative benchmark of traditional and modern image\\ncaptioning models.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='BLEU scores and visualizations such as t-SNE. Our key contributions\\ninclude:\\n• A comparative benchmark of traditional and modern image\\ncaptioning models.\\n• A hallucination filtering module leveraging object detection\\nto ground captions in detected visual content.\\n• Visualization tools to interpret model behavior and feature\\nrepresentations.\\nIn the broader multimodal AI landscape, this work offers a practi-\\ncal and educational examination of how emerging technologies can\\nbe harnessed to address persistent challenges in image captioning,\\nnotably hallucination and factual consistency.\\n2 Related Work\\nThe field of image captioning has evolved significantly, with nu-\\nmerous models tackling the challenge of generating accurate and\\ncontextually relevant descriptions from images.\\n• Show and Tell [1]: Introduced the foundational encoder-\\ndecoder architecture using CNNs and RNNs. It produced\\ngeneric captions but struggled with novel object descrip-\\ntions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Show and Tell [1]: Introduced the foundational encoder-\\ndecoder architecture using CNNs and RNNs. It produced\\ngeneric captions but struggled with novel object descrip-\\ntions.\\n• Show, Attend and Tell [2]: Employed attention mecha-\\nnisms to focus on specific regions of the image. Despite\\nimprovements, it sometimes misaligned attention, leading\\nto incorrect descriptions.\\n• Bottom-Up and Top-Down Attention [3]: Combined ob-\\nject detection and attention to improve detail, though it\\noccasionally hallucinated non-existent objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='2 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\n• Adversarial Semantic Alignment [4]: Leveraged GANs\\nto improve semantic alignment between captions and im-\\nage content. However, training GANs on discrete text data\\nremains a challenge.\\n• Visual Fact Checker (VFC) [5]: Proposed a training-free\\ncaption fact-checking system using object detection and\\nVQA. Its effectiveness depends on the external tools’ quality.\\n• BLIP [6]: A vision-language model achieving state-of-the-\\nart results across multiple tasks, though prone to hallucina-\\ntion without explicit grounding.\\n• ALOHa [7]: Introduced a large-language-model-based hal-\\nlucination detection metric, improving on previous fixed-\\nvocabulary approaches.\\n• Reducing Object Hallucination [8]: Proposed training\\nobjective optimization and object presence constraints to\\nreduce hallucination.\\n• Object Grounding and Hallucination [9]: Found that\\ngrounding alone may not sufficiently reduce hallucination,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='objective optimization and object presence constraints to\\nreduce hallucination.\\n• Object Grounding and Hallucination [9]: Found that\\ngrounding alone may not sufficiently reduce hallucination,\\nemphasizing the need for holistic strategies.\\n• Scalable Vision Learners [10]: Showed that captioning\\nmodels can be robust visual learners with appropriate scale\\nand fine-tuning.\\n• EVCap [11]: Used external memory for object name retrieval\\nto improve open-world captioning.\\n• Multimodal Hallucination Detection in 3D [12]: Ad-\\ndressed hallucination across modalities for 3D content but\\nfocused on a different domain.\\n• Captioning Evaluation with LLMs [13]: Highlighted the\\nneed for better metrics capturing hallucination and factual\\nconsistency in image captioning.\\n• Pseudo Content Hallucination[14]: Used pseudo-generated\\ncontent for unpaired image captioning, albeit with potential\\nnoise.\\n• Hyperbolic Learning for Open-World Detection [15]:\\nApplied synthetic captions in hyperbolic space to improve'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='content for unpaired image captioning, albeit with potential\\nnoise.\\n• Hyperbolic Learning for Open-World Detection [15]:\\nApplied synthetic captions in hyperbolic space to improve\\ndetection, aiding captioning tasks.\\n3 Proposed Approach and Contributions\\nIn this paper, we present an end-to-end image captioning pipeline\\nthat synergizes state-of-the-art vision-language models with ad-\\nvanced evaluation methods to generate accurate and contextually\\nrelevant captions. Our approach centers around comparing the per-\\nformance of BLIP (Bootstrapped Language-Image Pretraining), a\\nstate-of-the-art vision-language model, to that of a CNN-RNN-based\\ncaptioning architecture using DenseNet201 features. Additionally,\\nwe integrate YOLOv5-based object detection to mitigate hallucinated\\nobjects in generated captions—addressing the well-documented is-\\nsue of hallucination in captioning models.\\n3.1 Primary Contributions\\n(1) Multi-model Captioning Benchmark: We establish a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='objects in generated captions—addressing the well-documented is-\\nsue of hallucination in captioning models.\\n3.1 Primary Contributions\\n(1) Multi-model Captioning Benchmark: We establish a\\ncomparative benchmark between a traditional CNN-RNN\\narchitecture and a transformer-based vision-language model\\n(BLIP). Using evaluation metrics such as BLEU, we demon-\\nstrate the superior performance of recent pre-trained trans-\\nformers in generating coherent and relevant image captions.\\n(2) Hallucination Reduction via Object Detection: Drawing\\ninspiration from recent research in hallucination detection\\nand semantic grounding, we employ YOLOv5 to detect ob-\\njects within the image and remove tokens in the generated\\ncaptions that lack visual grounding. This results in more\\nfactual and accurate descriptions.\\n(3) Interactive Visual Analysis: We develop side-by-side visu-\\nalizations that display input images, captions generated by\\nboth BLIP and CNN-RNN models, and highlight hallucinated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='(3) Interactive Visual Analysis: We develop side-by-side visu-\\nalizations that display input images, captions generated by\\nboth BLIP and CNN-RNN models, and highlight hallucinated\\ncontent. These visual aids enhance model interpretability\\nand support thorough error analysis.\\n(4) Robust Preprocessing and Dataset Exploration: We ap-\\nply preprocessing techniques such as lemmatization, stop-\\nword removal, and caption length filtering on the Flickr8k\\ndataset. These steps contribute to a cleaner training dataset\\nand more reliable tokenizer performance.\\n(5) End-to-End Pipeline with BLIP + YOLOv5 Integration:\\nTo the best of our knowledge, this work is among the first in\\ncoursework to combine BLIP-based captioning with YOLOv5-\\nbased hallucination filtering, DenseNet-based baselines, and\\ninteractive visualizations in a cohesive and modular pipeline.\\nOur work bridges the gap between captioning accuracy and fac-\\ntual coherence, offering a flexible framework that can be extended'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='interactive visualizations in a cohesive and modular pipeline.\\nOur work bridges the gap between captioning accuracy and fac-\\ntual coherence, offering a flexible framework that can be extended\\nwith other vision-language models such as CLIP or grounding-\\nspecific techniques. All code is made publicly available and is Colab-\\nfriendly to support reproducibility and experimentation.\\n4 Methodology\\nOur project aims to solve the problem of automatic image captioning\\nusing both conventional deep learning models and state-of-the-\\nart transformer-based models. We explore two primary modeling\\napproaches, along with hallucination filtering and comprehensive\\nevaluation.\\n4.1 CNN-RNN Baseline using DenseNet201 + LSTM\\nDecoder\\nWe adopt a standard encoder-decoder architecture comprising:\\n• Encoder: DenseNet201, a deep CNN pretrained on Ima-\\ngeNet, is used to extract 1920-dimensional feature vectors\\nfrom input images. The classification head is removed to\\nretain only visual embeddings.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='geNet, is used to extract 1920-dimensional feature vectors\\nfrom input images. The classification head is removed to\\nretain only visual embeddings.\\n• Decoder: An LSTM-based RNN processes the image fea-\\ntures concatenated with embedded word sequences to pre-\\ndict the next word. The decoder includes dropout layers for\\nregularization and a softmax output over the vocabulary.\\nThis model is trained using a custom data generator that dynam-\\nically pairs image features with caption sequences. Captions are\\ntokenized into (input, output) word pairs, and sequences are\\npadded and one-hot encoded.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 3\\n4.2 BLIP: Bootstrapped Language-Image Pretraining\\nWe leverage BLIP, a state-of-the-art pretrained vision-language\\nmodel from Salesforce. BLIP uses a Vision Transformer (ViT) en-\\ncoder and a language transformer decoder. Unlike our CNN-RNN\\nbaseline, BLIP directly takes raw images and generates captions\\nthrough pretrained attention mechanisms. We initialize BLIP in\\nzero-shot mode via the Hugging Face Transformers library, with-\\nout further fine-tuning. This enables a comparative study between\\nhandcrafted and pretrained models.\\n4.3 Hallucination Minimization with YOLOv5 Integration\\nTo address the common issue of hallucination—generating captions\\nwith objects not present in the image—we integrate the YOLOv5\\nobject detection framework. For each test image:\\n• YOLOv5 is used to detect objects in the image.\\n• The generated caption is post-processed, and non-grounded'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='object detection framework. For each test image:\\n• YOLOv5 is used to detect objects in the image.\\n• The generated caption is post-processed, and non-grounded\\ntokens (not in the detected object list) are removed, exclud-\\ning stopwords and function words.\\nThis approach semantically aligns captions with detected content,\\ndrawing on current research in factual grounding and hallucination\\nmitigation.\\n4.4 Evaluation and Visualization\\n• We evaluate captions using BLEU-1 to BLEU-4 scores.\\n• We visualize results by showing:\\n– Original image with ground truth caption\\n– DenseNet-LSTM-generated caption\\n– BLIP-generated caption\\n– YOLOv5-filtered caption (hallucination-minimized)\\n• We apply t-SNE and KMeans clustering on image features\\nto analyze semantic grouping.\\n• Additional metrics such as METEOR and ROUGE are calcu-\\nlated using the nlg-eval toolkit where available.\\n5 Preprocessing, Model Architecture, and Training\\n5.1 Preprocessing and Data Preparation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Additional metrics such as METEOR and ROUGE are calcu-\\nlated using the nlg-eval toolkit where available.\\n5 Preprocessing, Model Architecture, and Training\\n5.1 Preprocessing and Data Preparation\\nImage Preprocessing. To ensure consistency and model compatibility,\\nthe following steps are performed on raw images:\\n• Resizing: All images are resized to224 × 224 pixels to match\\nthe input requirement of DenseNet201.\\n• Normalization: Pixel values are normalized to the range\\n[0, 1] by dividing each pixel by 255. This aids in faster con-\\nvergence during training.\\n• Feature Extraction: DenseNet201, pretrained on ImageNet,\\nis used without its final classification layer. We extract the\\n1920-dimensional feature vector from the second-to-last\\nlayer, capturing high-level semantics.\\n• Feature Storage: Extracted features are stored in a dic-\\ntionary with image filenames as keys to avoid redundant\\ncomputation and accelerate model training.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='layer, capturing high-level semantics.\\n• Feature Storage: Extracted features are stored in a dic-\\ntionary with image filenames as keys to avoid redundant\\ncomputation and accelerate model training.\\nText Preprocessing. Since raw captions are not model-compatible,\\nwe perform the following preprocessing:\\n• Lowercasing: Converts all text to lowercase to avoid case-\\nbased duplicates.\\n• Punctuation and Number Removal: Uses regular expres-\\nsions to clean special characters and numeric values.\\n• Whitespace Normalization: Removes extra spaces and\\nnormalizes the spacing.\\n• Short Word Filtering: Removes tokens with fewer than\\ntwo characters.\\n• Lemmatization: Reduces words to their root form using\\nspaCy.\\n• Stopword Removal: Filters out common stopwords for\\nbetter semantic focus.\\n• Start/End Tokens: Captions are enclosed with startseq\\nand endseq to define boundaries for generation.\\nTokenization and Splitting.\\n• Tokenizer: Keras’sTokenizer is used to convert words into'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Start/End Tokens: Captions are enclosed with startseq\\nand endseq to define boundaries for generation.\\nTokenization and Splitting.\\n• Tokenizer: Keras’sTokenizer is used to convert words into\\ninteger indices, resulting in a vocabulary of approximately\\n6700 words.\\n• Max Caption Length: The longest caption length ( 34 to-\\nkens) is saved and used for sequence padding.\\n• Padding: All sequences are padded to the maximum length\\nto standardize input shapes.\\n• Train-Test Split: The dataset is split into 85% training and\\n15% validation, ensuring no image overlap.\\n5.2 Model Architecture\\nVisual Feature Extraction using DenseNet201.\\n• Dense Connectivity: Promotes gradient flow and feature\\nreuse.\\n• Transfer Learning:Leverages pretrained ImageNet weights\\nfor faster convergence.\\n• Output Feature Vector: Extracts a 1920-dimensional rep-\\nresentation per image.\\nTextual Decoder with LSTM. The caption generation model includes:\\n• Embedding Layer: Transforms tokenized words into dense'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='resentation per image.\\nTextual Decoder with LSTM. The caption generation model includes:\\n• Embedding Layer: Transforms tokenized words into dense\\n256-dimensional vectors.\\n• Image Compression: The 1920-dimensional feature vec-\\ntor is processed through a dense layer to align with text\\nembeddings.\\n• Concatenation: The processed image vector is concate-\\nnated with the embedded caption sequence.\\n• LSTM Decoder: A 512-unit LSTM processes the combined\\nsequence to predict the next word.\\n• Residual Connection: Adds LSTM output back to the im-\\nage vector to reinforce visual grounding.\\n• Dense Layers: A fully connected Dense(128) layer followed\\nby a Dense layer with softmax activation over the vocabu-\\nlary.\\nThe final model contains approximately 5.74 million trainable\\nparameters.\\n5.3 Training Strategy\\nCustom Data Generator. To accommodate varying caption lengths\\nand memory constraints, a custom Keras Sequence generator is\\nimplemented to:\\n• Load precomputed image features.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='4 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\n• Dynamically generate (input sequence, target word) training\\npairs.\\n• Apply tokenization, padding, and one-hot encoding on-the-\\nfly.\\nTraining Configuration.\\n• Loss Function: Categorical Crossentropy for multi-class\\nword prediction.\\n• Optimizer: Adam with a learning rate of 0.001.\\n• Batch Size: 64\\n• Epochs: Trained for up to 30 epochs.\\n• Callbacks:\\n– ModelCheckpoint: Saves the model with the lowest\\nvalidation loss.\\n– EarlyStopping: Stops training if no improvement is\\nseen for 5 epochs.\\n– ReduceLROnPlateau: Halves the learning rate if vali-\\ndation loss plateaus for 3 epochs.\\n6 Evaluation and Results\\nEvaluation Metrics\\nWe evaluate the performance of our captioning models using the\\nfollowing metrics:\\n• BLEU-1 to BLEU-4: Measures n-gram precision between\\ngenerated and reference captions.\\n• METEOR: Considers synonym matching and word stems,\\nproviding recall-oriented evaluation.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='following metrics:\\n• BLEU-1 to BLEU-4: Measures n-gram precision between\\ngenerated and reference captions.\\n• METEOR: Considers synonym matching and word stems,\\nproviding recall-oriented evaluation.\\n• ROUGE-L: Evaluates the longest common subsequence be-\\ntween candidate and reference captions.\\n• CIDEr: Measures consensus between generated captions\\nand human annotations. Due to the limited size of Flickr8k,\\nCIDEr scores were found to be low.\\n7 Flickr8k Dataset Overview\\nFor our project, we use the Flickr8k dataset, a widely used bench-\\nmarking dataset for automatic image captioning. It is a collection\\nof images with multiple human-written captions, making it an ideal\\ntool for evaluating the performance of machine learning models in\\ngenerating textual descriptions.\\n7.1 Characteristics of the Dataset\\n• Source: The dataset was introduced by Hodosh, Young, and\\nHockenmaier (2013).\\n• Accessible on Kaggle: https://www.kaggle.com/datasets/\\nadityajn105/flickr8k\\n• Size:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='7.1 Characteristics of the Dataset\\n• Source: The dataset was introduced by Hodosh, Young, and\\nHockenmaier (2013).\\n• Accessible on Kaggle: https://www.kaggle.com/datasets/\\nadityajn105/flickr8k\\n• Size:\\n– Total Images: 8,092 JPEG images.\\n– Total Captions: 40,000 human-written captions.\\n– Captions per Image: Each image has five different\\ncaptions written by human annotators.\\n• Format:\\n– Images are available in JPEG format.\\n– Captions are stored in a text file linking each image to\\nits corresponding descriptions.\\nFig. 1. Sample Dataset\\n7.2 Challenges in the Flickr8k Dataset\\n• Data Size Constraint: Unlike MS-COCO (123,000+ im-\\nages), Flickr8k contains only 8,092 images, making gener-\\nalization more difficult.\\n• Caption Variability: Each image has multiple captions,\\nintroducing lexical and syntactic differences. Evaluating\\ncaptions is challenging due to varying levels of detail.\\n• Diverse Image Content: The dataset includes human be-\\nhavior, animals, objects, and natural scenes , requiring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='captions is challenging due to varying levels of detail.\\n• Diverse Image Content: The dataset includes human be-\\nhavior, animals, objects, and natural scenes , requiring\\nmodels to handle contextual understanding.\\n• No Explicit Object Annotations:Unlike MS-COCO, Flickr8k\\ndoes not include bounding box annotations. Models must\\ninfer semantic relationships from image features alone.\\n• Computational Efficiency: The dataset’ssmall size al-\\nlows for rapid model training and testing , but high-\\ncapacity models like transformers risk overfitting with-\\nout sufficient regularization.\\n8 Why Flickr8k?\\nThe Flickr8k dataset is chosen because it allows us to:\\n• Evaluate models under low-resource conditions.\\n• Analyze performance on a dataset with diverse cap-\\ntions and image types.\\n• Benchmark models without requiring massive com-\\nputational resources.\\nTo mitigate dataset limitations, we employ:\\n• Pretrained Transformer Models (e.g., BLIP, GIT) to lever-\\nage larger datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Benchmark models without requiring massive com-\\nputational resources.\\nTo mitigate dataset limitations, we employ:\\n• Pretrained Transformer Models (e.g., BLIP, GIT) to lever-\\nage larger datasets.\\n• Data Augmentation techniques to artificially increase cap-\\ntion diversity.\\n• Regularization & Fine-tuning to reduce overfitting risks.\\n9 Results\\nThis section presents the experimental evaluations of the proposed\\nCNN-LSTM-based image captioning model. We benchmarked our\\nmodel against the state-of-the-art BLIP model to assess both gener-\\native quality and factual grounding. Additionally, we analyzed the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 5\\neffect of hallucination filtering on caption accuracy using YOLOv5\\nobject detection.\\n9.1 Evaluation Setup\\nWe used a subset of theFlickr8k dataset comprising 6,000 training\\nand 2,000 validation images. Each image has five corresponding\\ncaptions. The following metrics were used to quantitatively evaluate\\ncaptioning quality:\\n• BLEU (1 to 4): N-gram precision.\\n• METEOR: Semantic matching with synonyms and stems.\\n• ROUGE-L: Longest common subsequence overlap.\\n• CIDEr: Consensus-based evaluation (ineffective on small\\ndatasets).\\nThe models compared:\\n• Baseline: CNN-LSTM using DenseNet201 features.\\n• Transformer-based: BLIP (Bootstrapped Language-Image\\nPretraining).\\n9.2 Training and Validation Loss Analysis\\nFigure 2 presents the training and validation loss curves observed\\nduring model training. Initially, both losses decrease steadily, in-\\ndicating that the model is learning effective representations from'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='during model training. Initially, both losses decrease steadily, in-\\ndicating that the model is learning effective representations from\\nthe image-caption pairs. After around the 5th epoch, the validation\\nloss begins to plateau while the training loss continues to decline,\\nsuggesting the model is starting to slightly overfit the training data.\\nDespite this, the gap between training and validation loss remains\\nsmall, which indicates good generalization. The trend suggests that\\nour custom data generator and regularization techniques, such as\\ndropout, are effective in mitigating overfitting. This learning behav-\\nior supports the robustness of the training setup and highlights the\\nmodel’s ability to capture the underlying semantics required for\\ngenerating meaningful image captions.\\n9.3 Feature Representation Analysis\\nTo evaluate how well our feature extractor (DenseNet201) captures\\nthe semantic diversity in images, we employ t-SNE (t-Distributed'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='9.3 Feature Representation Analysis\\nTo evaluate how well our feature extractor (DenseNet201) captures\\nthe semantic diversity in images, we employ t-SNE (t-Distributed\\nStochastic Neighbor Embedding) for dimensionality reduction and\\nvisualization. This technique allows us to project high-dimensional\\nfeature vectors into a 2D plane, revealing patterns and clusters.\\n• Figure 3 displays the image-level t-SNE representation clus-\\ntered using KMeans. This provides qualitative evidence that\\nsimilar image types group together in the feature space.\\nThese plots confirm that DenseNet201-derived features retain\\nmeaningful distinctions necessary for downstream caption genera-\\ntion.\\nFig. 3. t-SNE + KMeans cluster visualization of image features with thumb-\\nnails from the Flickr8k dataset. Similar images tend to group together,\\nvalidating our DenseNet feature extractor.\\n9.4 Qualitative Comparison of Captions\\nTo gain a more intuitive understanding of model behavior, we'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='validating our DenseNet feature extractor.\\n9.4 Qualitative Comparison of Captions\\nTo gain a more intuitive understanding of model behavior, we\\npresent side-by-side comparisons of image captions generated by\\nDenseNet and BLIP. Each example includes the original image, the\\nground-truth caption, predictions by both models, and their respec-\\ntive CLIP similarity scores. This helps assess factual grounding and\\nsemantic accuracy.\\n9.5 Quantitative Results\\nThese examples demonstrate BLIP’s ability to generate semantically\\nrich and grounded captions compared to DenseNet, particularly in\\ncomplex scenes involving multiple objects or fine-grained relation-\\nships.\\nTable 1. Average Metric Comparison Across 100 Test Samples\\nMetric DenseNet Model BLIP Model\\nBLEU-1 0.2078 0.1512\\nBLEU-2 0.0936 0.0778\\nBLEU-3 0.0519 0.0055\\nBLEU-4 0.0319 0.0000\\nMETEOR 0.1231 0.1165\\nROUGE-L 0.2654 0.2479\\nCIDEr 0.0000 0.0000\\nInsight: Despite BLIP’s advanced architecture, the CNN-LSTM'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='BLEU-2 0.0936 0.0778\\nBLEU-3 0.0519 0.0055\\nBLEU-4 0.0319 0.0000\\nMETEOR 0.1231 0.1165\\nROUGE-L 0.2654 0.2479\\nCIDEr 0.0000 0.0000\\nInsight: Despite BLIP’s advanced architecture, the CNN-LSTM\\nmodel showed stronger performance on n-gram-based metrics due\\nto better generalization on smaller datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='6 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\nFig. 2. Training vs. Validation Loss and BLEU Score Comparison Before and After Hallucination Filtering. The left plot shows the convergence of training\\nand validation loss, while the table on the right highlights the drop in BLEU scores after filtering hallucinated tokens using YOLOv5-detected objects. This\\nvalidates that the original model tends to hallucinate terms not grounded in visual input.\\nFig. 4. Example: BLIP generates a more semantically rich caption (\"snowy\\nslope\") and yields a significantly higher CLIP score than DenseNet, reflecting\\nimproved alignment with the visual content.\\n9.6 BLEU Score Comparison\\nTable 2. BLEU-1 Score Comparison (Sample Batch)\\nModel BLEU-1 Score\\nDenseNet 0.2284\\nBLIP 0.1751\\n9.7 Hallucination Filtering with YOLOv5\\nWe employed YOLOv5 object detection to detect visual elements\\nand filter out non-grounded words from the generated captions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='DenseNet 0.2284\\nBLIP 0.1751\\n9.7 Hallucination Filtering with YOLOv5\\nWe employed YOLOv5 object detection to detect visual elements\\nand filter out non-grounded words from the generated captions.\\nFig. 5. Example: While both models identify the dog, DenseNet overlooks\\nthe muzzle detail present in the ground truth. BLIP provides a more contex-\\ntually accurate description but still misses that specificity.\\nTable 3. BLEU Score Before and After Hallucination Filtering\\nFiltering Stage BLEU-1 BLEU-2\\nBefore 0.1814 0.1016\\nAfter 0.0313 0.0132\\nObservation: The substantial drop in BLEU scores post-filtering\\nconfirms the presence of hallucinated tokens that were not visually\\ngrounded.\\n9.8 Qualitative Results\\nWe visualized predictions of both models for randomly selected im-\\nages. DenseNet-based captions appeared more factual, while BLIP’s'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 7\\nFig. 6. Example: BLIP correctly identifies the main subjects—a lion and\\na buffalo—demonstrating its grounding strength. DenseNet, on the other\\nhand, misidentifies the context entirely.\\nFig. 7. Example: While both models recognize a running dog, only the\\nground truth acknowledges the muzzle, which is missed by BLIP and\\nDenseNet. The omission lowers their CLIP scores, highlighting the challenge\\nin object-level grounding.\\nwere descriptive but often hallucinated objects not present in the\\nimage.\\n9.9 Summary of Findings\\n• CNN-LSTM outperforms BLIP in BLEU and ROUGE metrics\\non Flickr8k.\\n• Hallucination is a serious issue in transformer-based models\\nunder small data regimes.\\n• YOLOv5-based hallucination filtering helped us identify se-\\nmantic inconsistencies.\\n10 Conclusion\\nIn this project, we did the job of automatic image captioning us-\\ning traditional and state-of-the-art approaches. We developed a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='mantic inconsistencies.\\n10 Conclusion\\nIn this project, we did the job of automatic image captioning us-\\ning traditional and state-of-the-art approaches. We developed a\\nCNN-LSTM model where DenseNet201 was used as a visual feature\\nextractor and a custom LSTM-based decoder to decode text descrip-\\ntions. In addition to this, we also compared this baseline with recent\\ntransformer-based vision-language models like BLIP.\\nExperiments, conducted on the Flickr8k dataset, reveal that transformer-\\nbased models like BLIP outperform traditional CNN-LSTM architec-\\ntures on qualitative and quantitative metrics like BLEU scores. BLIP\\nuse led to more fluent, varied, and contextually appropriate captions.\\nAside from this, we also incorporated hallucination detection from\\nrecent literature and assessed whether the generated captions were\\ngrounded in visual content.\\nThe project illustrates the increasing capability of pre-trained\\nVLMs to generate high-quality captions, especially when fine-tuned'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='grounded in visual content.\\nThe project illustrates the increasing capability of pre-trained\\nVLMs to generate high-quality captions, especially when fine-tuned\\non heterogeneous datasets. Training and generalization, however,\\nremain sensitive to dataset size and feature quality. We also observed\\nthat lighter-weight models like CNN-LSTM, though less expressive,\\ncan be used in resource-constrained environments.\\nIn the future, we will:\\n• Incorporate CLIP-based benchmarking for better alignment\\nof image-text embeddings.\\n• Fine-tune BLIP and other vision-language models on domain-\\nspecific caption datasets.\\n• Experiment with multimodal grounding techniques to re-\\nduce hallucinations and improve factual consistency even\\nmore.\\n• Scale to larger datasets like MS-COCO or NoCaps to verify\\ngeneralization.\\nOverall, our project provides insights into how generation quality\\nand visual semantic alignment differ between conventional and\\ntransformer-based frameworks.\\n11 Individual Contributions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Overall, our project provides insights into how generation quality\\nand visual semantic alignment differ between conventional and\\ntransformer-based frameworks.\\n11 Individual Contributions\\n11.1 Vishal\\nVishal led the data preprocessing and image feature extraction\\npipeline. He implemented techniques to resize, normalize, and clean\\nimage-caption pairs effectively. He integrated DenseNet201 as a\\npretrained CNN for extracting high-dimensional feature vectors,\\nstored efficiently for reuse. Vishal also managed aspects of caption\\ntokenization and ensured compatibility between the image and text\\nmodalities during training.\\n11.2 Avipsa\\nAvipsa focused on model architecture development and com-\\nparison. She implemented the CNN-LSTM-based caption gener-\\nation pipeline and worked on training configurations such as hy-\\nperparameter tuning, dropout strategies, and callbacks . She\\nalso integrated BLIP, a transformer-based model, for captioning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='ation pipeline and worked on training configurations such as hy-\\nperparameter tuning, dropout strategies, and callbacks . She\\nalso integrated BLIP, a transformer-based model, for captioning\\ncomparison, and generated visualizations for model output, loss\\ntrends, and BLEU-based evaluations. Avipsa further contributed to\\nhallucination filtering using YOLOv5 object detection, addressing\\ngrounding issues as cited in recent literature.\\n11.3 Chanchal\\nChanchal handled evaluation, visualization, and benchmarking.\\nShe performed metric analysis using BLEU, METEOR, ROUGE,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='8 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\nand CIDEr , and supported hallucination evaluation using pre-\\nand post-filter BLEU scores. She contributed to generating t-SNE\\nplots and clustering visualizations to interpret model embed-\\ndings. Chanchal also began exploring CLIP-based benchmarks\\nand worked on preparing comprehensive documentation for final\\nreporting and presentation.\\n12 REFERENCES\\nReferences\\n[1] Vinyals et al., Show and Tell: A Neural Image Caption Generator , CVPR 2015.\\nhttps://arxiv.org/abs/1411.4555\\n[2] Xu et al., Show, Attend and Tell, ICML 2015. https://arxiv.org/abs/1502.03044\\n[3] Anderson et al., Bottom-Up and Top-Down Attention , CVPR 2018. https://arxiv.\\norg/abs/1707.07998\\n[4] Dognin et al., Adversarial Semantic Alignment , CVPR 2019. https:\\n//openaccess.thecvf.com/content_CVPR_2019/papers/Dognin_Adversarial_\\nSemantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.\\ncom/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-\\nFidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf\\n[6] Li et al., BLIP, ICML 2022. https://proceedings.mlr.press/v162/li22n.html\\n[7] CVPR 2024. https://arxiv.org/abs/2404.02904\\n[8] Biten et al., Let There Be a Clock on the Beach , WACV 2022. https://arxiv.org/abs/\\n2110.01705\\n[9] EMNLP 2024. https://arxiv.org/abs/2406.14492\\n[10] NeurIPS 2023. https://arxiv.org/abs/2306.07915\\n[11] Li et al., EVCap, CVPR 2024. https://arxiv.org/abs/2311.15879\\n[12] NeurIPS 2024. https://papers.nips.cc/paper_files/paper/2024/hash/\\nd75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html\\n[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.\\n[15] Hyperbolic Learning with Synthetic Captions , CVPR 2024.\\nProject Repository\\nThe complete code and resources for this project can be found on\\nGitHub:\\nhttps://github.com/Avipsa-Bhujabal/Image𝐶𝑎𝑝𝑡𝑖𝑜𝑛𝑖𝑛𝑔 𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af19c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87616536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 598.85it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2b4977a8290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b1e70",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f09af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d7a209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2b497973310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd3ec511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/379933077\\nImproving Earth Observations by correlating Multiple Satellite Data: A\\nComparative Analysis of Landsat, MODIS and Sentinel Satellite Data for Flood\\nMapping\\nConference Paper · February 2024\\nDOI: 10.23919/INDIACom61295.2024.10498948\\nCITATIONS\\n7\\nREADS\\n209\\n9 authors, including:\\nSonali Kadam\\nBharati Vidyapeeth College of Engineering for Women\\n54 PUBLICATIONS\\xa0\\xa0\\xa0123 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAnjali Kadam\\nBharati Vidyapeeth's College of Engineering for Women, Pune,India\\n22 PUBLICATIONS\\xa0\\xa0\\xa023 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAhilya Bandgar\\nBharati Vidyapeeth's College Of Engineering for Women\\n4 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nRavindra V. Kale\\nNational Institute of Hydrology\\n45 PUBLICATIONS\\xa0\\xa0\\xa0341 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Anjali Kadam on 27 June 2024.\\nThe user has requested enhancement of the downloaded file.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\nBharati Vidyapeeth's Institute of Computer Applications and Management (BVICAM), New Delhi (INDIA)  \\n \\n \\nImproving earth observations by correlating multiple \\nsatellite data: A Comparative Analysis of Landsat, \\nMODIS, and Sentinel Satellite Data for flood \\nmapping \\nSonali Kadam \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nsonali.kadam@bharatividyapeeth.edu \\nAnjali Kadam \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nAnjali.kadam@bharatividyapeeth.edu \\nPrakash Devale \\nDepartment of Information Technology \\nBharati Vidyapeeth (Deemed to be) \\nUniversity College of Engineering \\nPune, Maharashtra \\nprdevale@bvucoep.edu.in \\nAhilya Bandgar \\nDepartment of Computer Engineering\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Bharati Vidyapeeth (Deemed to be) \\nUniversity College of Engineering \\nPune, Maharashtra \\nprdevale@bvucoep.edu.in \\nAhilya Bandgar \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nbandgarahilya@gmail.com \\nRajlaxmi Manepatil \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nmanepatilrajlaxmi@gmail.com \\n \\nRavindra Kale  \\nNational Institute of Hydrology \\nRoorkee, Uttarakhand, India \\nravikale.nihr@gmail.com \\nJotiram Gujar \\nDepartment of Chemical Engineering \\nSinhgad College of Engineering  \\nPune, Maharashtra \\njotiramgujar@gmail.com \\n \\nChanchal Bundele \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of \\nEngineering for Women  \\nPune, Maharashtra \\nchanchalbundele04@gmail.com \\n \\nTanmayi Chavan  \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of  \\nEngineering for Women  \\nPune, Maharashtra \\ntanmayichavan0502@gmail.com'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='chanchalbundele04@gmail.com \\n \\nTanmayi Chavan  \\nDepartment of Computer Engineering \\nBharati Vidyapeeth’s College of  \\nEngineering for Women  \\nPune, Maharashtra \\ntanmayichavan0502@gmail.com \\n  \\n \\nAbstract— Flooding is a recurring and severe calamity, \\naggravated by climate change and rapid urbanization, resulting \\nin substantial loss of life, property, and economic disruption. To \\naddress this critical issue, a comprehensive study focuses on \\ndelineating flood-prone zones using satellite data from Landsat, \\nMODIS, and Sentinel sources. Geographic Information System \\n(GIS) technology w ith Google Earth Engine (GEE) platform, \\nproviding an environment which is based on the cloud for \\nanalysis of geospatial data. The methodology encompasses data \\ncollection, preprocessing, and the application of algorithms for \\nflood mapping and depth estimati on. By assessing flood extents \\nand depths, it contributes to informed decision- making for flood'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='collection, preprocessing, and the application of algorithms for \\nflood mapping and depth estimati on. By assessing flood extents \\nand depths, it contributes to informed decision- making for flood \\nmanagement and disaster preparedness. Researchers have made \\nprominent efforts to use these satellite data in the individual \\nmanner or using techniques like fusi on to combine two different \\nsatellite data. The author has made an attempt to design a \\nplatform where all three- satellite data would be available \\ntogether so that valuable insights are drawn in the field of flood \\nmapping. This paper is prominently focused to corelate those \\nsatellite data with the help of difference mapping. \\nKeywords— Landsat, MODIS, Sentinel, GIS, flood extent mapping, \\nflood depth estimation. \\nI. INTRODUCTION \\nA flooding event can be described as the non- permanent \\ninundation of water that submer ges usual arid regions in any \\ngeographical area. Flooding resulting from rapid snowmelt,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='I. INTRODUCTION \\nA flooding event can be described as the non- permanent \\ninundation of water that submer ges usual arid regions in any \\ngeographical area. Flooding resulting from rapid snowmelt, \\nstorm surges impacting inland regions from the sea, persistent \\nand intense rainfall during the monsoon period, and the \\ndestruction of dams, embankments, or levees in t he season of \\nhigh winds and heavy rains [1].Deforestation has results in, the \\nincreased frequency of coastal storms can rise because of \\nabrupt changes in land use, inappropriate management of urban \\nstormwater runoffs, and climate change. In recent times, t here \\nhas been a notable increase in the frequency of global flooding \\nevents [2]. Flash floods and the more common river floods are \\nthe two main categories of flooding.River floods usually cause \\nmore property damage, but flash floods usually cause more \\nfatalities [2]. \\nSatellite data is very informative way to extract the images of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='more property damage, but flash floods usually cause more \\nfatalities [2]. \\nSatellite data is very informative way to extract the images of \\nthe particular area in an efficient manner [22]. Launched in \\nJuly 1972, Landsat 1 was the first digital spaceborne sensor \\ndeveloped for monitoring the terrestrial environment, NASA is \\nin charge of designing and launching the Landsat satellites and \\nsensors into near -polar low earth orbit [14]. The U.S. \\nGeological Survey (USGS) is in charge of flight operations, \\narchiving, ground processing, and distribution, and an affiliated'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\nLandsat Science Team performs scientific and technical \\nevaluation, Landsat has a highest series of missions (landsat -\\n1,2,3,4,5,7,8,9) [10, 14]. Earth Observations entered a new era \\nwith the launch of the Earth Observing System (EOS) Terra \\nsatellite in December 1999 and the Aqua satellite in May 2002. \\nThe NASA Ocean Biology Processing Group (OBPG) and \\nother science teams contributed to these satellite launches [12,       \\n13]. Sentinel -1 (S1) and Sentinel-2 (S2)  which are  \\nmultispectral optical instruments of the Europ ean Space \\nAgency were launched in 2014 and 2017 respectively [15, 16]. \\nIn this research paper, a concerted effort is undertaken to \\nintegrate the capabilities of three prominent Earth observation \\nsatellites – namely, LANDSAT, MODIS, and SENTINEL –'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='In this research paper, a concerted effort is undertaken to \\nintegrate the capabilities of three prominent Earth observation \\nsatellites – namely, LANDSAT, MODIS, and SENTINEL –  \\nwithin a unified platform utilizing Geographic Information \\nSystem (GIS) tools. The primary objective is to harness the \\nsynergies offered by these satellite systems to deliver efficient \\nand timely flood difference maps. By amalgamating data from \\nmultiple sources, th is approach aims to enhance the precision \\nand reliability of flood monitoring, providing researchers and \\nend-users with prompt access to valuable information. \\nUltimately, this paper envisions a comprehensive and user -\\nfriendly solution that not only leverag es the strengths of \\nmultiple satellite systems but also harnesses the power of GIS \\ntools to deliver swift and reliable flood difference maps. The \\nintegration of LANDSAT, MODIS, and SENTINEL within a \\nunified platform represents a noteworthy advancement in t he'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='tools to deliver swift and reliable flood difference maps. The \\nintegration of LANDSAT, MODIS, and SENTINEL within a \\nunified platform represents a noteworthy advancement in t he \\nfield of remote sensing and geographic information science, \\noffering a valuable resource for those involved in flood \\nresearch and mitigation efforts. \\nII. L\\nITERATURE REVIEW \\nStochastic approaches to flood control encompass a variety of \\nanalytical methods aime d at understanding and managing the \\nuncertainties associated with flooding events. Among these \\napproaches are Monte Carlo Markov Chain Algorithms \\n(MCMC) [3], which leverage probabilistic algorithms to \\nsimulate and sample from probability distributions, ena bling \\nthe assessment of various flood scenarios. Artificial Neural \\nNetwork (ANN)-based predictive models [4] represent another \\nfacet of stochastic analysis, utilizing machine learning \\ntechniques to predict river discharge and other relevant'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Network (ANN)-based predictive models [4] represent another \\nfacet of stochastic analysis, utilizing machine learning \\ntechniques to predict river discharge and other relevant \\nparameters base d on historical data. Conventional modelling \\nschemes [5] in this context typically refer to traditional, \\ndeterministic models used in hydrology and hydraulics  [6] to \\nsimulate the behaviour of rivers and water systems. The Index -\\nFlood Method  [7] involves st atistical analysis of historical \\nflood data to estimate the probability of floods of different \\nmagnitudes occurring, contributing to a more comprehensive \\nunderstanding of flood risk. \\nOn the other hand, Geographic Information System (GIS) \\ncalculations, part icularly those conducted using tools like \\nGoogle Earth Engine (GEE), provide a spatial perspective in \\nflood control strategies.GIS focuses on the analysis of spatial \\nand geographic data  [24], allowing for the mapping and'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Google Earth Engine (GEE), provide a spatial perspective in \\nflood control strategies.GIS focuses on the analysis of spatial \\nand geographic data  [24], allowing for the mapping and \\nassessment of physical characterist ics of the terrain and land \\nuse. GEE, being a cloud-based platform, facilitates the efficient \\nprocessing and analysis of large -scale [26] Earth observation \\ndata. While stochastic approaches delve into the predictive \\nmodelling of floods, GIS calculations [8] emphasize the spatial \\naspects of flood -prone areas, considering factors such as \\ntopography, land cover, and infrastructure that influence the \\ndynamics of flooding events. The satellite data provides \\nrequired data over long period of time [10], also the geo spatial \\nmethodologies [22] have made a significant contribution for the \\nflood [11], disaster analysis [23]. \\nWith the GEE computing platform, you can do away with the \\nneed for downloading, preprocessing, and a powerful'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='flood [11], disaster analysis [23]. \\nWith the GEE computing platform, you can do away with the \\nneed for downloading, preprocessing, and a powerful \\ncomputing environment all of which are  typically associated \\nwith using remotely sensed data. GEE is an adaptable and \\ntransparent platform that can handle a wide range of research \\nareas, from forestry to crop mapping to drought monitoring, \\neven though it is optimized for big data. Land use land \\ncoverchange monitoring is made more dependable and \\neffective by its ability to leverage cloud computing resources \\nand access a wealth of multisource datasets [17, 18].In addition \\nto enabling the integration of multi-source and  sensing images, \\nthe rise an d development of satellite remote sensing , cloud \\nstorage, exemplified by GEEalso making full-band image \\ncomputing possible [18]. \\nIII. \\nDATASET USED \\nAs mentioned, we have used LANSAT, MODIS and \\nSNTINEL satellites to perform our analysis in our study.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"computing possible [18]. \\nIII. \\nDATASET USED \\nAs mentioned, we have used LANSAT, MODIS and \\nSNTINEL satellites to perform our analysis in our study. \\nBelow is a list of these satellites' specific attributes. \\nTABLE I. CHARACTERISTICS OF LANDSAT SATELLITE  \\nSatellite Sensor Launc\\nhed \\nYear \\nBands \\nLandsat - 1 MSS 1972 Green, Red, NIR, IR \\nLandsat - 2 MSS 1975 Green, Red, Near-Infrared, \\nInfrared \\nLandsat - 3 RBV 1978 Blue, Green, Red \\nLandsat - 4 TM 1982 TIR, MIR, NIR \\nLandsat - 5 TM 1984 TIR, MIR, NIR, and visible \\nLandsat - 6 TM 1993 TIR, MIR, NIR, and visible \\nLandsat - 7 ETM+ 1999 Evident, TIR, panchromatic, \\nNIR, MIR \\nLandsat - 8 OLI/TIRS 2013 Visible, Panchromatic, SWI, \\nNIR, and TIR \\nLandsat - 9 OLI/TI RS 2021 Visible NR, Visible Blue, \\nVG, RP, SWIR 1, SWIR 2, \\nand Coastal Aerosol \\n  \\nTable I shows the characteristics of Landsat data. The spatial \\nresolution of the Green, Red, and Near Infrared bands on \\nLandsat 1–5 is 60 meters when using the multispectral scanner\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Table I shows the characteristics of Landsat data. The spatial \\nresolution of the Green, Red, and Near Infrared bands on \\nLandsat 1–5 is 60 meters when using the multispectral scanner \\n(MSS). The thermal band in Landsat 4 -5 has a spatial \\nresolution of 120( 17) meters, while the blue, green, red, near \\ninfrared, and SWIR -1/2 bands have a spatial resolution of 30 \\nmeters using Thematic Mapper(TM) [8, 19]. \\n The thermal and panchromatic bands in Landsat 7 have a \\nspatial resolution of 15 and 30 meters, respectively, while the \\nblue, green, red, and NIR SWIR -1/2 bands have a spatial \\nresolution of 30 meters.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\nThe Coastal Aerosol, Blue, Gr een Red, NIR, SWIR- 1/2, and \\nCirrus have a resolution of 30 meters in Landsat 8 -9 using \\nOperational Land Imager and Thermal Infrared Sensors; the \\nPanchromatic band has a resolution of 15 meters, and the \\nTIRS-1/2 has a resolution of 100 meters [19]. \\nTABLE II. Characteristics of MODIS Satellite \\nSatellite Sensor Launch \\nYear \\nBands \\nTerra  MODIS 1999 36 spectral bands that span \\nthe visible to thermal \\ninfrared portions of the \\nelectromagnetic spectrum. \\nAqua MODIS 2002 Like Terra MODIS, it has 36 \\nspectral bands that allow for \\ndetailed observations of the \\nEarth's surface and \\natmosphere. \\n  \\nThe spatial resolution of the Aqua and Terra satellites is \\nidentical; bands 1 and 2 have a 250 m spatial resolution, bands \\n3-7 have a 500 m spatial resolution, and bands 8 -36 have a\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='The spatial resolution of the Aqua and Terra satellites is \\nidentical; bands 1 and 2 have a 250 m spatial resolution, bands \\n3-7 have a 500 m spatial resolution, and bands 8 -36 have a \\n1000 m spatial resolution, respectively [20]. Table 2 illustrates \\nthe characteristics of MODIS satellite [9]. \\nTABLE III.            Characteristics of Sentinel Satellite  \\nSatellite Sensor Launc\\nh Year \\nBands \\nSentinel - 1 C- band \\nSAR \\n1-A: \\n2014 \\n1-B: \\n2016 \\nThe single SAR sensor \\ncarried by Sentinel -1 can \\nfunction in multiple \\npolarizations, such as HH \\nand VV. \\nSentinel - 2 MSI 2017 A multi-spectral imager with \\n13 spectral bands, including \\nvisible, NIR and SWIR \\nwavelengths, is carried by \\nSentinel-2A and Sentinel -\\n2B. \\nSentinel - 3 OLC/SL\\nSTR/SA\\nRA \\n2018 Sentinel-3A and Sentinel -3B \\nare equipped with multiple \\nspectral bands to measure \\nland surface temperature and \\nocean color. \\n \\nSentinel-1 has four operational modes, SM mode features'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='RA \\n2018 Sentinel-3A and Sentinel -3B \\nare equipped with multiple \\nspectral bands to measure \\nland surface temperature and \\nocean color. \\n \\nSentinel-1 has four operational modes, SM mode features \\n5X5 metre, IW features 5X20 metre, EW features 20X40 metre \\nand WV mode features 5X5 metre of spatial resolution. \\nThe Sentinel-2 has a spatial resolution of 10 m for the Blue, \\nGreen, Red, and NIR bands; 20 m for the vegetation red edge, \\nNIR, and band -11/12 for the SWIR bands; and 60 m eters for \\nthe coastal aerosol, water vapour, and SWIR Cirrus bands. \\nIn Sentinel-3 as per SLSTR the initial six spectral bands has \\nthe VNIR along with SWIR, in VNIR the band 1 -3 and in \\nSWIR the bands 4 -6 have a spatial resolution of 500 meters  \\n[10]. \\nIV. METHODOLOGY \\nFig. 1 shows the methodology for generating a hazard map, \\nusing the combination of Landsat, MODIS and Sentinel \\nmethodologies together. The method for each satellite is briefly \\nexplained below.  \\nA. Methods For Sentinel'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"using the combination of Landsat, MODIS and Sentinel \\nmethodologies together. The method for each satellite is briefly \\nexplained below.  \\nA. Methods For Sentinel \\n The following steps illustrateflood depth and flood area \\ncalculations of Sentinel dataset using GEE. \\n• Gathering data, preparing it for processing, and \\nloading the COPERNICUS/S1_GRD satellite dataset \\nand data of study area into the GEE Code Editor – \\nLoad the dataset of COPERNIC US/S1_GRD along \\nwith the river basin dataset. Next, filter the collection \\nto exclusively contain images with the \\n'instrumentMode' property set to 'IW'. Subsequently, \\nnarrow down the collection even further to \\nencompass only those images where the \\n'transmitterReceiverPolarisation' property contains \\n'VV'. Following this, apply a spatial filter to the \\ncollection, ensuring it comprises only images that \\nintersect with a specified area of interest (AOI). \\nLastly, concluded by selecting solely the 'VV' band\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='collection, ensuring it comprises only images that \\nintersect with a specified area of interest (AOI). \\nLastly, concluded by selecting solely the \\'VV\\' band \\nfrom each image within the filtered collection. \\n \\n• Mosaic and clip satellite data – \\nIn this, an Earth Engine Image Collection is subjected \\nto two different filtering methods. One which creates \\na composite image that represents the \"before\" flood \\nimages, from the collection that falls between the \\ngiven dates are first filtered. Then, they are combined \\ninto one image using the ‘mosaic\\' function. Similar to \\nthe first filtering process, a second filtering operation \\nis performed to choose images according to the flood \\noccurred dates. These images are likewise selected, \\nand they are then combined using ‘mosaic\\' to create a \\ncomposite image that represents the \"after\" flood \\nimagery.  \\n• Apply Smoothening Filter– \\nA smoothing filter is applied to two previously \\nclipped images. First, the before image, which has'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='composite image that represents the \"after\" flood \\nimagery.  \\n• Apply Smoothening Filter– \\nA smoothing filter is applied to two previously \\nclipped images. First, the before image, which has \\nbeen clipped and is subjected to a focal m edian filter \\nwith a radius of 30 meters and a circular \\nneighbourhood, resulting in a smoothed version. \\nSimilarly, smoothening of after image is carried out. \\n \\n• Calculating Flood extent-  \\nThe smoothed before image is subtracted from the \\nsmoothed after image i n a difference operation to get \\nan image that shows the difference between the two \\ntimes. A binary mask picture is then created by \\napplying a threshold to the difference image, where \\npixels with values less than - 3 are thought to indicate \\nthe extent of a f lood. In order to effectively mask out \\nall non-flood areas and produce a final binary image \\nthat highlights the flooded parts, the flood extent \\nmask is applied to itself using the \\'updateMask\\' \\nfunction.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\n \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\ndata, if available, can be loaded to determine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It then \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the   ability to add \\nadditional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts  \\noutlier identification. It uses a cumulative cost \\nmethod to determine the floodwater surface elevation \\nmodel, then uses a low -pass filter to determine the \\nfloodwater depth and smooth it out. Regular water \\nbodies and 0 values are excluded using optional \\nmasking processes.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='model, then uses a low -pass filter to determine the \\nfloodwater depth and smooth it out. Regular water \\nbodies and 0 values are excluded using optional \\nmasking processes.  \\nB. Methods For Landsat \\nThe following steps illustrateflood depth and flood area \\ncalculations of Landsat dataset using GEE. \\n• Gathering information, preprocessing it, and loading \\nthe LANDSAT/LC08/C02/T1_RT satellite dataset \\nand study area data into the GEE Code Editor – \\nA specific area of interest (AOI) corresponding to the \\nriver basin is selected from a table and added to the \\nmap for visualization. Then, a Landsat image \\ncollection is filtered to include only images from the \\ndate ranging within the b ounds of the previously \\ndefined AOI. The images are mosaicked into a single \\ncomposite, and the result is clipped to the same AOI. \\nTwo bands, B5 and B3, are selected from this dataset. \\nFor each band, visualization parameters are set to \\nspecify the minimum a nd maximum values for'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Two bands, B5 and B3, are selected from this dataset. \\nFor each band, visualization parameters are set to \\nspecify the minimum a nd maximum values for \\ndisplay within the specified range of 0.0 to 30,000.0. \\n• Visualizing NDWI - \\nTo visualize NDWI is carried out by subtracting \\nBand B2 from Band B1 and then dividing the result \\nby the sum of Band B1 and Band B2. Visualization \\nparameters ar e set with a minimum value of -1, a \\nmaximum value of 1, and a colour palette ranging \\nfrom white (for values less than -1), through red, to \\nblue (for values greater than 1). Finally, the NDWI \\nlayer is added into the map with the specified \\nvisualization settings which allows to visualize water \\nbodies and their intensity within the selected dataset. \\n• Calculating Flooded Area-  \\nA binary mask is made to identify the blue region in \\nthe NDWI (Normalized Difference Water Index) \\nlayer in order to determine the floode d area. The \\nNDWI \\nvalues between 0 and 1, which often depict aquatic'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='the NDWI (Normalized Difference Water Index) \\nlayer in order to determine the floode d area. The \\nNDWI \\nvalues between 0 and 1, which often depict aquatic \\nbodies, fall into this blue zone. By determining \\nwhether NDWI values are larger than 0 and less than \\n1, the binary mask is produced. To determine the \\nflooded area within each pixel, this m ask is then \\nmultiplied by the pixel area image. This procedure \\ngenerates a \"floodedAreaImage\" that enables the \\nquantification of flooded areas within the dataset. \\nEach pixel value reflects the area of the flooded zone \\nin square meters. \\n• Total Flooded Area Calculation – \\nThe entire flooded area is determined in square \\nmetres. First, a region reduction of the \\n\\'floodedAreaImage\\' is performed using the sum \\nreducer within the defined area of interest (AOI) \\ngeometry. To properly manage big datasets, a scale of \\n30 m eters, which corresponds to the resolution of \\nLandsat pictures, is established. The entire area that'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='geometry. To properly manage big datasets, a scale of \\n30 m eters, which corresponds to the resolution of \\nLandsat pictures, is established. The entire area that \\nhas been inundated is then calculated in square \\nmeters. The result, which represents the entire \\nflooded area in square kilometres, is displayed to the \\nconsole for reference after the value is divided by 1e6 \\nto show the result in square kilometres. \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\nFig. 1.  Methodology for Calculating Hazard Map.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\ndata, if available, can be loaded to d etermine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It then \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the ability to add \\nadditional water bodies to the flood  extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a \\ncumulative cost method to determine the floodwater \\nsurface elevation model, then uses a low -pass filter to \\ndetermine the floodwater depth and s mooth it out. \\nRegular water bodies and 0 values are excluded using \\noptional masking processes.  \\nC. Methods For Modis \\nThe following steps illustrate flood depth and flood area \\ncalculations of  MODIS dataset using GEE.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"optional masking processes.  \\nC. Methods For Modis \\nThe following steps illustrate flood depth and flood area \\ncalculations of  MODIS dataset using GEE. \\n• Gathering data, preprocessing it, and l oading the \\nStudy Area data and Satellite dataset \\n(MODIS/006/MOD09A1) – \\nThe area of interest (AOI) corresponding to the \\ntargeted river basin which is selected from the table \\nand added to the map for visualization. Next, a \\nMODIS Image Collection is filtered to include \\nimages from the range of flood occurred dates, within \\nthe bounds of the specified AOI. The images are then \\nmosaicked where in this case, the median value is \\nused to creating a composite image of the selected \\ndate, and the result is clipped to th e AOI. Two bands, \\n'sur_refl_b02' and 'sur_refl_b06', are selected from \\nthis dataset. Visualization parameters are set to define \\nthe display range for each band, and both b02 and \\nb06 bands are added as layers to the map, allowing\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"this dataset. Visualization parameters are set to define \\nthe display range for each band, and both b02 and \\nb06 bands are added as layers to the map, allowing \\nfor the visualization of MODIS data for the AOI river \\nbasin. \\n \\n• NDWI Calculation and Flooded Area Estimation - \\n'sur_refl_b02' (b02) and'sur_refl_b06' (b06) MODIS \\nbands are used to calculate the NDWI. An 'NDWI' \\nlayer is produced by computing the NDWI by \\ndeducting the near -infrared values from the green \\nvalues and dividing the result by the sum of b02 and \\nb06. This NDWI layer is added to the map, and the \\nparameters are set to display NDWI values in the \\nrange of - 1 to 1 with a colour palette ranging from \\nwhite to red to blue. Furtherm ore, a binary mask with \\nvalues between 0 and 1 is made for the blue region of \\nthe NDWI. Potential water bodies are identified by \\nthis 'blueRegionMask'. The pixel area image is then \\nmultiplied by this mask to enable the estimation of\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content=\"the NDWI. Potential water bodies are identified by \\nthis 'blueRegionMask'. The pixel area image is then \\nmultiplied by this mask to enable the estimation of \\nthe flooded area in each pixel, which can be valuable \\nfor flood analysis and mapping. \\n \\n• Total Flooded Area Calculation – \\nThere is a calculation of the total flooded area which \\nis calculated in square meters and is the total area \\nsubmerged by flooding. Using the sum reducer, a \\nregion reduction operation is performed on the \\n'floodedAreaImage,' which reflects the extent of \\nflooded regions within each pixel. This reduction is \\ncarried out inside the boundaries of the area of \\ninterest (AOI) that was previously specified. To \\nmanage huge datasets effectively, a specified scale of \\n30 meters, which corresponds to the resolution of the \\nMODIS data, is chosen (it can be changed as \\nnecessary). Additionally, a maximum pixel limit of 1 \\nbillion pixels is determined. Calculated results can be\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='MODIS data, is chosen (it can be changed as \\nnecessary). Additionally, a maximum pixel limit of 1 \\nbillion pixels is determined. Calculated results can be \\nfurther processed or displayed as needed for flood \\nassessment and analysis. The result, which represents \\nthe entire flooded area in square kilometres, is \\ndisplayed to the console for reference after the value \\nis divided by 1e6 to show the result in square \\nkilometres. \\n \\n• Calculating Flood Depth – \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\ndata, if available, can be loaded to determine the \\ndepth of the flood. The code then specifies a number \\nof data sources and processing stages options. It the n \\nloads worldwide surface water data and digital \\nelevation model (DEM) data, with the ability to add \\nadditional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='additional water bodies to the flood extent if \\nnecessary. The programme fills in gaps in the DEM \\ndata and conducts outlier identification. It uses a \\ncumulative cost method to determine the floodwater \\nsurface elevation model, then uses a low -pass filter to \\ndetermine the floodwater depth and smooth it out. \\nRegular water bodies and 0 values are excluded using \\noptional masking processes.  \\nV. R\\nESULTS AND DISCUSSION \\n    The proposed study evaluates the flood extent maps during \\nthe period of 2019 August to September in Mahanadi River \\nbasin. For this evaluation, calculated flood extent maps for the \\nindividual satellite data using above mentioned methods. Fig.  \\n2 depicts the flood extend map of sentinel data calculated by \\nadjusting the threshold-value.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 18th INDIACom; INDIACom-2024; IEEE Conference ID: 61295 \\n2024 11th International Conference on “Computing for Sustainable Global Development”, 28 th Feb-01st March, 2024  \\n \\n \\n \\nFig. 2.  Flood extent map using Sentinel data. \\nFig. 3 depicts the flood extent map of Landsat data, featuring \\nNDWI with bands B3 and B5. \\n \\nFig. 3. Flood extent map using Landsat data. \\nFig. 4 depicts the flood extent map of Modis data, with the \\nhelp of binary flood mask using band sur_refl_b0 1 and \\nsur_refl_b02. \\n \\nFig. 4. Flood extent map using Modis data. \\nThe above Fig. 4 illustrate the flood extent areas with \\nvariations in their values. The results obtained out of flood \\nextent maps are not that efficient to refer them for further \\nstudies. In order to refer a correct flood extent area, we need to \\napply certain techniques and methods to get an accurate flood \\nextent area. In this study an attempt is made to get an accurate'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='studies. In order to refer a correct flood extent area, we need to \\napply certain techniques and methods to get an accurate flood \\nextent area. In this study an attempt is made to get an accurate \\nflood extent area using the concept of difference mapping. To \\ngenerate a difference map, individual flood extent map are \\nused and the bands  of interest from both sensors are \\nsubtracted. The pixel -wise difference is calculated and the \\nobtained calculation is the accurate flood extent area. \\n \\nFig. 5. Difference map of Sentinel and Modis data \\nFig. 2, 3 and 4 illustrates the flood extent maps for  the \\nSentinel, Landsat and Modis data respectively showing a \\nsignificant difference between the flood extent area. Hence an \\nattempt is made to get the results more accurately using the \\ndifference maps. The difference map is generated using \\nsubtracting the bands of interest from both sensors and the \\npixel-wise difference is calculated.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='difference maps. The difference map is generated using \\nsubtracting the bands of interest from both sensors and the \\npixel-wise difference is calculated. \\nThe research related to the difference mapping for flood extent \\ncalculation is generally limited to the individual satellite data \\n[21], in this paper an attempt is made to cal culate the \\ndifference map for multiple satellite data in combination. \\nThe difference map is then encompassed with the GIS tool to \\nget the accurate results for further studies, hence as per user’s \\nchoice a combination of results would be obtained on one \\nplatform. This difference map can be used for generating flood \\ninundation maps, flood hazard maps, for monitoring the river \\ndynamics, time series analysis, land cover changes, etc \\nV.\\n CONCLUSION AND FUTURE SCOPE \\n Proposed study provides a comprehensive examina tion of \\nmultiple satellite datasets consolidated onto a singular platform. \\nThe subsequent evaluation and comparison of flood extent'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Proposed study provides a comprehensive examina tion of \\nmultiple satellite datasets consolidated onto a singular platform. \\nThe subsequent evaluation and comparison of flood extent \\nareas for each satellite data source revealed significant \\ndifferences, prompting a detailed analysis and the creation of a \\ndifference map to visualize and interpret these variations in \\naccuracy. This study contributes valuable insights for \\nresearchers and practitioners relying on satellite data for flood \\nmonitoring and related applications. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='Improving earth observations by correlating multiple satellite data: A Comparative Analysi s of Landsat, MODIS, and Sentinel \\nSatellite Data for flood mapping \\n \\n \\nfields of early warning systems, flood management, \\nresearchers, etc. \\n To quantify and visualize the observed differences in \\naccuracy among the various satellite datasets, we employed a \\nsystematic approach. Specifically, we generated a difference \\nmap that highlighted the disparities in flood extent \\nidentification across the differ ent satellite datasets. This map \\nserved as a valuable tool for effectively illustrating the \\nvariations in accuracy and allowed for a nuanced interpretation \\nof the discrepancies.Also, the difference maps play a vital role \\nin flood mapping by enabling the detection, analysis, and \\nmonitoring of changes in the landscape related to flooding. \\nThese maps provide valuable information for emergency \\nresponse, risk assessment, and long-term flood management.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='monitoring of changes in the landscape related to flooding. \\nThese maps provide valuable information for emergency \\nresponse, risk assessment, and long-term flood management. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the \\nfields of early warning systems, flood management, \\nresearchers, etc. But, certain limitations such as limited \\nspectral information, land cover changes or lack of data \\naccessibility may lead to the discrepancies in the results.  \\nR\\nEFERENCES \\n[1] Nsangou D, Kpoumié A, Mfonka Z, Bateni SM, Ngouh AN, Ndam          \\nNgoupayou JR, “The Mfoundi Watershed at Yaoundé in the \\nHumid Tropical Zone of Camer oon: a case study of urban food \\nsusceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='susceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol. \\n5, Issue 4, pp.861-871,2021.  \\n[3] Marinho G. Andrade, Marcelo D. Fragoso, Adriano A.F.M. \\nCarneiro, “A stochastic approach to the flood control problem,” \\nApplied Mathematical Modelling, vol. 25, Issue 6, 2001. \\n[4] Yiming Wei, Weixuan Xu, Ying Fan, Hsien -Tang Tasi, “Artificial \\nneural network based predi ctive method for flood disaster,” \\nComputers & Industrial Engineering vol. 42, Issues 2–4, 2002.  \\n[5] Shangyou Zhang, Ian Cordery, Ashish Sharma, “Application of an \\nimproved linear storage routing model for the estimation of large \\nfloods”, Journal of Hydrology, vol. 258, Issues 1–4, 2002. \\n[6] Karsten Jaspe, Joachim Gurtz, Herbert Lang, “Advanced flood \\nforecasting in Alpine watersheds by coupling meteorological \\nobservations and forecasts with a distributed hydrological model,”'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='[6] Karsten Jaspe, Joachim Gurtz, Herbert Lang, “Advanced flood \\nforecasting in Alpine watersheds by coupling meteorological \\nobservations and forecasts with a distributed hydrological model,” \\nJournal of Hydrology vol. 267, Issues 1–2,2002.  \\n[7] Thomas Rodding Kjeldsen, Jeff Smithers, Roland Schulze, \\n“Regional flood frequency analysis in the KwaZulu -Natal \\nprovince, South Africa, using the index -flood method,” Journal of \\nHydrology, vol. 255, Issues 1–4, 2002. \\n[8] Helder I. Chamine, Alcides  J. S. C. Pereira, Ana  C. Teodoro \\n“Remote sensing and  GIS applications in  earth and environmental \\nsystems sciences,” Springer Nature, vol. 3, 2021. \\n[9] Christopher J. Crawford, David P. Roy, Saeed Arab, \\nChristopherBarnes, et al., “The 50 -year Landsat collection 2 \\narchive,” Science of Remote Sensing, vol. 8, 2023. \\n[10] Christoph Kubitza, Vijesh Krishna, “Estimating adoption and \\nimpacts of agricultural management practices in developing'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='archive,” Science of Remote Sensing, vol. 8, 2023. \\n[10] Christoph Kubitza, Vijesh Krishna, “Estimating adoption and \\nimpacts of agricultural management practices in developing \\ncountries using satellite data. A scoping review,” Springer, vol. \\n40,2020. \\n[11] Sreechanth Sundaram, Suresh Devaraj & Kiran Yarrakula \\n“Modeling, mapping and analysis of urban floods in India —a \\nreview on geospatial methodologies,” Springer, vol. 28,2021. \\n[12] Alexei Lyapustin et al., “Calibration of the SNPP and NOAA 20 \\nVIIRS sensors for continuity of  the MODIS climate data records”, \\nRemote Sensing of Environment, vol. 295, 2023. \\n[13] M. Bellaoui, K. Bouchouicha, B. Oulimar, “Daily Global Solar \\nRadiation Based on MODIS Products: The Case Study of ADRAR \\nRegion (Algeria)”, Springer, vol. 102, 2019. \\n[14] Ekrem Sara lioglu, Can vatandaslar “Land use/land cover \\nclassification with Landsat -8 and Landsat -9 satellite images: a \\ncomparative analysis between forest - and agriculture -dominated'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='[14] Ekrem Sara lioglu, Can vatandaslar “Land use/land cover \\nclassification with Landsat -8 and Landsat -9 satellite images: a \\ncomparative analysis between forest - and agriculture -dominated \\nlandscapes using different machine learning methods”, Springer \\nvol. 57,2022. \\n[15] Beste Tavus, Sultan Kocaman, Candan Gokceoglu, “Flood damage \\nassessment with Sentinel-1 and Sentinel -2 data after Sardoba dam \\nbreak with GLCM features and Random Forest method,” Science \\nof The Total Environment vol. 816, 2022. \\n[16] Giacomo Caporusso, Marino Dell’Olo, et al, “Use of the Sentinel-1 \\nSatellite Data in the SNAP Platform and the WebGNOME \\nSimulation Model for Change Detection Analyses on the Persian \\nGulf Oil Spill,” Springer vol. 13379, 2022. \\n[17] Hamdi A.  Zurqani, “An automated approach for developing a \\nregional-scale 1 -m forest canopy cover dataset using machine \\nlearning and Google Earth Engine cloud computing platform,” \\nSoftware Impacts, vol. 19, 2024.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='regional-scale 1 -m forest canopy cover dataset using machine \\nlearning and Google Earth Engine cloud computing platform,” \\nSoftware Impacts, vol. 19, 2024. \\n[18] Jintao Liang a, Chao Chen b, Yongze Song c, Weiwei Sun d, Gang\\n Yang d, “Long-term mapping of land use and cover changes using \\nLandsat images on the Google Earth Engine Cloud Platform in bay \\narea - A case study of Hangzhou Bay, China,” Sustainable \\nHorizons, vol. 7, 2023. \\n[19] USGS, “What are the band designations for the Landsat satellites?”  \\n[Online]. Available: https://www.usgs.gov/faqs/what-are-band-\\ndesignations-landsat-satellites. [Accessed: 5- Dec- 2023]. \\n[20] Sadashiva Devadiga, “Terra & Aqua Moderate Resolution Imaging \\nSpectroradiometer (MODIS)” [Online] Available:  \\nhttps://ladsweb.modaps.eosdis.nasa.gov/missions-and-\\nmeasurements/modis/#:~:text=MODIS%20data%20products%2C\\n%20in%20three,and%20in%20the%20lower%20atmosphere. \\n[Accessed: 7- Dec- 2023].'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='https://ladsweb.modaps.eosdis.nasa.gov/missions-and-\\nmeasurements/modis/#:~:text=MODIS%20data%20products%2C\\n%20in%20three,and%20in%20the%20lower%20atmosphere. \\n[Accessed: 7- Dec- 2023]. \\n[21] Ramesh Sivan Pillai, Kevin M. Jacobs Chloe M. Mattilio, Ela V. \\nPiskorski, “Rapid flood inundation mapping by differen cing water \\nindices from pre - and post -flood Landsat images ,” Springer vol. \\n15,2021. \\n[22] Malay S. Bhatt, Tejas P. Patalia, “ Content-based high -resolution \\nsatellite image classification,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2018 vol. 11, 2019. \\n[23] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Proposal and evaluation of tsunami disaster drill support \\nsystem using tablet computer,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2023 vol. 15, 2023. \\n[24] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Bhoomi Prahari - e governance tool for monitoring'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}, page_content='of Computer Applications and Management 2023 vol. 15, 2023. \\n[24] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Bhoomi Prahari - e governance tool for monitoring \\nencroachment on government land using mobile and GIS \\ntechnology,” IJIT - Bharati Vidyapeeth’s Institute of Comp uter \\nApplications and Management 2023 vol. 14, 2023. \\n[25] Venkata Sangameswar Mandavillil, Nagabhushanarao Madamala, \\n“Detection of natural disaster affected areas using R,” IJIT- Bharati \\nVidyapeeth’s Institute of Computer Applications and Management \\n2018 vol. 10, 2018. \\n[26] Mohd. Tajammul, Rafat Parveen, “Auto encryption algorithm for \\nuploading data on cloud storage”, IJIT - Bharati Vidyapeeth’s \\nInstitute of Computer Applications and Management 2020  vol. \\n12,2020.\\n \\nView publication stats'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning\\nProject Option: Comparative Analysis\\nAVIPSA BHUJABAL, University at Buffalo, USA\\nCHANCHAL BUNDELE, University at Buffalo, USA\\nVISHAL LAVANGARE,University at Buffalo, USA\\n1 Abstract\\nThe goal of this project is to construct a robust image captioning\\npipeline capable of generating high-quality, semantically accurate\\ncaptions for actual images. We explore a number of architectures\\nto this end, beginning with a baseline CNN-LSTM pipeline using\\nDenseNet201 as the feature extractor and an LSTM decoder trained\\non the Flickr8k dataset. For comparison with newer models, we\\nadded BLIP (Bootstrapping Language-Image Pretraining), a state-\\nof-the-art Vision-Language Model (VLM) that has been extremely\\neffective for captioning tasks.\\nTo improve the factual correctness of generated captions and\\nreduce hallucination—when models describe objects that are not\\npresent in the image—we integrated object detection using YOLOv5.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='To improve the factual correctness of generated captions and\\nreduce hallucination—when models describe objects that are not\\npresent in the image—we integrated object detection using YOLOv5.\\nThis allowed us to identify the actual content of the image and\\neliminate semantically unnecessary or hallucinated words from\\ngenerated captions. This approach to reducing hallucination was\\nguided by methods discussed in papers our advisor exposed us to.\\nWe evaluated the models in terms of BLEU-1 to BLEU-4 scores and\\nalso plotted performance using t-SNE to verify clustering in image\\nfeature space. Quantitative and qualitative results show that BLIP\\nsignificantly outperforms traditional CNN-RNN models, producing\\nmore accurate and descriptive captions. The hallucination filtering\\nmethod enhanced the factual accuracy of the captions regarding\\nvisual content.\\nOverall, this project demonstrates a complete image captioning\\npipeline with a focus on benchmarking, hallucination detection, and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='visual content.\\nOverall, this project demonstrates a complete image captioning\\npipeline with a focus on benchmarking, hallucination detection, and\\nvisualization, offering a pragmatic platform for building semanti-\\ncally grounded captioning systems.\\n1.1 Why This is Interesting\\nImage captioning, the task of generating natural language descrip-\\ntions from images, has recently emerged as a sought-after problem\\ndue to its extensive applications in accessibility aides, content-based\\nimage retrieval systems, social media automation, and human-robot\\ninteraction. The prime challenge lies in effectively bridging the\\nvision-language gap—capturing the content in an image and con-\\nveying it through coherent, meaningful text.\\nTraditionally, encoder-decoder architectures have been employed,\\nwhere a convolutional neural network (CNN) extracts image fea-\\ntures and a recurrent neural network (RNN), such as LSTM, gen-\\nerates a caption. Although these models initially demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='where a convolutional neural network (CNN) extracts image fea-\\ntures and a recurrent neural network (RNN), such as LSTM, gen-\\nerates a caption. Although these models initially demonstrated\\npromise, they often suffered from poor generalization and hallu-\\ncination—generating information not present in the image. The\\nadvent of pretrained vision-language models (VLMs) like BLIP and\\nAuthors’ Contact Information: Avipsa Bhujabal, University at Buffalo, Buffalo, USA,\\nNew York, avipsabh@buffalo.edu; Chanchal Bundele, University at Buffalo, Buffalo,\\nUSA, cbundele@buffalo.edu; Vishal Lavangare, University at Buffalo, Buffalo, USA,\\nvlavanga@buffalo.edu.\\nCLIP marked a significant shift toward multimodal transformers,\\nresulting in enhanced performance across captioning, retrieval, and\\nvisual question answering (VQA) tasks.\\nOur research presents a full image captioning pipeline that com-\\npares a traditional CNN-RNN model using DenseNet201 with a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='visual question answering (VQA) tasks.\\nOur research presents a full image captioning pipeline that com-\\npares a traditional CNN-RNN model using DenseNet201 with a\\nVLM-based model using BLIP. Beyond performance evaluation, we\\nincorporate object detection using YOLOv5 to address hallucination\\nby anchoring captions to actual detected objects, inspired by recent\\nefforts in semantic grounding.\\nThe motivation stems from real-world implications—such as in\\nassistive technology for the visually impaired, where hallucinated\\ncaptions (e.g., describing a dog that isn’t present) can mislead users.\\nA hallucination-aware captioning model enhances the reliability\\nand informativeness of AI-generated descriptions.\\nWe train both baseline and state-of-the-art models on the Flickr8k\\ndataset, integrate hallucination filtering, and evaluate them using\\nBLEU scores and visualizations such as t-SNE. Our key contributions\\ninclude:\\n• A comparative benchmark of traditional and modern image\\ncaptioning models.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='BLEU scores and visualizations such as t-SNE. Our key contributions\\ninclude:\\n• A comparative benchmark of traditional and modern image\\ncaptioning models.\\n• A hallucination filtering module leveraging object detection\\nto ground captions in detected visual content.\\n• Visualization tools to interpret model behavior and feature\\nrepresentations.\\nIn the broader multimodal AI landscape, this work offers a practi-\\ncal and educational examination of how emerging technologies can\\nbe harnessed to address persistent challenges in image captioning,\\nnotably hallucination and factual consistency.\\n2 Related Work\\nThe field of image captioning has evolved significantly, with nu-\\nmerous models tackling the challenge of generating accurate and\\ncontextually relevant descriptions from images.\\n• Show and Tell [1]: Introduced the foundational encoder-\\ndecoder architecture using CNNs and RNNs. It produced\\ngeneric captions but struggled with novel object descrip-\\ntions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Show and Tell [1]: Introduced the foundational encoder-\\ndecoder architecture using CNNs and RNNs. It produced\\ngeneric captions but struggled with novel object descrip-\\ntions.\\n• Show, Attend and Tell [2]: Employed attention mecha-\\nnisms to focus on specific regions of the image. Despite\\nimprovements, it sometimes misaligned attention, leading\\nto incorrect descriptions.\\n• Bottom-Up and Top-Down Attention [3]: Combined ob-\\nject detection and attention to improve detail, though it\\noccasionally hallucinated non-existent objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='2 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\n• Adversarial Semantic Alignment [4]: Leveraged GANs\\nto improve semantic alignment between captions and im-\\nage content. However, training GANs on discrete text data\\nremains a challenge.\\n• Visual Fact Checker (VFC) [5]: Proposed a training-free\\ncaption fact-checking system using object detection and\\nVQA. Its effectiveness depends on the external tools’ quality.\\n• BLIP [6]: A vision-language model achieving state-of-the-\\nart results across multiple tasks, though prone to hallucina-\\ntion without explicit grounding.\\n• ALOHa [7]: Introduced a large-language-model-based hal-\\nlucination detection metric, improving on previous fixed-\\nvocabulary approaches.\\n• Reducing Object Hallucination [8]: Proposed training\\nobjective optimization and object presence constraints to\\nreduce hallucination.\\n• Object Grounding and Hallucination [9]: Found that\\ngrounding alone may not sufficiently reduce hallucination,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='objective optimization and object presence constraints to\\nreduce hallucination.\\n• Object Grounding and Hallucination [9]: Found that\\ngrounding alone may not sufficiently reduce hallucination,\\nemphasizing the need for holistic strategies.\\n• Scalable Vision Learners [10]: Showed that captioning\\nmodels can be robust visual learners with appropriate scale\\nand fine-tuning.\\n• EVCap [11]: Used external memory for object name retrieval\\nto improve open-world captioning.\\n• Multimodal Hallucination Detection in 3D [12]: Ad-\\ndressed hallucination across modalities for 3D content but\\nfocused on a different domain.\\n• Captioning Evaluation with LLMs [13]: Highlighted the\\nneed for better metrics capturing hallucination and factual\\nconsistency in image captioning.\\n• Pseudo Content Hallucination[14]: Used pseudo-generated\\ncontent for unpaired image captioning, albeit with potential\\nnoise.\\n• Hyperbolic Learning for Open-World Detection [15]:\\nApplied synthetic captions in hyperbolic space to improve'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='content for unpaired image captioning, albeit with potential\\nnoise.\\n• Hyperbolic Learning for Open-World Detection [15]:\\nApplied synthetic captions in hyperbolic space to improve\\ndetection, aiding captioning tasks.\\n3 Proposed Approach and Contributions\\nIn this paper, we present an end-to-end image captioning pipeline\\nthat synergizes state-of-the-art vision-language models with ad-\\nvanced evaluation methods to generate accurate and contextually\\nrelevant captions. Our approach centers around comparing the per-\\nformance of BLIP (Bootstrapped Language-Image Pretraining), a\\nstate-of-the-art vision-language model, to that of a CNN-RNN-based\\ncaptioning architecture using DenseNet201 features. Additionally,\\nwe integrate YOLOv5-based object detection to mitigate hallucinated\\nobjects in generated captions—addressing the well-documented is-\\nsue of hallucination in captioning models.\\n3.1 Primary Contributions\\n(1) Multi-model Captioning Benchmark: We establish a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='objects in generated captions—addressing the well-documented is-\\nsue of hallucination in captioning models.\\n3.1 Primary Contributions\\n(1) Multi-model Captioning Benchmark: We establish a\\ncomparative benchmark between a traditional CNN-RNN\\narchitecture and a transformer-based vision-language model\\n(BLIP). Using evaluation metrics such as BLEU, we demon-\\nstrate the superior performance of recent pre-trained trans-\\nformers in generating coherent and relevant image captions.\\n(2) Hallucination Reduction via Object Detection: Drawing\\ninspiration from recent research in hallucination detection\\nand semantic grounding, we employ YOLOv5 to detect ob-\\njects within the image and remove tokens in the generated\\ncaptions that lack visual grounding. This results in more\\nfactual and accurate descriptions.\\n(3) Interactive Visual Analysis: We develop side-by-side visu-\\nalizations that display input images, captions generated by\\nboth BLIP and CNN-RNN models, and highlight hallucinated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='(3) Interactive Visual Analysis: We develop side-by-side visu-\\nalizations that display input images, captions generated by\\nboth BLIP and CNN-RNN models, and highlight hallucinated\\ncontent. These visual aids enhance model interpretability\\nand support thorough error analysis.\\n(4) Robust Preprocessing and Dataset Exploration: We ap-\\nply preprocessing techniques such as lemmatization, stop-\\nword removal, and caption length filtering on the Flickr8k\\ndataset. These steps contribute to a cleaner training dataset\\nand more reliable tokenizer performance.\\n(5) End-to-End Pipeline with BLIP + YOLOv5 Integration:\\nTo the best of our knowledge, this work is among the first in\\ncoursework to combine BLIP-based captioning with YOLOv5-\\nbased hallucination filtering, DenseNet-based baselines, and\\ninteractive visualizations in a cohesive and modular pipeline.\\nOur work bridges the gap between captioning accuracy and fac-\\ntual coherence, offering a flexible framework that can be extended'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='interactive visualizations in a cohesive and modular pipeline.\\nOur work bridges the gap between captioning accuracy and fac-\\ntual coherence, offering a flexible framework that can be extended\\nwith other vision-language models such as CLIP or grounding-\\nspecific techniques. All code is made publicly available and is Colab-\\nfriendly to support reproducibility and experimentation.\\n4 Methodology\\nOur project aims to solve the problem of automatic image captioning\\nusing both conventional deep learning models and state-of-the-\\nart transformer-based models. We explore two primary modeling\\napproaches, along with hallucination filtering and comprehensive\\nevaluation.\\n4.1 CNN-RNN Baseline using DenseNet201 + LSTM\\nDecoder\\nWe adopt a standard encoder-decoder architecture comprising:\\n• Encoder: DenseNet201, a deep CNN pretrained on Ima-\\ngeNet, is used to extract 1920-dimensional feature vectors\\nfrom input images. The classification head is removed to\\nretain only visual embeddings.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='geNet, is used to extract 1920-dimensional feature vectors\\nfrom input images. The classification head is removed to\\nretain only visual embeddings.\\n• Decoder: An LSTM-based RNN processes the image fea-\\ntures concatenated with embedded word sequences to pre-\\ndict the next word. The decoder includes dropout layers for\\nregularization and a softmax output over the vocabulary.\\nThis model is trained using a custom data generator that dynam-\\nically pairs image features with caption sequences. Captions are\\ntokenized into (input, output) word pairs, and sequences are\\npadded and one-hot encoded.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 3\\n4.2 BLIP: Bootstrapped Language-Image Pretraining\\nWe leverage BLIP, a state-of-the-art pretrained vision-language\\nmodel from Salesforce. BLIP uses a Vision Transformer (ViT) en-\\ncoder and a language transformer decoder. Unlike our CNN-RNN\\nbaseline, BLIP directly takes raw images and generates captions\\nthrough pretrained attention mechanisms. We initialize BLIP in\\nzero-shot mode via the Hugging Face Transformers library, with-\\nout further fine-tuning. This enables a comparative study between\\nhandcrafted and pretrained models.\\n4.3 Hallucination Minimization with YOLOv5 Integration\\nTo address the common issue of hallucination—generating captions\\nwith objects not present in the image—we integrate the YOLOv5\\nobject detection framework. For each test image:\\n• YOLOv5 is used to detect objects in the image.\\n• The generated caption is post-processed, and non-grounded'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='object detection framework. For each test image:\\n• YOLOv5 is used to detect objects in the image.\\n• The generated caption is post-processed, and non-grounded\\ntokens (not in the detected object list) are removed, exclud-\\ning stopwords and function words.\\nThis approach semantically aligns captions with detected content,\\ndrawing on current research in factual grounding and hallucination\\nmitigation.\\n4.4 Evaluation and Visualization\\n• We evaluate captions using BLEU-1 to BLEU-4 scores.\\n• We visualize results by showing:\\n– Original image with ground truth caption\\n– DenseNet-LSTM-generated caption\\n– BLIP-generated caption\\n– YOLOv5-filtered caption (hallucination-minimized)\\n• We apply t-SNE and KMeans clustering on image features\\nto analyze semantic grouping.\\n• Additional metrics such as METEOR and ROUGE are calcu-\\nlated using the nlg-eval toolkit where available.\\n5 Preprocessing, Model Architecture, and Training\\n5.1 Preprocessing and Data Preparation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Additional metrics such as METEOR and ROUGE are calcu-\\nlated using the nlg-eval toolkit where available.\\n5 Preprocessing, Model Architecture, and Training\\n5.1 Preprocessing and Data Preparation\\nImage Preprocessing. To ensure consistency and model compatibility,\\nthe following steps are performed on raw images:\\n• Resizing: All images are resized to224 × 224 pixels to match\\nthe input requirement of DenseNet201.\\n• Normalization: Pixel values are normalized to the range\\n[0, 1] by dividing each pixel by 255. This aids in faster con-\\nvergence during training.\\n• Feature Extraction: DenseNet201, pretrained on ImageNet,\\nis used without its final classification layer. We extract the\\n1920-dimensional feature vector from the second-to-last\\nlayer, capturing high-level semantics.\\n• Feature Storage: Extracted features are stored in a dic-\\ntionary with image filenames as keys to avoid redundant\\ncomputation and accelerate model training.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='layer, capturing high-level semantics.\\n• Feature Storage: Extracted features are stored in a dic-\\ntionary with image filenames as keys to avoid redundant\\ncomputation and accelerate model training.\\nText Preprocessing. Since raw captions are not model-compatible,\\nwe perform the following preprocessing:\\n• Lowercasing: Converts all text to lowercase to avoid case-\\nbased duplicates.\\n• Punctuation and Number Removal: Uses regular expres-\\nsions to clean special characters and numeric values.\\n• Whitespace Normalization: Removes extra spaces and\\nnormalizes the spacing.\\n• Short Word Filtering: Removes tokens with fewer than\\ntwo characters.\\n• Lemmatization: Reduces words to their root form using\\nspaCy.\\n• Stopword Removal: Filters out common stopwords for\\nbetter semantic focus.\\n• Start/End Tokens: Captions are enclosed with startseq\\nand endseq to define boundaries for generation.\\nTokenization and Splitting.\\n• Tokenizer: Keras’sTokenizer is used to convert words into'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Start/End Tokens: Captions are enclosed with startseq\\nand endseq to define boundaries for generation.\\nTokenization and Splitting.\\n• Tokenizer: Keras’sTokenizer is used to convert words into\\ninteger indices, resulting in a vocabulary of approximately\\n6700 words.\\n• Max Caption Length: The longest caption length ( 34 to-\\nkens) is saved and used for sequence padding.\\n• Padding: All sequences are padded to the maximum length\\nto standardize input shapes.\\n• Train-Test Split: The dataset is split into 85% training and\\n15% validation, ensuring no image overlap.\\n5.2 Model Architecture\\nVisual Feature Extraction using DenseNet201.\\n• Dense Connectivity: Promotes gradient flow and feature\\nreuse.\\n• Transfer Learning:Leverages pretrained ImageNet weights\\nfor faster convergence.\\n• Output Feature Vector: Extracts a 1920-dimensional rep-\\nresentation per image.\\nTextual Decoder with LSTM. The caption generation model includes:\\n• Embedding Layer: Transforms tokenized words into dense'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='resentation per image.\\nTextual Decoder with LSTM. The caption generation model includes:\\n• Embedding Layer: Transforms tokenized words into dense\\n256-dimensional vectors.\\n• Image Compression: The 1920-dimensional feature vec-\\ntor is processed through a dense layer to align with text\\nembeddings.\\n• Concatenation: The processed image vector is concate-\\nnated with the embedded caption sequence.\\n• LSTM Decoder: A 512-unit LSTM processes the combined\\nsequence to predict the next word.\\n• Residual Connection: Adds LSTM output back to the im-\\nage vector to reinforce visual grounding.\\n• Dense Layers: A fully connected Dense(128) layer followed\\nby a Dense layer with softmax activation over the vocabu-\\nlary.\\nThe final model contains approximately 5.74 million trainable\\nparameters.\\n5.3 Training Strategy\\nCustom Data Generator. To accommodate varying caption lengths\\nand memory constraints, a custom Keras Sequence generator is\\nimplemented to:\\n• Load precomputed image features.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='4 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\n• Dynamically generate (input sequence, target word) training\\npairs.\\n• Apply tokenization, padding, and one-hot encoding on-the-\\nfly.\\nTraining Configuration.\\n• Loss Function: Categorical Crossentropy for multi-class\\nword prediction.\\n• Optimizer: Adam with a learning rate of 0.001.\\n• Batch Size: 64\\n• Epochs: Trained for up to 30 epochs.\\n• Callbacks:\\n– ModelCheckpoint: Saves the model with the lowest\\nvalidation loss.\\n– EarlyStopping: Stops training if no improvement is\\nseen for 5 epochs.\\n– ReduceLROnPlateau: Halves the learning rate if vali-\\ndation loss plateaus for 3 epochs.\\n6 Evaluation and Results\\nEvaluation Metrics\\nWe evaluate the performance of our captioning models using the\\nfollowing metrics:\\n• BLEU-1 to BLEU-4: Measures n-gram precision between\\ngenerated and reference captions.\\n• METEOR: Considers synonym matching and word stems,\\nproviding recall-oriented evaluation.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='following metrics:\\n• BLEU-1 to BLEU-4: Measures n-gram precision between\\ngenerated and reference captions.\\n• METEOR: Considers synonym matching and word stems,\\nproviding recall-oriented evaluation.\\n• ROUGE-L: Evaluates the longest common subsequence be-\\ntween candidate and reference captions.\\n• CIDEr: Measures consensus between generated captions\\nand human annotations. Due to the limited size of Flickr8k,\\nCIDEr scores were found to be low.\\n7 Flickr8k Dataset Overview\\nFor our project, we use the Flickr8k dataset, a widely used bench-\\nmarking dataset for automatic image captioning. It is a collection\\nof images with multiple human-written captions, making it an ideal\\ntool for evaluating the performance of machine learning models in\\ngenerating textual descriptions.\\n7.1 Characteristics of the Dataset\\n• Source: The dataset was introduced by Hodosh, Young, and\\nHockenmaier (2013).\\n• Accessible on Kaggle: https://www.kaggle.com/datasets/\\nadityajn105/flickr8k\\n• Size:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='7.1 Characteristics of the Dataset\\n• Source: The dataset was introduced by Hodosh, Young, and\\nHockenmaier (2013).\\n• Accessible on Kaggle: https://www.kaggle.com/datasets/\\nadityajn105/flickr8k\\n• Size:\\n– Total Images: 8,092 JPEG images.\\n– Total Captions: 40,000 human-written captions.\\n– Captions per Image: Each image has five different\\ncaptions written by human annotators.\\n• Format:\\n– Images are available in JPEG format.\\n– Captions are stored in a text file linking each image to\\nits corresponding descriptions.\\nFig. 1. Sample Dataset\\n7.2 Challenges in the Flickr8k Dataset\\n• Data Size Constraint: Unlike MS-COCO (123,000+ im-\\nages), Flickr8k contains only 8,092 images, making gener-\\nalization more difficult.\\n• Caption Variability: Each image has multiple captions,\\nintroducing lexical and syntactic differences. Evaluating\\ncaptions is challenging due to varying levels of detail.\\n• Diverse Image Content: The dataset includes human be-\\nhavior, animals, objects, and natural scenes , requiring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='captions is challenging due to varying levels of detail.\\n• Diverse Image Content: The dataset includes human be-\\nhavior, animals, objects, and natural scenes , requiring\\nmodels to handle contextual understanding.\\n• No Explicit Object Annotations:Unlike MS-COCO, Flickr8k\\ndoes not include bounding box annotations. Models must\\ninfer semantic relationships from image features alone.\\n• Computational Efficiency: The dataset’ssmall size al-\\nlows for rapid model training and testing , but high-\\ncapacity models like transformers risk overfitting with-\\nout sufficient regularization.\\n8 Why Flickr8k?\\nThe Flickr8k dataset is chosen because it allows us to:\\n• Evaluate models under low-resource conditions.\\n• Analyze performance on a dataset with diverse cap-\\ntions and image types.\\n• Benchmark models without requiring massive com-\\nputational resources.\\nTo mitigate dataset limitations, we employ:\\n• Pretrained Transformer Models (e.g., BLIP, GIT) to lever-\\nage larger datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='• Benchmark models without requiring massive com-\\nputational resources.\\nTo mitigate dataset limitations, we employ:\\n• Pretrained Transformer Models (e.g., BLIP, GIT) to lever-\\nage larger datasets.\\n• Data Augmentation techniques to artificially increase cap-\\ntion diversity.\\n• Regularization & Fine-tuning to reduce overfitting risks.\\n9 Results\\nThis section presents the experimental evaluations of the proposed\\nCNN-LSTM-based image captioning model. We benchmarked our\\nmodel against the state-of-the-art BLIP model to assess both gener-\\native quality and factual grounding. Additionally, we analyzed the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 5\\neffect of hallucination filtering on caption accuracy using YOLOv5\\nobject detection.\\n9.1 Evaluation Setup\\nWe used a subset of theFlickr8k dataset comprising 6,000 training\\nand 2,000 validation images. Each image has five corresponding\\ncaptions. The following metrics were used to quantitatively evaluate\\ncaptioning quality:\\n• BLEU (1 to 4): N-gram precision.\\n• METEOR: Semantic matching with synonyms and stems.\\n• ROUGE-L: Longest common subsequence overlap.\\n• CIDEr: Consensus-based evaluation (ineffective on small\\ndatasets).\\nThe models compared:\\n• Baseline: CNN-LSTM using DenseNet201 features.\\n• Transformer-based: BLIP (Bootstrapped Language-Image\\nPretraining).\\n9.2 Training and Validation Loss Analysis\\nFigure 2 presents the training and validation loss curves observed\\nduring model training. Initially, both losses decrease steadily, in-\\ndicating that the model is learning effective representations from'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='during model training. Initially, both losses decrease steadily, in-\\ndicating that the model is learning effective representations from\\nthe image-caption pairs. After around the 5th epoch, the validation\\nloss begins to plateau while the training loss continues to decline,\\nsuggesting the model is starting to slightly overfit the training data.\\nDespite this, the gap between training and validation loss remains\\nsmall, which indicates good generalization. The trend suggests that\\nour custom data generator and regularization techniques, such as\\ndropout, are effective in mitigating overfitting. This learning behav-\\nior supports the robustness of the training setup and highlights the\\nmodel’s ability to capture the underlying semantics required for\\ngenerating meaningful image captions.\\n9.3 Feature Representation Analysis\\nTo evaluate how well our feature extractor (DenseNet201) captures\\nthe semantic diversity in images, we employ t-SNE (t-Distributed'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='9.3 Feature Representation Analysis\\nTo evaluate how well our feature extractor (DenseNet201) captures\\nthe semantic diversity in images, we employ t-SNE (t-Distributed\\nStochastic Neighbor Embedding) for dimensionality reduction and\\nvisualization. This technique allows us to project high-dimensional\\nfeature vectors into a 2D plane, revealing patterns and clusters.\\n• Figure 3 displays the image-level t-SNE representation clus-\\ntered using KMeans. This provides qualitative evidence that\\nsimilar image types group together in the feature space.\\nThese plots confirm that DenseNet201-derived features retain\\nmeaningful distinctions necessary for downstream caption genera-\\ntion.\\nFig. 3. t-SNE + KMeans cluster visualization of image features with thumb-\\nnails from the Flickr8k dataset. Similar images tend to group together,\\nvalidating our DenseNet feature extractor.\\n9.4 Qualitative Comparison of Captions\\nTo gain a more intuitive understanding of model behavior, we'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='validating our DenseNet feature extractor.\\n9.4 Qualitative Comparison of Captions\\nTo gain a more intuitive understanding of model behavior, we\\npresent side-by-side comparisons of image captions generated by\\nDenseNet and BLIP. Each example includes the original image, the\\nground-truth caption, predictions by both models, and their respec-\\ntive CLIP similarity scores. This helps assess factual grounding and\\nsemantic accuracy.\\n9.5 Quantitative Results\\nThese examples demonstrate BLIP’s ability to generate semantically\\nrich and grounded captions compared to DenseNet, particularly in\\ncomplex scenes involving multiple objects or fine-grained relation-\\nships.\\nTable 1. Average Metric Comparison Across 100 Test Samples\\nMetric DenseNet Model BLIP Model\\nBLEU-1 0.2078 0.1512\\nBLEU-2 0.0936 0.0778\\nBLEU-3 0.0519 0.0055\\nBLEU-4 0.0319 0.0000\\nMETEOR 0.1231 0.1165\\nROUGE-L 0.2654 0.2479\\nCIDEr 0.0000 0.0000\\nInsight: Despite BLIP’s advanced architecture, the CNN-LSTM'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='BLEU-2 0.0936 0.0778\\nBLEU-3 0.0519 0.0055\\nBLEU-4 0.0319 0.0000\\nMETEOR 0.1231 0.1165\\nROUGE-L 0.2654 0.2479\\nCIDEr 0.0000 0.0000\\nInsight: Despite BLIP’s advanced architecture, the CNN-LSTM\\nmodel showed stronger performance on n-gram-based metrics due\\nto better generalization on smaller datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='6 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\nFig. 2. Training vs. Validation Loss and BLEU Score Comparison Before and After Hallucination Filtering. The left plot shows the convergence of training\\nand validation loss, while the table on the right highlights the drop in BLEU scores after filtering hallucinated tokens using YOLOv5-detected objects. This\\nvalidates that the original model tends to hallucinate terms not grounded in visual input.\\nFig. 4. Example: BLIP generates a more semantically rich caption (\"snowy\\nslope\") and yields a significantly higher CLIP score than DenseNet, reflecting\\nimproved alignment with the visual content.\\n9.6 BLEU Score Comparison\\nTable 2. BLEU-1 Score Comparison (Sample Batch)\\nModel BLEU-1 Score\\nDenseNet 0.2284\\nBLIP 0.1751\\n9.7 Hallucination Filtering with YOLOv5\\nWe employed YOLOv5 object detection to detect visual elements\\nand filter out non-grounded words from the generated captions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='DenseNet 0.2284\\nBLIP 0.1751\\n9.7 Hallucination Filtering with YOLOv5\\nWe employed YOLOv5 object detection to detect visual elements\\nand filter out non-grounded words from the generated captions.\\nFig. 5. Example: While both models identify the dog, DenseNet overlooks\\nthe muzzle detail present in the ground truth. BLIP provides a more contex-\\ntually accurate description but still misses that specificity.\\nTable 3. BLEU Score Before and After Hallucination Filtering\\nFiltering Stage BLEU-1 BLEU-2\\nBefore 0.1814 0.1016\\nAfter 0.0313 0.0132\\nObservation: The substantial drop in BLEU scores post-filtering\\nconfirms the presence of hallucinated tokens that were not visually\\ngrounded.\\n9.8 Qualitative Results\\nWe visualized predictions of both models for randomly selected im-\\nages. DenseNet-based captions appeared more factual, while BLIP’s'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Comparative Analysis of ML Models for Automatic Image Captioning • 7\\nFig. 6. Example: BLIP correctly identifies the main subjects—a lion and\\na buffalo—demonstrating its grounding strength. DenseNet, on the other\\nhand, misidentifies the context entirely.\\nFig. 7. Example: While both models recognize a running dog, only the\\nground truth acknowledges the muzzle, which is missed by BLIP and\\nDenseNet. The omission lowers their CLIP scores, highlighting the challenge\\nin object-level grounding.\\nwere descriptive but often hallucinated objects not present in the\\nimage.\\n9.9 Summary of Findings\\n• CNN-LSTM outperforms BLIP in BLEU and ROUGE metrics\\non Flickr8k.\\n• Hallucination is a serious issue in transformer-based models\\nunder small data regimes.\\n• YOLOv5-based hallucination filtering helped us identify se-\\nmantic inconsistencies.\\n10 Conclusion\\nIn this project, we did the job of automatic image captioning us-\\ning traditional and state-of-the-art approaches. We developed a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='mantic inconsistencies.\\n10 Conclusion\\nIn this project, we did the job of automatic image captioning us-\\ning traditional and state-of-the-art approaches. We developed a\\nCNN-LSTM model where DenseNet201 was used as a visual feature\\nextractor and a custom LSTM-based decoder to decode text descrip-\\ntions. In addition to this, we also compared this baseline with recent\\ntransformer-based vision-language models like BLIP.\\nExperiments, conducted on the Flickr8k dataset, reveal that transformer-\\nbased models like BLIP outperform traditional CNN-LSTM architec-\\ntures on qualitative and quantitative metrics like BLEU scores. BLIP\\nuse led to more fluent, varied, and contextually appropriate captions.\\nAside from this, we also incorporated hallucination detection from\\nrecent literature and assessed whether the generated captions were\\ngrounded in visual content.\\nThe project illustrates the increasing capability of pre-trained\\nVLMs to generate high-quality captions, especially when fine-tuned'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='grounded in visual content.\\nThe project illustrates the increasing capability of pre-trained\\nVLMs to generate high-quality captions, especially when fine-tuned\\non heterogeneous datasets. Training and generalization, however,\\nremain sensitive to dataset size and feature quality. We also observed\\nthat lighter-weight models like CNN-LSTM, though less expressive,\\ncan be used in resource-constrained environments.\\nIn the future, we will:\\n• Incorporate CLIP-based benchmarking for better alignment\\nof image-text embeddings.\\n• Fine-tune BLIP and other vision-language models on domain-\\nspecific caption datasets.\\n• Experiment with multimodal grounding techniques to re-\\nduce hallucinations and improve factual consistency even\\nmore.\\n• Scale to larger datasets like MS-COCO or NoCaps to verify\\ngeneralization.\\nOverall, our project provides insights into how generation quality\\nand visual semantic alignment differ between conventional and\\ntransformer-based frameworks.\\n11 Individual Contributions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Overall, our project provides insights into how generation quality\\nand visual semantic alignment differ between conventional and\\ntransformer-based frameworks.\\n11 Individual Contributions\\n11.1 Vishal\\nVishal led the data preprocessing and image feature extraction\\npipeline. He implemented techniques to resize, normalize, and clean\\nimage-caption pairs effectively. He integrated DenseNet201 as a\\npretrained CNN for extracting high-dimensional feature vectors,\\nstored efficiently for reuse. Vishal also managed aspects of caption\\ntokenization and ensured compatibility between the image and text\\nmodalities during training.\\n11.2 Avipsa\\nAvipsa focused on model architecture development and com-\\nparison. She implemented the CNN-LSTM-based caption gener-\\nation pipeline and worked on training configurations such as hy-\\nperparameter tuning, dropout strategies, and callbacks . She\\nalso integrated BLIP, a transformer-based model, for captioning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='ation pipeline and worked on training configurations such as hy-\\nperparameter tuning, dropout strategies, and callbacks . She\\nalso integrated BLIP, a transformer-based model, for captioning\\ncomparison, and generated visualizations for model output, loss\\ntrends, and BLEU-based evaluations. Avipsa further contributed to\\nhallucination filtering using YOLOv5 object detection, addressing\\ngrounding issues as cited in recent literature.\\n11.3 Chanchal\\nChanchal handled evaluation, visualization, and benchmarking.\\nShe performed metric analysis using BLEU, METEOR, ROUGE,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='8 • Avipsa Bhujabal, Chanchal Bundele, and Vishal Lavangare\\nand CIDEr , and supported hallucination evaluation using pre-\\nand post-filter BLEU scores. She contributed to generating t-SNE\\nplots and clustering visualizations to interpret model embed-\\ndings. Chanchal also began exploring CLIP-based benchmarks\\nand worked on preparing comprehensive documentation for final\\nreporting and presentation.\\n12 REFERENCES\\nReferences\\n[1] Vinyals et al., Show and Tell: A Neural Image Caption Generator , CVPR 2015.\\nhttps://arxiv.org/abs/1411.4555\\n[2] Xu et al., Show, Attend and Tell, ICML 2015. https://arxiv.org/abs/1502.03044\\n[3] Anderson et al., Bottom-Up and Top-Down Attention , CVPR 2018. https://arxiv.\\norg/abs/1707.07998\\n[4] Dognin et al., Adversarial Semantic Alignment , CVPR 2019. https:\\n//openaccess.thecvf.com/content_CVPR_2019/papers/Dognin_Adversarial_\\nSemantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.\\ncom/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-\\nFidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf\\n[6] Li et al., BLIP, ICML 2022. https://proceedings.mlr.press/v162/li22n.html\\n[7] CVPR 2024. https://arxiv.org/abs/2404.02904\\n[8] Biten et al., Let There Be a Clock on the Beach , WACV 2022. https://arxiv.org/abs/\\n2110.01705\\n[9] EMNLP 2024. https://arxiv.org/abs/2406.14492\\n[10] NeurIPS 2023. https://arxiv.org/abs/2306.07915\\n[11] Li et al., EVCap, CVPR 2024. https://arxiv.org/abs/2311.15879\\n[12] NeurIPS 2024. https://papers.nips.cc/paper_files/paper/2024/hash/\\nd75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html\\n[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'creationdate': '2025-05-10T14:29:11+00:00', 'moddate': '2025-05-10T14:29:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'Team 28-VLM Hallucination.pdf', 'file_type': 'pdf'}, page_content='[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.\\n[15] Hyperbolic Learning with Synthetic Captions , CVPR 2024.\\nProject Repository\\nThe complete code and resources for this project can be found on\\nGitHub:\\nhttps://github.com/Avipsa-Bhujabal/Image𝐶𝑎𝑝𝑡𝑖𝑜𝑛𝑖𝑛𝑔 𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174dd782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 85 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (85, 384)\n",
      "Adding 85 documents to vector store...\n",
      "Successfully added 85 documents to vector store\n",
      "Total documents in collection: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db452ac",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7430dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a028a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2b497851490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189cf26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the image captioning research paper explaining?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 65.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_254c49a3_84',\n",
       "  'content': '[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.\\n[15] Hyperbolic Learning with Synthetic Captions , CVPR 2024.\\nProject Repository\\nThe complete code and resources for this project can be found on\\nGitHub:\\nhttps://github.com/Avipsa-Bhujabal/Image𝐶𝑎𝑝𝑡𝑖𝑜𝑛𝑖𝑛𝑔 𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛',\n",
       "  'metadata': {'content_length': 371,\n",
       "   'subject': '',\n",
       "   'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX',\n",
       "   'creationdate': '2025-05-10T14:29:11+00:00',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 8,\n",
       "   'page_label': '8',\n",
       "   'doc_index': 84,\n",
       "   'source_file': 'Team 28-VLM Hallucination.pdf',\n",
       "   'moddate': '2025-05-10T14:29:11+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'page': 7,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf',\n",
       "   'title': 'Comparative Analysis of ML Models for Automatic Image Captioning'},\n",
       "  'similarity_score': 0.08450764417648315,\n",
       "  'distance': 0.9154923558235168,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_0c6ad923_83',\n",
       "  'content': 'Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.\\ncom/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-\\nFidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf\\n[6] Li et al., BLIP, ICML 2022. https://proceedings.mlr.press/v162/li22n.html\\n[7] CVPR 2024. https://arxiv.org/abs/2404.02904\\n[8] Biten et al., Let There Be a Clock on the Beach , WACV 2022. https://arxiv.org/abs/\\n2110.01705\\n[9] EMNLP 2024. https://arxiv.org/abs/2406.14492\\n[10] NeurIPS 2023. https://arxiv.org/abs/2306.07915\\n[11] Li et al., EVCap, CVPR 2024. https://arxiv.org/abs/2311.15879\\n[12] NeurIPS 2024. https://papers.nips.cc/paper_files/paper/2024/hash/\\nd75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html\\n[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf',\n",
       "   'page': 7,\n",
       "   'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'total_pages': 8,\n",
       "   'source_file': 'Team 28-VLM Hallucination.pdf',\n",
       "   'title': 'Comparative Analysis of ML Models for Automatic Image Captioning',\n",
       "   'subject': '',\n",
       "   'creationdate': '2025-05-10T14:29:11+00:00',\n",
       "   'trapped': '/False',\n",
       "   'page_label': '8',\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX',\n",
       "   'content_length': 942,\n",
       "   'doc_index': 83,\n",
       "   'moddate': '2025-05-10T14:29:11+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'},\n",
       "  'similarity_score': 0.033638715744018555,\n",
       "  'distance': 0.9663612842559814,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is the image captioning research paper explaining?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69446446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the conclusion on flood mapping paper?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_fc120cf0_37',\n",
       "  'content': 'monitoring of changes in the landscape related to flooding. \\nThese maps provide valuable information for emergency \\nresponse, risk assessment, and long-term flood management. \\nThe objective of this research is to get the difference map \\nbetween multiple satellite data in combination, so that an \\naccurate flood extent area would help to get the users in the \\nfields of early warning systems, flood management, \\nresearchers, etc. But, certain limitations such as limited \\nspectral information, land cover changes or lack of data \\naccessibility may lead to the discrepancies in the results.  \\nR\\nEFERENCES \\n[1] Nsangou D, Kpoumié A, Mfonka Z, Bateni SM, Ngouh AN, Ndam          \\nNgoupayou JR, “The Mfoundi Watershed at Yaoundé in the \\nHumid Tropical Zone of Camer oon: a case study of urban food \\nsusceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol.',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'content_length': 986,\n",
       "   'total_pages': 8,\n",
       "   'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'creator': 'Acrobat PDFMaker 9.0 for Word',\n",
       "   'rgid': 'PB:379933077_AS:11431281256064873@1719493068149',\n",
       "   'page_label': '8',\n",
       "   'author': 'Ahilya Bandgar',\n",
       "   'moddate': '2024-02-06T15:08:58+05:30',\n",
       "   'doc_index': 37,\n",
       "   'creationdate': '2024-02-06T15:08:55+05:30',\n",
       "   'page': 7,\n",
       "   'producer': 'Adobe PDF Library 9.0',\n",
       "   'file_type': 'pdf',\n",
       "   'sourcemodified': 'D:20240206093818'},\n",
       "  'similarity_score': 0.1314191222190857,\n",
       "  'distance': 0.8685808777809143,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_32e4d755_5',\n",
       "  'content': 'I. INTRODUCTION \\nA flooding event can be described as the non- permanent \\ninundation of water that submer ges usual arid regions in any \\ngeographical area. Flooding resulting from rapid snowmelt, \\nstorm surges impacting inland regions from the sea, persistent \\nand intense rainfall during the monsoon period, and the \\ndestruction of dams, embankments, or levees in t he season of \\nhigh winds and heavy rains [1].Deforestation has results in, the \\nincreased frequency of coastal storms can rise because of \\nabrupt changes in land use, inappropriate management of urban \\nstormwater runoffs, and climate change. In recent times, t here \\nhas been a notable increase in the frequency of global flooding \\nevents [2]. Flash floods and the more common river floods are \\nthe two main categories of flooding.River floods usually cause \\nmore property damage, but flash floods usually cause more \\nfatalities [2]. \\nSatellite data is very informative way to extract the images of',\n",
       "  'metadata': {'producer': 'Adobe PDF Library 9.0',\n",
       "   'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'creator': 'Acrobat PDFMaker 9.0 for Word',\n",
       "   'rgid': 'PB:379933077_AS:11431281256064873@1719493068149',\n",
       "   'author': 'Ahilya Bandgar',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2024-02-06T15:08:55+05:30',\n",
       "   'page_label': '2',\n",
       "   'content_length': 965,\n",
       "   'page': 1,\n",
       "   'sourcemodified': 'D:20240206093818',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'total_pages': 8,\n",
       "   'doc_index': 5,\n",
       "   'moddate': '2024-02-06T15:08:58+05:30'},\n",
       "  'similarity_score': 0.058084189891815186,\n",
       "  'distance': 0.9419158101081848,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_1031fe58_43',\n",
       "  'content': 'https://ladsweb.modaps.eosdis.nasa.gov/missions-and-\\nmeasurements/modis/#:~:text=MODIS%20data%20products%2C\\n%20in%20three,and%20in%20the%20lower%20atmosphere. \\n[Accessed: 7- Dec- 2023]. \\n[21] Ramesh Sivan Pillai, Kevin M. Jacobs Chloe M. Mattilio, Ela V. \\nPiskorski, “Rapid flood inundation mapping by differen cing water \\nindices from pre - and post -flood Landsat images ,” Springer vol. \\n15,2021. \\n[22] Malay S. Bhatt, Tejas P. Patalia, “ Content-based high -resolution \\nsatellite image classification,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2018 vol. 11, 2019. \\n[23] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Proposal and evaluation of tsunami disaster drill support \\nsystem using tablet computer,” IJIT- Bharati Vidyapeeth’s Institute \\nof Computer Applications and Management 2023 vol. 15, 2023. \\n[24] Hidemi   Fukada, Yuichi   Hashimoto, Miyuki   Oki, Yusuke   \\nOkuno, “Bhoomi Prahari - e governance tool for monitoring',\n",
       "  'metadata': {'doc_index': 43,\n",
       "   'producer': 'Adobe PDF Library 9.0',\n",
       "   'page_label': '8',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'creationdate': '2024-02-06T15:08:55+05:30',\n",
       "   'sourcemodified': 'D:20240206093818',\n",
       "   'moddate': '2024-02-06T15:08:58+05:30',\n",
       "   'content_length': 992,\n",
       "   'creator': 'Acrobat PDFMaker 9.0 for Word',\n",
       "   'total_pages': 8,\n",
       "   'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 7,\n",
       "   'rgid': 'PB:379933077_AS:11431281256064873@1719493068149',\n",
       "   'author': 'Ahilya Bandgar'},\n",
       "  'similarity_score': 0.04146915674209595,\n",
       "  'distance': 0.958530843257904,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3fc2787b_38',\n",
       "  'content': 'susceptibility mapping,” Earth Systems and Environment 2021. \\n[2] Asinya EA, Alam MJB, “Flood risk in rivers: climate driven or \\nmorphological adjustment,” Earth Systems and Environment, vol. \\n5, Issue 4, pp.861-871,2021.  \\n[3] Marinho G. Andrade, Marcelo D. Fragoso, Adriano A.F.M. \\nCarneiro, “A stochastic approach to the flood control problem,” \\nApplied Mathematical Modelling, vol. 25, Issue 6, 2001. \\n[4] Yiming Wei, Weixuan Xu, Ying Fan, Hsien -Tang Tasi, “Artificial \\nneural network based predi ctive method for flood disaster,” \\nComputers & Industrial Engineering vol. 42, Issues 2–4, 2002.  \\n[5] Shangyou Zhang, Ian Cordery, Ashish Sharma, “Application of an \\nimproved linear storage routing model for the estimation of large \\nfloods”, Journal of Hydrology, vol. 258, Issues 1–4, 2002. \\n[6] Karsten Jaspe, Joachim Gurtz, Herbert Lang, “Advanced flood \\nforecasting in Alpine watersheds by coupling meteorological \\nobservations and forecasts with a distributed hydrological model,”',\n",
       "  'metadata': {'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'producer': 'Adobe PDF Library 9.0',\n",
       "   'page': 7,\n",
       "   'rgid': 'PB:379933077_AS:11431281256064873@1719493068149',\n",
       "   'page_label': '8',\n",
       "   'total_pages': 8,\n",
       "   'author': 'Ahilya Bandgar',\n",
       "   'moddate': '2024-02-06T15:08:58+05:30',\n",
       "   'content_length': 988,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'sourcemodified': 'D:20240206093818',\n",
       "   'creator': 'Acrobat PDFMaker 9.0 for Word',\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 38,\n",
       "   'creationdate': '2024-02-06T15:08:55+05:30'},\n",
       "  'similarity_score': 0.007641911506652832,\n",
       "  'distance': 0.9923580884933472,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_2c0bc546_25',\n",
       "  'content': 'geometry. To properly manage big datasets, a scale of \\n30 m eters, which corresponds to the resolution of \\nLandsat pictures, is established. The entire area that \\nhas been inundated is then calculated in square \\nmeters. The result, which represents the entire \\nflooded area in square kilometres, is displayed to the \\nconsole for reference after the value is divided by 1e6 \\nto show the result in square kilometres. \\n• Calculating Flood Depth-  \\nThe flood extent data, which can be a feature \\ncollection or an image, and optionally, water extent \\nFig. 1.  Methodology for Calculating Hazard Map.',\n",
       "  'metadata': {'doc_index': 25,\n",
       "   'creator': 'Acrobat PDFMaker 9.0 for Word',\n",
       "   'sourcemodified': 'D:20240206093818',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'moddate': '2024-02-06T15:08:58+05:30',\n",
       "   'author': 'Ahilya Bandgar',\n",
       "   'rgid': 'PB:379933077_AS:11431281256064873@1719493068149',\n",
       "   'page_label': '5',\n",
       "   'total_pages': 8,\n",
       "   'page': 4,\n",
       "   'producer': 'Adobe PDF Library 9.0',\n",
       "   'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf',\n",
       "   'creationdate': '2024-02-06T15:08:55+05:30',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 594},\n",
       "  'similarity_score': 0.001477956771850586,\n",
       "  'distance': 0.9985220432281494,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is the conclusion on flood mapping paper?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933112d0",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a62dfab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6019767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"llama-3.1-8b-instant\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6acb94bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: llama-3.1-8b-instant\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "956c3be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the image captioning research paper explaining?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_254c49a3_84',\n",
       "  'content': '[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.\\n[15] Hyperbolic Learning with Synthetic Captions , CVPR 2024.\\nProject Repository\\nThe complete code and resources for this project can be found on\\nGitHub:\\nhttps://github.com/Avipsa-Bhujabal/Image𝐶𝑎𝑝𝑡𝑖𝑜𝑛𝑖𝑛𝑔 𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛',\n",
       "  'metadata': {'moddate': '2025-05-10T14:29:11+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'subject': '',\n",
       "   'page_label': '8',\n",
       "   'creationdate': '2025-05-10T14:29:11+00:00',\n",
       "   'source_file': 'Team 28-VLM Hallucination.pdf',\n",
       "   'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'doc_index': 84,\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf',\n",
       "   'page': 7,\n",
       "   'content_length': 371,\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'Comparative Analysis of ML Models for Automatic Image Captioning',\n",
       "   'total_pages': 8,\n",
       "   'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX'},\n",
       "  'similarity_score': 0.08450764417648315,\n",
       "  'distance': 0.9154923558235168,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_0c6ad923_83',\n",
       "  'content': 'Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf\\n[5] Ge et al., Visual Fact Checker , CVPR 2024. https://openaccess.thecvf.\\ncom/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-\\nFidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf\\n[6] Li et al., BLIP, ICML 2022. https://proceedings.mlr.press/v162/li22n.html\\n[7] CVPR 2024. https://arxiv.org/abs/2404.02904\\n[8] Biten et al., Let There Be a Clock on the Beach , WACV 2022. https://arxiv.org/abs/\\n2110.01705\\n[9] EMNLP 2024. https://arxiv.org/abs/2406.14492\\n[10] NeurIPS 2023. https://arxiv.org/abs/2306.07915\\n[11] Li et al., EVCap, CVPR 2024. https://arxiv.org/abs/2311.15879\\n[12] NeurIPS 2024. https://papers.nips.cc/paper_files/paper/2024/hash/\\nd75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html\\n[13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\\n[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.',\n",
       "  'metadata': {'doc_index': 83,\n",
       "   'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX',\n",
       "   'total_pages': 8,\n",
       "   'title': 'Comparative Analysis of ML Models for Automatic Image Captioning',\n",
       "   'page': 7,\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2025-05-10T14:29:11+00:00',\n",
       "   'page_label': '8',\n",
       "   'creationdate': '2025-05-10T14:29:11+00:00',\n",
       "   'subject': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf',\n",
       "   'source_file': 'Team 28-VLM Hallucination.pdf',\n",
       "   'content_length': 942,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'},\n",
       "  'similarity_score': 0.033638715744018555,\n",
       "  'distance': 0.9663612842559814,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"What is the image captioning research paper explaining?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524ee11",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c66c934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82923fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the image captioning research paper explaining?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research papers explain various aspects of image captioning, including:\n",
      "\n",
      "- Evaluating image captioning in the age of multimodal LLMs (Large Language Models) ([13] Image Captioning Evaluation in the Age of Multimodal LLMs, arXiv 2025).\n",
      "- Generating improved image captions using pseudo content hallucination for unpaired image captioning ([14] Pseudo Content Hallucination for Unpaired Image Captioning, ACM MM 2024).\n",
      "- Using hyperbolic learning with synthetic captions for image captioning ([15] Hyperbolic Learning with Synthetic Captions, CVPR 2024).\n",
      "\n",
      "These papers aim to improve the quality and accuracy of image captioning models.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is the image captioning research paper explaining?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d3d59",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37319ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files:\n",
      "  - ..\\data\\pdf\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf\n",
      "  - ..\\data\\pdf\\Team 28-VLM Hallucination.pdf\n"
     ]
    }
   ],
   "source": [
    "# Check if PDFs are actually being found\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_dir = Path(\"../data\")\n",
    "pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for f in pdf_files:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74db0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in vector store: 85\n",
      "✓ Vector store has 85 documents\n"
     ]
    }
   ],
   "source": [
    "# Check if documents are in ChromaDB\n",
    "print(f\"Documents in vector store: {vectorstore.collection.count()}\")\n",
    "if vectorstore.collection.count() == 0:\n",
    "    print(\"❌ Vector store is EMPTY! Documents not being added.\")\n",
    "else:\n",
    "    print(f\"✓ Vector store has {vectorstore.collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5cee4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What was the objective of image captioning research paper?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n",
      "Retrieved 1 documents:\n",
      "\n",
      "[1] Similarity Score: 0.0499\n",
      "Content: [13] Image Captioning Evaluation in the Age of Multimodal LLMs , arXiv 2025.\n",
      "[14] Pseudo Content Hallucination for Unpaired Image Captioning , ACM MM 2024.\n",
      "[15] Hyperbolic Learning with Synthetic Capt...\n",
      "Metadata: {'page_label': '8', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'doc_index': 84, 'moddate': '2025-05-10T14:29:11+00:00', 'content_length': 371, 'file_type': 'pdf', 'title': 'Comparative Analysis of ML Models for Automatic Image Captioning', 'subject': '', 'total_pages': 8, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX', 'source_file': 'Team 28-VLM Hallucination.pdf', 'page': 7, 'trapped': '/False', 'creationdate': '2025-05-10T14:29:11+00:00', 'source': '..\\\\data\\\\pdf\\\\Team 28-VLM Hallucination.pdf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval with lower threshold to see what's being found\n",
    "query = \"What was the objective of image captioning research paper?\"\n",
    "results = rag_retriever.retrieve(query, top_k=5, score_threshold=0.0)  # Set to 0 to see all results\n",
    "\n",
    "print(f\"Retrieved {len(results)} documents:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Similarity Score: {doc['similarity_score']:.4f}\")\n",
    "    print(f\"Content: {doc['content'][:200]}...\")\n",
    "    print(f\"Metadata: {doc['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acbbe769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 85\n",
      "\n",
      "First chunk content preview:\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/379933077\n",
      "Improving Earth Observations by correlating Multiple Satellite Data: A\n",
      "Comparative Analysis of Landsat, MODIS and Sentinel Satellite Data for Flood\n",
      "Mapping\n",
      "Conference Paper · Febru\n",
      "\n",
      "First chunk metadata:\n",
      "{'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 for Word', 'creationdate': '2024-02-06T15:08:55+05:30', 'author': 'Ahilya Bandgar', 'moddate': '2024-02-06T15:08:58+05:30', 'sourcemodified': 'D:20240206093818', 'rgid': 'PB:379933077_AS:11431281256064873@1719493068149', 'source': '..\\\\data\\\\pdf\\\\Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'Improving-Earth-Observations-by-correlating-Multiple-Satellite-Data-A-Comparative-Analysis-of-Landsat-MODIS-and-Sentinel-Satellite-Data-for-Flood-Mapping.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Check first few chunks that were created\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "if len(chunks) > 0:\n",
    "    print(f\"\\nFirst chunk content preview:\")\n",
    "    print(chunks[0].page_content[:300])\n",
    "    print(f\"\\nFirst chunk metadata:\")\n",
    "    print(chunks[0].metadata)\n",
    "else:\n",
    "    print(\"❌ No chunks created! Check if PDFs are loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83943e44",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73b86453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Summary of VLM Hallucination paper'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Summary of VLM Hallucination paper\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bc5707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What was the objective of image captioning research paper?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: No relevant context found.\n",
      "Summary: There is no information to summarize as the provided answer is a statement indicating that no relevant context was found.\n",
      "History: {'question': 'What was the objective of image captioning research paper?', 'answer': 'No relevant context found.', 'sources': [], 'summary': 'There is no information to summarize as the provided answer is a statement indicating that no relevant context was found.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"What was the objective of image captioning research paper?\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
