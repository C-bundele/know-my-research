{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02a68aa",
   "metadata": {},
   "source": [
    "### Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f75a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Structure\n",
    "\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2c2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Chanchal Bundele', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"exmaple.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Chanchal Bundele\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c3fab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55215a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa09219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144eb66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chanc\\anaconda3\\envs\\ragproj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\n",
    "    \"../data/text_files/python_intro.txt\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22dc80a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614d529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-02-07T22:05:58-05:00', 'source': '..\\\\data\\\\pdf\\\\Clustering_Research_Report.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Clustering_Research_Report.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'python-docx', 'subject': '', 'keywords': '', 'moddate': '2026-02-07T22:05:58-05:00', 'trapped': '', 'modDate': \"D:20260207220558-05'00'\", 'creationDate': \"D:20260207220558-05'00'\", 'page': 0}, page_content='Research Report: Comparative Analysis of Clustering Techniques on High-\\nDimensional Data \\nAbstract \\nThis work presents a comprehensive comparative analysis of unsupervised learning \\nmethods across four high-dimensional datasets: MNIST, Fashion-MNIST, UCI-HAR, and \\nCIFAR-10. The study investigates how clustering performance varies when combined with \\ndifferent dimensionality reduction techniques including PCA, Kernel PCA, t‑SNE, and UMAP. \\nFive clustering algorithms—K-Means, DBSCAN, BD‑DBSCAN, Fuzzy C‑Means, and Deep \\nFuzzy Clustering—are rigorously evaluated using Silhouette Score, Davies–Bouldin Index, \\nCluster Entropy, Adjusted Rand Index (ARI), and Normalized Mutual Information (NMI). \\n \\nResults show that dimensionality reduction is essential for effective clustering in high-\\ndimensional spaces, with UMAP consistently producing the best representations for \\ndownstream clustering. Among clustering techniques, Fuzzy C‑Means provides the most \\nstable and accurate results, especially when combined with UMAP. This pattern holds \\nacross all datasets, including the challenging CIFAR‑10 dataset. The findings highlight UMAP \\n+ Fuzzy C‑Means as a strong general‑purpose pipeline for clustering high-dimensional, \\nnonlinear, and noisy data. \\n \\nConclusion \\nThis study conducted a detailed empirical comparison of clustering algorithms paired with \\nmultiple dimensionality reduction techniques on four diverse high‑dimensional datasets. \\nThe experiments demonstrate that clustering performance improves substantially after \\ndimensionality reduction, with UMAP outperforming PCA, Kernel PCA, and t‑SNE in terms of \\ncluster separability, structure preservation, and computational efficiency. \\n \\nAcross clustering algorithms, Fuzzy C‑Means consistently achieved the best performance \\nwhen combined with UMAP, yielding high ARI/NMI scores and low entropy across MNIST, \\nFashion‑MNIST, UCI‑HAR, and CIFAR‑10. This combination proved especially effective for \\ndatasets with overlapping classes or nonlinear manifolds. DBSCAN and BD‑DBSCAN showed \\nimproved performance after UMAP transformation but lacked consistency across datasets. \\nDeep Fuzzy Clustering showed moderate improvements but was sensitive to model \\nconfiguration. \\n \\nOverall, UMAP + Fuzzy C‑Means emerges as the strongest and most universally applicable \\nclustering pipeline for high-dimensional data. The study also extends the existing literature \\nby including BD‑DBSCAN in the comparative framework and introducing CIFAR‑10 as an \\nadditional benchmark dataset. Future work may explore deep manifold learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-02-07T22:05:58-05:00', 'source': '..\\\\data\\\\pdf\\\\Clustering_Research_Report.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Clustering_Research_Report.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'python-docx', 'subject': '', 'keywords': '', 'moddate': '2026-02-07T22:05:58-05:00', 'trapped': '', 'modDate': \"D:20260207220558-05'00'\", 'creationDate': \"D:20260207220558-05'00'\", 'page': 1}, page_content='representations, hybrid graph-clustering models, and real‑world industrial datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-01T00:44:34+00:00', 'source': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'total_pages': 6, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-01T00:44:34+00:00', 'trapped': '', 'modDate': 'D:20250401004434Z', 'creationDate': 'D:20250401004434Z', 'page': 0}, page_content='Unsupervised Learning: Comparative Analysis of\\nClustering Techniques on High-Dimensional Data\\n∗Vishnu Vardhan Baligodugula, ∗Fathi Amsaad\\n† Department of Computer Science and Engineering, Wright State University\\nEmail: †{baligodugula.2,fathi.amsaad}@wright.edu\\nAbstract—This paper presents a comprehensive comparative\\nanalysis of prominent clustering algorithms—K-means, DB-\\nSCAN, and Spectral Clustering—on high-dimensional datasets.\\nWe introduce a novel evaluation framework that assesses clus-\\ntering performance across multiple dimensionality reduction\\ntechniques (PCA, t-SNE, and UMAP) using diverse quantitative\\nmetrics. Experiments conducted on MNIST, Fashion-MNIST,\\nand UCI HAR datasets reveal that preprocessing with UMAP\\nconsistently improves clustering quality across all algorithms,\\nwith Spectral Clustering demonstrating superior performance on\\ncomplex manifold structures. Our findings show that algorithm\\nselection should be guided by data characteristics, with K-\\nmeans excelling in computational efficiency, DBSCAN in handling\\nirregular clusters, and Spectral Clustering in capturing complex\\nrelationships. This research contributes a systematic approach\\nfor evaluating and selecting clustering techniques for high-\\ndimensional data applications.\\nI. INTRODUCTION\\nExtracting meaningful patterns from unlabeled datasets con-\\ntinues to rely heavily on unsupervised learning methods, with\\nclustering techniques at their core. Modern datasets across\\nvarious fields have grown increasingly complex, creating\\nsignificant challenges when attempting to cluster data with\\nmany dimensions. While supervised approaches benefit from\\nclear performance metrics through comparison with known\\nlabels, unsupervised methods face a more nuanced evalua-\\ntion landscape that requires balancing multiple considerations\\nto determine effectiveness.RetryClaude can make mistakes.\\nPlease double-check responses.\\nDespite the proliferation of clustering algorithms, there is\\nlimited consensus on which techniques perform optimally for\\ndifferent types of high-dimensional data. Most comparative\\nstudies focus on a narrow range of algorithms or metrics,\\noften neglecting the critical interaction between dimensionality\\nreduction techniques and clustering algorithms. This research\\ngap hinders practitioners from making informed decisions\\nwhen selecting appropriate methods for their specific appli-\\ncations. This paper addresses these challenges by introducing\\na systematic evaluation framework for comparing clustering\\nalgorithms across multiple high-dimensional datasets. Our\\napproach integrates dimensionality reduction techniques with\\nclustering algorithms and evaluates performance using a com-\\nprehensive set of metrics.\\nThe key contributions of this work include a systematic\\ncomparison of K-means, DBSCAN, and Spectral Clustering on\\nthree distinct high-dimensional datasets, analysis of the impact\\nof dimensionality reduction techniques (PCA, t-SNE, and\\nUMAP) on clustering performance, evaluation using multiple\\ncomplementary metrics that provide a holistic assessment of\\nclustering quality, and practical insights for algorithm selection\\nbased on data characteristics and performance requirements.\\nThe findings of this study provide valuable guidance for\\nresearchers and practitioners working with high-dimensional\\ndata across various domains, including image recognition,\\nactivity classification, and general pattern discovery.\\nII. RELATED WORK\\nA. Clustering Algorithms\\nClustering techniques have evolved significantly since the\\nintroduction of K-means by MacQueen in 1967 [1]. Recent\\nadvances include modifications to improve computational ef-\\nficiency [2] and adaptations for specific data types [3]. DB-\\nSCAN, introduced by Ester et al. [4], revolutionized density-\\nbased clustering and has seen numerous extensions, including\\nOPTICS [5] and HDBSCAN [6], which address parameter\\nsensitivity and variable-density clusters.\\nSpectral clustering, formalized by Ng et al. [7], leverages\\ngraph theory to capture complex data manifolds. Recent work\\nby Von Luxburg [8] provides a comprehensive theoretical\\nframework, while Zelnik-Manor and Perona [9] address au-\\ntomatic parameter selection for spectral clustering.\\nB. Comparative Studies\\nSeveral studies have compared clustering algorithms, but\\nmost focus on low-dimensional data or limited algorithm sets.\\nRodriguez and Laio [10] compared density-based methods,\\nwhile Wang et al. [11] evaluated partitional algorithms on\\nspecific domains. Comprehensive comparisons by Xu and\\nWunsch [12] provided valuable taxonomies but predated many\\nmodern techniques.\\nA notable gap exists in understanding how algorithm per-\\nformance varies across different types of high-dimensional\\ndata and how this interacts with dimensionality reduction\\ntechniques a gap our work addresses directly.\\nC. Dimensionality Reduction\\nDimensionality reduction has become integral to processing\\nhigh-dimensional data. While PCA [13] remains widely used,\\nnonlinear techniques like t-SNE [14] have gained popularity\\nfor visualization. UMAP, introduced by McInnes et al. [15],\\noffers advantages in preserving both local and global structure\\nwhile maintaining computational efficiency.\\narXiv:2503.23215v1  [cs.LG]  29 Mar 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-01T00:44:34+00:00', 'source': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'total_pages': 6, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-01T00:44:34+00:00', 'trapped': '', 'modDate': 'D:20250401004434Z', 'creationDate': 'D:20250401004434Z', 'page': 1}, page_content='The interaction between dimensionality reduction and clus-\\ntering performance has been explored by Sander et al. [16],\\nwho examined how preprocessing affects density-based clus-\\ntering. However, comprehensive analyses across multiple al-\\ngorithms, reduction techniques, and datasets remain limited.\\nD. Evaluation Metrics\\nEvaluation of clustering results presents unique challenges.\\nInternal metrics such as the Silhouette Coefficient [17] and\\nDavies-Bouldin Index [18] assess cluster structure without\\nground truth, while external metrics like the Adjusted Rand\\nIndex [19] and Normalized Mutual Information [20] leverage\\nknown labels when available. Our work builds on these foun-\\ndations by integrating multiple evaluation paradigms [24] to\\nprovide a more complete assessment of clustering performance\\n[25].\\nIII. METHODOLOGY\\nA. Datasets\\nWe selected three widely-used high-dimensional datasets\\nthat represent different domains and data characteristics:\\n• MNIST [21] : A dataset of 70,000 handwritten digits (0-\\n9), each represented as a 28×28 grayscale image (784 di-\\nmensions). MNIST contains well-separated clusters with\\nrelatively simple structure.\\n• Fashion-MNIST [22] : A dataset of 70,000 fashion prod-\\nuct images across 10 categories, also in 28×28 grayscale\\nformat (784 dimensions). Compared to MNIST, Fashion-\\nMNIST presents more complex intra-class variations and\\nless distinct boundaries between clusters.\\n• UCI Human Activity Recognition (HAR) [23] : A\\ndataset containing 10,299 instances of smartphone sensor\\nreadings (561 dimensions) for six physical activities. This\\ndataset features time-series data with different statistical\\nproperties from image data.\\nThese datasets were chosen to represent different levels\\nof clustering difficulty, data types, and application domains,\\nenabling a more comprehensive evaluation of algorithm per-\\nformance.\\nB. Preprocessing\\nAll datasets underwent the following preprocessing steps:\\n• Normalization: Features were standardized to zero mean\\nand unit variance using the standard score method:\\nz = x −µ\\nσ\\nwhere µ is the mean and σ is the standard deviation of\\nthe feature.\\n• Dimensionality Reduction: We applied three techniques\\nto reduce data to 50 dimensions for algorithm processing\\nand 2 dimensions for visualization:\\n– PCA: A linear technique that preserves global vari-\\nance.\\n– t-SNE: A non-linear technique that preserves local\\nneighborhood structure.\\n– UMAP: A non-linear technique that balances local\\nand global structure preservation.\\nAll implementations used scikit-learn and UMAP\\nlibraries with default parameters except where noted.\\nC. Clustering Algorithms\\nWe implemented three distinct clustering algorithms repre-\\nsenting different approaches:\\n• K-means:\\n– Parameter: number of clusters k = 10 for MNIST\\nand Fashion-MNIST, k = 6 for HAR.\\n– Implementation: scikit-learn with k-means++\\ninitialization and 10 random restarts.\\n– Complexity: O(nkdi), where n is the number of\\nsamples, k is the number of clusters, d is the number\\nof dimensions, and i is the number of iterations.\\n• DBSCAN:\\n– Parameters: ε (neighborhood distance) determined\\nvia nearest-neighbor distance plot, minPts = 10.\\n– Implementation: scikit-learn with ball-tree\\nalgorithm for neighborhood queries.\\n– Complexity: O(n2) in the worst case, O(n log n)\\nwith spatial indexing.\\n• Spectral Clustering:\\n– Parameters: number of clusters same as K-means,\\nnearest-neighbors kernel with n_neighbors =\\n10.\\n– Implementation: scikit-learn with Normal-\\nized Cuts formulation.\\n– Complexity: O(n3) in naive implementation, O(n2)\\nwith approximation techniques.\\nD. Evaluation Framework\\nWe employed both internal and external evaluation metrics:\\n1) Internal Metrics (no ground truth required): :\\n• Silhouette Coefficient: Measures how similar points are\\nto their own cluster compared to other clusters (−1 to 1,\\nhigher is better).\\n• Davies-Bouldin Index: Ratio of within-cluster distances\\nto between-cluster distances (lower is better).\\n• Calinski-Harabasz Index: Ratio of between-cluster dis-\\npersion to within-cluster dispersion (higher is better).\\n2) External Metrics (using ground truth labels): :\\n• Adjusted Rand Index (ARI): Measures agreement be-\\ntween true and predicted labels, adjusted for chance (0 to\\n1, higher is better).\\n• Normalized Mutual Information (NMI): Information\\ntheoretic measure of clustering quality (0 to 1, higher is\\nbetter).\\n3) Computational Metrics: :\\n• Training Time: CPU time required for algorithm execu-\\ntion.\\n• Memory Usage: Peak memory consumption during exe-\\ncution.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-01T00:44:34+00:00', 'source': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'total_pages': 6, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-01T00:44:34+00:00', 'trapped': '', 'modDate': 'D:20250401004434Z', 'creationDate': 'D:20250401004434Z', 'page': 2}, page_content='For each algorithm-dataset-reduction technique combina-\\ntion, we conducted 10 runs with different random initializa-\\ntions (where applicable) and reported the mean and standard\\ndeviation of all metrics.\\nIV. EXPERIMENTS AND RESULTS\\nA. Impact of Dimensionality Reduction\\nTable I presents the performance of each clustering algo-\\nrithm on raw data (784 dimensions for MNIST and Fashion-\\nMNIST, 561 for HAR) versus data reduced to 50 dimensions\\nusing different techniques.\\nSeveral key findings emerge from these results:\\n1) All algorithms benefit significantly from dimensionality\\nreduction, with UMAP consistently providing the best\\nperformance across all algorithms and datasets.\\n2) The improvement from raw data to UMAP preprocess-\\ning is most dramatic for DBSCAN, which shows 2-3×\\nimprovement in ARI scores.\\n3) Spectral Clustering achieves the highest absolute per-\\nformance when combined with UMAP, particularly on\\nMNIST (ARI = 0.794).\\n4) The relative benefit of nonlinear techniques (t-SNE,\\nUMAP) over linear PCA is greatest for Fashion-MNIST,\\nsuggesting its clusters have more complex manifold\\nstructure.\\nFig. 1 visualizes the clusters identified by each algorithm\\non the MNIST dataset after UMAP reduction to 2 dimensions.\\nFig. 1. 2D visualization of clustering results on MNIST after UMAP dimen-\\nsionality reduction. Colors represent cluster assignments. Spectral Clustering\\nachieves the highest ARI score.\\nB. Comparative Analysis of Clustering Algorithms\\nTable II presents a comparison of the three algorithms across\\nvarious metrics after UMAP dimensionality reduction to 50\\ndimensions.\\nThese results highlight several important patterns:\\n1) Algorithm Performance: Spectral Clustering consis-\\ntently achieves the highest ARI and NMI scores across\\nall datasets, suggesting superior cluster identification\\nwhen evaluated against ground truth.\\n2) Internal vs. External Metrics: DBSCAN achieves\\nthe best internal metric scores (Silhouette and Davies-\\nBouldin) despite not having the highest agreement with\\nground truth labels. This suggests DBSCAN finds more\\ncompact and well-separated clusters that don’t necessar-\\nily align with class labels.\\n3) Computational Efficiency: K-means demonstrates clear\\nsuperiority in computational efficiency, executing 15-\\n50× faster than Spectral Clustering, making it a practical\\nchoice for large datasets or time-sensitive applications.\\n4) Dataset Difficulty: All algorithms achieve lower perfor-\\nmance on Fashion-MNIST compared to MNIST, con-\\nfirming its greater clustering challenge due to more\\ncomplex class structure.\\nFig. 2 visualizes the performance of all three algorithms\\nacross different metrics for all datasets after UMAP reduction.\\nFig. 2.\\nPerformance comparison across algorithms and datasets. Top: ARI\\nscores showing clustering accuracy. Bottom: Execution time (log scale)\\nshowing computational efficiency.\\nC. Clustering Stability Analysis\\nTo assess clustering stability, we performed 100 runs of each\\nalgorithm on the MNIST dataset with different random initial-\\nizations (for K-means and Spectral Clustering) and calculated\\nthe standard deviation of ARI scores. Results are presented in\\nTable III.\\nThe stability analysis reveals:\\n1) DBSCAN shows perfect stability (StdDev = 0) across\\nall runs, as it is deterministic given fixed parameters.\\n2) K-means shows highest variability when used with t-\\nSNE, suggesting sensitivity to the local optima created\\nby t-SNE’s nonlinear mapping.\\n3) Spectral Clustering with UMAP provides the best com-\\nbination of high performance (ARI = 0.794) and good\\nstability (StdDev = 0.024).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-01T00:44:34+00:00', 'source': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'total_pages': 6, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-01T00:44:34+00:00', 'trapped': '', 'modDate': 'D:20250401004434Z', 'creationDate': 'D:20250401004434Z', 'page': 3}, page_content='TABLE I\\nPERFORMANCE COMPARISON WITH DIFFERENT DIMENSIONALITY REDUCTION TECHNIQUES (ARI SCORES)\\nAlgorithm\\nDataset\\nRaw Data\\nPCA\\nt-SNE\\nUMAP\\nK-means\\nMNIST\\n0.367±0.001\\n0.494±0.002\\n0.721±0.015\\n0.758±0.012\\nK-means\\nFashion-MNIST\\n0.341±0.002\\n0.411±0.003\\n0.587±0.021\\n0.625±0.019\\nK-means\\nUCI HAR\\n0.399±0.006\\n0.586±0.004\\n0.684±0.009\\n0.712±0.008\\nDBSCAN\\nMNIST\\n0.245±0.000\\n0.319±0.000\\n0.675±0.000\\n0.671±0.000\\nDBSCAN\\nFashion-MNIST\\n0.176±0.000\\n0.251±0.000\\n0.493±0.000\\n0.547±0.000\\nDBSCAN\\nUCI HAR\\n0.289±0.000\\n0.427±0.000\\n0.592±0.000\\n0.645±0.000\\nSpectral\\nMNIST\\n0.448±0.012\\n0.563±0.008\\n0.763±0.011\\n0.794±0.009\\nSpectral\\nFashion-MNIST\\n0.392±0.015\\n0.476±0.012\\n0.623±0.014\\n0.684±0.011\\nSpectral\\nUCI HAR\\n0.471±0.009\\n0.613±0.007\\n0.721±0.008\\n0.755±0.006\\nTABLE II\\nALGORITHM PERFORMANCE COMPARISON AFTER UMAP REDUCTION (MEAN±STD)\\nAlgorithm\\nDataset\\nARI\\nNMI\\nSilhouette\\nDavies-Bouldin\\nTime (s)\\nK-means\\nMNIST\\n0.758±0.012\\n0.814±0.007\\n0.427±0.005\\n1.342±0.037\\n2.7±0.1\\nDBSCAN\\nMNIST\\n0.671±0.000\\n0.768±0.000\\n0.513±0.000\\n0.987±0.000\\n45.3±1.2\\nSpectral\\nMNIST\\n0.794±0.009\\n0.837±0.006\\n0.485±0.008\\n1.129±0.042\\n127.8±3.5\\nK-means\\nFashion-MNIST\\n0.625±0.019\\n0.681±0.011\\n0.312±0.007\\n1.587±0.045\\n3.1±0.2\\nDBSCAN\\nFashion-MNIST\\n0.547±0.000\\n0.632±0.000\\n0.435±0.000\\n1.253±0.000\\n47.9±1.6\\nSpectral\\nFashion-MNIST\\n0.684±0.011\\n0.715±0.009\\n0.396±0.010\\n1.321±0.037\\n134.3±4.2\\nK-means\\nUCI HAR\\n0.712±0.008\\n0.747±0.006\\n0.352±0.004\\n1.431±0.028\\n0.9±0.1\\nDBSCAN\\nUCI HAR\\n0.645±0.000\\n0.708±0.000\\n0.467±0.000\\n1.165±0.000\\n11.2±0.5\\nSpectral\\nUCI HAR\\n0.755±0.006\\n0.783±0.005\\n0.422±0.007\\n1.247±0.031\\n38.7±1.1\\nTABLE III\\nCLUSTERING STABILITY ANALYSIS ON MNIST DATASET\\nAlgorithm\\nDimensionality Reduction\\nARI Mean\\nARI StdDev\\nStability Score\\nK-means\\nPCA\\n0.494\\n0.017\\n0.966\\nK-means\\nt-SNE\\n0.721\\n0.045\\n0.938\\nK-means\\nUMAP\\n0.758\\n0.028\\n0.963\\nDBSCAN\\nPCA\\n0.319\\n0.000\\n1.000\\nDBSCAN\\nt-SNE\\n0.675\\n0.000\\n1.000\\nDBSCAN\\nUMAP\\n0.671\\n0.000\\n1.000\\nSpectral\\nPCA\\n0.563\\n0.021\\n0.963\\nSpectral\\nt-SNE\\n0.763\\n0.037\\n0.952\\nSpectral\\nUMAP\\n0.794\\n0.024\\n0.970\\nFig. 3 displays cluster assignment stability visualization,\\nshowing how consistently points are assigned to the same\\ncluster across multiple runs.\\nFig. 3.\\nStability analysis showing ARI score distribution across 100 runs\\nwith different random initializations. DBSCAN shows perfect stability (no\\nvariation) while K-means and Spectral Clustering show small variations.\\nD. Performance on Different Data Characteristics\\nTo understand how algorithm performance varies with data\\ncharacteristics, we created synthetic datasets with controlled\\nproperties:\\n1) Varying cluster separability\\n2) Different cluster shapes\\n3) Presence of noise\\nTable IV summarizes how each algorithm performs under\\nthese conditions.\\nTABLE IV\\nALGORITHM PERFORMANCE ON DIFFERENT DATA CHARACTERISTICS\\n(ARI SCORES)\\nData Characteristic\\nK-means\\nDBSCAN\\nSpectral\\nWell-separated spherical\\n0.982\\n0.957\\n0.978\\nOverlapping spherical\\n0.781\\n0.648\\n0.762\\nNon-spherical (moons)\\n0.512\\n0.943\\n0.967\\nDifferent densities\\n0.623\\n0.891\\n0.854\\nHigh noise (15%)\\n0.587\\n0.832\\n0.761\\nUnbalanced clusters\\n0.814\\n0.693\\n0.775\\nThese results confirm theoretical expectations:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-01T00:44:34+00:00', 'source': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'total_pages': 6, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-01T00:44:34+00:00', 'trapped': '', 'modDate': 'D:20250401004434Z', 'creationDate': 'D:20250401004434Z', 'page': 4}, page_content='1) K-means excels with spherical, well-separated, and bal-\\nanced clusters\\n2) DBSCAN performs best with irregular shapes and in the\\npresence of noise\\n3) Spectral Clustering performs well across most condi-\\ntions, particularly with complex manifold structures\\nFig. 4 visualizes these synthetic datasets and the clusters\\nidentified by each algorithm.\\nV. DISCUSSION\\nA. Interpretation of Results\\nOur comprehensive analysis reveals several key insights:\\n• Dimensionality Reduction Impact: Preprocessing with\\ndimensionality reduction significantly improves cluster-\\ning performance across all algorithms and datasets.\\nUMAP consistently outperforms other techniques, likely\\ndue to its ability to preserve both local and global\\nstructure. This finding emphasizes the importance of\\npreprocessing in clustering pipelines.\\n• Algorithm Selection Considerations: While Spectral\\nClustering achieves the highest accuracy across datasets,\\nthe choice of algorithm should consider multiple factors:\\n– K-means offers excellent computational efficiency\\nwith competitive performance for well-structured\\ndata.\\n– DBSCAN excels in identifying compact clusters and\\nhandling noise without requiring a predefined cluster\\ncount.\\n– Spectral Clustering provides superior performance\\nfor complex manifold structures but at higher com-\\nputational cost.\\n• Metric Divergence: The discrepancy between internal\\nmetrics (Silhouette, Davies-Bouldin) and external metrics\\n(ARI, NMI) highlights the challenge of evaluating clus-\\nters without ground truth. DBSCAN’s superior internal\\nmetrics despite lower ARI suggests it identifies inherent\\nstructure that doesn’t necessarily align with predefined\\nclasses.\\n• Dataset Characteristics: Performance varies signifi-\\ncantly across datasets, with all algorithms achieving better\\nresults on MNIST than Fashion-MNIST. This confirms\\nthat the inherent separability of clusters in the data\\nsubstantially impacts algorithm performance.\\nB. Practical Implications\\nThese findings translate to practical recommendations for\\npractitioners:\\n• Preprocessing\\nPipeline: Always incorporate dimen-\\nsionality reduction in the clustering pipeline for high-\\ndimensional data, with UMAP as the preferred technique\\nwhen computational resources permit.\\n• Algorithm Selection Guidelines:\\n– When computational efficiency is critical: K-means\\nwith UMAP preprocessing.\\n– When cluster count is unknown or noise handling is\\nimportant: DBSCAN.\\n– When maximizing accuracy on complex data is the\\nprimary goal: Spectral Clustering.\\n• Evaluation Strategy: Use multiple complementary met-\\nrics when evaluating clustering performance, particularly\\nwhen ground truth is unavailable.\\n• Parameter Selection: For K-means and Spectral Clus-\\ntering, proper initialization (k-means++) and multiple\\nrestarts improve results; for DBSCAN, nearest-neighbor\\ndistance plots help identify appropriate ϵ values.\\nC. Limitations\\nOur study has several limitations that suggest directions for\\nfuture work:\\n• Parameter Sensitivity: While we optimized key param-\\neters, exhaustive parameter tuning was not performed,\\nand performance could potentially improve with more\\nextensive optimization.\\n• Scalability Challenges: Our analysis focused on datasets\\nwith fewer than 100,000 samples; scaling to larger\\ndatasets would require additional considerations, particu-\\nlarly for Spectral Clustering.\\n• Algorithm Coverage: We focused on three representative\\nalgorithms, but many variants and alternatives exist that\\nmight perform differently on these datasets.\\n• Dimensionality Reduction Setup: We used default pa-\\nrameters for dimensionality reduction techniques; cus-\\ntomizing these parameters for each dataset might improve\\nresults.\\nVI. CONCLUSION AND FUTURE WORK\\nThis paper presented a comprehensive comparative analysis\\nof clustering techniques for high-dimensional data. Our find-\\nings demonstrate that algorithm performance is significantly\\ninfluenced by both the intrinsic properties of the dataset\\nand the preprocessing techniques applied. UMAP consistently\\nimproves clustering performance across algorithms, while\\nSpectral Clustering generally achieves the highest accuracy at\\nincreased computational cost.\\nThe evaluation framework introduced in this work provides\\na systematic approach for comparing clustering algorithms\\nacross multiple dimensions of performance. Our results offer\\npractical guidance for practitioners in selecting appropriate\\ntechniques based on their specific requirements and data\\ncharacteristics.\\nFuture work should expand this analysis to include more\\ndiverse datasets from additional domains, extended algorithm\\ncoverage, including hierarchical methods and recent deep\\nclustering approaches, exploration of ensemble clustering tech-\\nniques that combine the strengths of multiple algorithms,\\nautomated parameter selection methods to reduce the expertise\\nrequired for effective clustering, and scaling studies to address\\nvery large datasets.\\nBy building on this systematic evaluation approach, we can\\ncontinue to improve our understanding of clustering algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-01T00:44:34+00:00', 'source': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CompAnalysis_CLUSTERING.pdf', 'total_pages': 6, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-01T00:44:34+00:00', 'trapped': '', 'modDate': 'D:20250401004434Z', 'creationDate': 'D:20250401004434Z', 'page': 5}, page_content='Fig. 4. Performance on different data characteristics. Top: Well-separated spherical clusters. Middle: Non-spherical moon-shaped clusters. Bottom: Data with\\nhigh noise. Each algorithm shows distinct strengths depending on data characteristics.\\nperformance and develop more effective techniques for unsu-\\npervised pattern discovery in high-dimensional data.\\nREFERENCES\\n[1] J. MacQueen, ”Some methods for classification and analysis of multi-\\nvariate observations,” Proc. Fifth Berkeley Symp. Math. Stat. Probab.,\\nvol. 1, no. 14, pp. 281–297, 1967.\\n[2] D. Arthur and S. Vassilvitskii, ”k-means++: The advantages of careful\\nseeding,” in Proc. ACM-SIAM Symp. Discrete Algorithms, 2007, pp.\\n1027–1035.\\n[3] S. Lloyd, ”Least squares quantization in PCM,” IEEE Trans. Inf. Theory,\\nvol. 28, no. 2, pp. 129–137, 1982.\\n[4] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, ”A density-based\\nalgorithm for discovering clusters in large spatial databases with noise,”\\nin Proc. Int. Conf. Knowl. Discovery Data Mining, 1996, pp. 226–231.\\n[5] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander, ”OPTICS:\\nOrdering points to identify the clustering structure,” in Proc. ACM\\nSIGMOD Int. Conf. Manag. Data, 1999, pp. 49–60.\\n[6] R. J. G. B. Campello, D. Moulavi, and J. Sander, ”Density-based\\nclustering based on hierarchical density estimates,” in Proc. Pacific-Asia\\nConf. Knowl. Discovery Data Mining, 2013, pp. 160–172.\\n[7] A. Y. Ng, M. I. Jordan, and Y. Weiss, ”On spectral clustering: Analysis\\nand an algorithm,” in Adv. Neural Inf. Process. Syst., 2001, pp. 849–856.\\n[8] U. Von Luxburg, ”A tutorial on spectral clustering,” Stat. Comput., vol.\\n17, no. 4, pp. 395–416, 2007.\\n[9] L. Zelnik-Manor and P. Perona, ”Self-tuning spectral clustering,” in Adv.\\nNeural Inf. Process. Syst., 2004, pp. 1601–1608.\\n[10] A. Rodriguez and A. Laio, ”Clustering by fast search and find of density\\npeaks,” Science, vol. 344, no. 6191, pp. 1492–1496, 2014.\\n[11] X. Wang, A. Mueen, H. Ding, G. Trajcevski, P. Scheuermann, and\\nE. Keogh, ”Experimental comparison of representation methods and\\ndistance measures for time series data,” Data Mining Knowl. Discovery,\\nvol. 26, no. 2, pp. 275–309, 2013.\\n[12] R. Xu and D. Wunsch, ”Survey of clustering algorithms,” IEEE Trans.\\nNeural Netw., vol. 16, no. 3, pp. 645–678, 2005.\\n[13] I. T. Jolliffe, Principal Component Analysis. New York, NY: Springer,\\n2002.\\n[14] L. van der Maaten and G. Hinton, ”Visualizing data using t-SNE,” J.\\nMach. Learn. Res., vol. 9, pp. 2579–2605, 2008.\\n[15] L. McInnes, J. Healy, and J. Melville, ”UMAP: Uniform manifold ap-\\nproximation and projection for dimension reduction,” arXiv:1802.03426,\\n2018.\\n[16] J. Sander, M. Ester, H.-P. Kriegel, and X. Xu, ”Density-based clustering\\nin spatial databases: The algorithm GDBSCAN and its applications,”\\nData Mining Knowl. Discovery, vol. 2, no. 2, pp. 169–194, 1998.\\n[17] P. J. Rousseeuw, ”Silhouettes: A graphical aid to the interpretation and\\nvalidation of cluster analysis,” J. Comput. Appl. Math., vol. 20, pp.\\n53–65, 1987.\\n[18] Baligodugula, Vishnu Vardhan. ”Unsupervised-based distributed ma-\\nchine learning for efficient data clustering and prediction.” (2023).\\n[19] M. Ur Rahman, B. Vishnu Vardhan, L. Jenith, V. Rakesh Reddy,\\n”Spectrum sensing using nmlmf algorithm in cognitive radio networks\\nfor health care monitoring applications” Int. J. Adv. Trends Comput.\\nSci. Eng., vol. 9, no. 5, pp. 1-7, 2020.\\n[20] Zıa Ur Rahman, Md, Baligodugula Vishnu Vardhan, Lakkakula Jenith,\\nVeeramreddy Rakesh Reddy, Sala Surekha, and Putluri Srinivasareddy.\\n”Adaptive exon prediction using maximum error normalized algo-\\nrithms.” In Proceedings of 2nd International Conference on Artificial\\nIntelligence: Advances and Applications: ICAIAA 2021, pp. 511-523.\\nSingapore: Springer Nature Singapore, 2022.\\n[21] https://www.kaggle.com/datasets/hojjatk/mnist-dataset\\n[22] https://www.kaggle.com/datasets/zalando-research/fashionmnist\\n[23] https://www.kaggle.com/competitions/uci-har\\n[24] Baligodugula, Vishnu Vardhan, and Fathi Amsaad. ”Enhancing the Per-\\nformance of Unsupervised Machine Learning Using Parallel Computing:\\nA Comparative Analysis.” In 2024 IEEE 3rd International Conference\\non Computing and Machine Intelligence (ICMI), pp. 1-5. IEEE, 2024.\\n[25] Baligodugula, Vishnu Vardhan, Fathi Amsaad, and Nz Jhanjhi. ”Ana-\\nlyzing the Parallel Computing Performance of Unsupervised Machine\\nLearning.” In 2024 IEEE 1st Karachi Section Humanitarian Technology\\nConference (KHI-HTC), pp. 1-6. IEEE, 2024.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29d24c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
